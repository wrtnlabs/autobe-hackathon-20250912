{"operations":[{"specification":"This set of API operations enables authentication and token management for the 'guest' role within the politicalNewsCrawler system. Guest users represent unauthenticated users who only have restricted access to public endpoints for retrieving news and popular topics without needing login credentials. These operations facilitate guest account creation (join) which assigns a temporary guest identity to users to track their session or basic metadata such as IP and user agent, and enable refreshing of access tokens via a refresh endpoint.\n\nThe join operation at /auth/guest/join allows creating a temporary guest account. It does not require login credentials since guests don't authenticate with passwords. This endpoint is public and serves to provide a guest entry point. Upon successful creation, a temporary JWT token is issued encapsulated in the response object IPoliticalNewsCrawlerGuest.IAuthorized which represents the authorized guest state.\n\nThe refresh operation at /auth/guest/refresh allows guests to refresh their issued temporary tokens using a valid refresh token. This supports session continuity without rejoining.\n\nSecurity considerations include restricting guest operations to temporary token issuance without password authentication. These endpoints ensure controlled issuance and refresh of JWT tokens while guests remain unauthenticated users with limited access.\n\nRelated operations with login and user credential validation are not applicable for the guest role since they have no persistent login credentials.\n\nThese operations reference the 'political_news_crawler_guests' Prisma schema table which records guest metadata such as IP address and user agent with auditing timestamps to track anonymous user sessions safely and compliantly.\n","authorizationType":"join","description":"Provides an endpoint for guest users (unauthenticated) to create a temporary guest account and receive JWT tokens to access public APIs. The join route allows automatically generating guest identifiers without passwords. The response type IPoliticalNewsCrawlerGuest.IAuthorized represents the token response containing temporary guest authentication details and metadata as per the political_news_crawler_guests schema.","summary":"Create a temporary guest account and issue JWT tokens. Maps to political_news_crawler_guests table for guest metadata tracking.","parameters":[],"requestBody":{"description":"Guest join request payload. For guests, this is typically empty or minimal since no credentials are needed.","typeName":"IPoliticalNewsCrawlerGuest.IRequest"},"responseBody":{"description":"Guest authorization token response containing JWT tokens and guest metadata.","typeName":"IPoliticalNewsCrawlerGuest.IAuthorized"},"authorizationRole":"guest","name":"join","path":"/auth/guest/join","method":"post"},{"specification":"Allows guest users holding temporary issued tokens to refresh their access credentials using the refresh token. This operation supports continued access for guest sessions without requiring rejoining or account recreation. Tokens are validated and renewed securely.\n\nThis operation does not require traditional login credentials. It accepts a refresh token and returns a new set of JWT tokens encapsulated in the response type IPoliticalNewsCrawlerGuest.IAuthorized.\n\nReferencing the political_news_crawler_guests table schema, the refresh process maintains guest session continuity while enforcing secure token management policies. Guests remain unauthenticated with limited permission scopes.\n\nThis operation is crucial to support guest user experiences where tokens have expiry but sessions persist beyond token lifetime.\n","authorizationType":"refresh","description":"Refresh temporary JWT tokens for a guest user session. Requires a valid refresh token and issues new tokens with updated expiry.","summary":"Refresh guest access tokens securely. Maps to political_news_crawler_guests table for session continuity without login.","parameters":[],"requestBody":{"description":"Payload containing a refresh token to renew guest JWT tokens securely.","typeName":"IPoliticalNewsCrawlerGuest.IRefresh"},"responseBody":{"description":"New authorized tokens response including JWT tokens and associated guest session metadata.","typeName":"IPoliticalNewsCrawlerGuest.IAuthorized"},"authorizationRole":"guest","name":"refresh","path":"/auth/guest/refresh","method":"post"},{"specification":"This operation retrieves a paginated and filterable list of all political news crawler crawl sources as stored in the political_news_crawler_crawl_sources table in the Prisma schema. The crawl sources consist of configurations for various external news sources including their URLs, description, and active status. This endpoint supports searching and sorting to allow consumers to find sources based on source code, URL, status, or creation date.","description":"Retrieve a filtered and paginated list of all political news crawler crawl sources. The crawl sources represent configurations for political news content retrieval from diverse external websites or APIs in the system.\n\nThis operation supports advanced searching and sorting capabilities, enabling clients to query sources by attributes such as source code, URL, and active status. It returns summarized information optimized for listing.\n\nSecurity considerations: This endpoint is publicly accessible with no authentication required as all source metadata is non-sensitive.\n\nThis operation relates directly to the political_news_crawler_crawl_sources table in the Prisma schema, which tracks source identifiers, URLs, activation status, and audit metadata. Response includes only relevant summary fields for efficient transmission.\n\nUsage with other endpoints: Use POST /politicalNewsCrawler/crawlSources for creation, GET /politicalNewsCrawler/crawlSources/{id} for details, and PUT /politicalNewsCrawler/crawlSources/{id} for updates as needed.","summary":"Search and retrieve a filtered, paginated list of political news crawler crawl sources","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for crawl source filtering","typeName":"IPoliticalNewsCrawlerCrawlSources.IRequest"},"responseBody":{"description":"Paginated list of crawl source summaries matching search criteria","typeName":"IPageIPoliticalNewsCrawlerCrawlSources.ISummary"},"authorizationType":null,"authorizationRole":null,"path":"/politicalNewsCrawler/politicalNewsCrawler/crawlSources","method":"patch","name":"index"},{"specification":"This operation retrieves detailed information of a single political news crawler crawl source identified by its primary key ID. It returns all metadata including source code, URL, status flags, and audit timestamps as defined in the political_news_crawler_crawl_sources table in the Prisma schema. This fetch by ID allows clients to obtain full details for a given source configuration.","description":"Retrieve details of a specific political news crawler crawl source by its unique ID. The crawl source is a configuration entity representing an external site or API source for political news crawling.\n\nThis endpoint returns the complete information for the crawl source record including the source code, URL, active flag, description, and creation/update timestamps.\n\nNo authentication is required since this information is not sensitive and is intended for public consumption.\n\nUse this endpoint to examine or verify configuration details of a specific crawl source. Combine with the crawl source listing endpoint PATCH /politicalNewsCrawler/crawlSources for full source management workflows.\n\nThe operation is closely tied to the political_news_crawler_crawl_sources Prisma table.","summary":"Retrieve a specific political news crawler crawl source by ID","parameters":[{"name":"id","description":"Unique identifier of the target crawl source","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Full detailed crawl source information","typeName":"IPoliticalNewsCrawlerCrawlSources"},"authorizationType":null,"authorizationRole":null,"path":"/politicalNewsCrawler/politicalNewsCrawler/crawlSources/{id}","method":"get","name":"at"},{"specification":"This operation creates a new political news crawler crawl source in the system, adding a new external source configuration to enable crawling. It accepts the source parameters including source code, URL, whether the source is active, and optional description, as defined in the political_news_crawler_crawl_sources Prisma table fields. The system generates the ID and timestamps internally. This endpoint allows clients to register new crawl sources dynamically as needed.","description":"Create a new political news crawler crawl source with specified configuration data. The client provides source code, source URL, active flag, and optional description.\n\nUpon successful creation, the system returns the full detailed crawl source record including generated ID and timestamps.\n\nThis operation is intended for administrative or automated system use to register new crawl source endpoints.\n\nIt directly maps to the political_news_crawler_crawl_sources table in the Prisma schema.","summary":"Create a new political news crawler crawl source","parameters":[],"requestBody":{"description":"Creation info of the crawl source","typeName":"IPoliticalNewsCrawlerCrawlSources.ICreate"},"responseBody":{"description":"Newly created crawl source information","typeName":"IPoliticalNewsCrawlerCrawlSources"},"authorizationType":null,"authorizationRole":null,"path":"/politicalNewsCrawler/politicalNewsCrawler/crawlSources","method":"post","name":"create"},{"specification":"This operation updates an existing political news crawler crawl source identified by its ID, modifying its configuration fields such as source code, URL, active status, and description. It performs full update of the crawl source record as defined in the political_news_crawler_crawl_sources Prisma table. The system updates the timestamp accordingly. This allows administrative maintenance of crawl source configurations.","description":"Update an existing political news crawler crawl source by ID with given update data. The client can modify source code, source URL, is_active flag, and description.\n\nThe system returns the updated full crawl source record upon success.\n\nThis operation supports CRUD maintenance over crawl source configurations and ensures uniform data integrity with Prisma schema field enforcement.\n\nCorresponds to the political_news_crawler_crawl_sources Prisma table.","summary":"Update an existing political news crawler crawl source","parameters":[{"name":"id","description":"Unique identifier of the target crawl source","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update info of the crawl source","typeName":"IPoliticalNewsCrawlerCrawlSources.IUpdate"},"responseBody":{"description":"Updated crawl source information","typeName":"IPoliticalNewsCrawlerCrawlSources"},"authorizationType":null,"authorizationRole":null,"path":"/politicalNewsCrawler/politicalNewsCrawler/crawlSources/{id}","method":"put","name":"update"},{"specification":"This operation deletes a specific crawl source from the political_news_crawler_crawl_sources table identified by its UUID. This delete operation is a hard delete whereby the record is permanently removed from the database. Admin role authorization is required to perform this operation, ensuring that only authorized users can remove crawl sources which may have dependencies such as schedules, jobs and raw data.","description":"Deletes a crawl source identified by its UUID from the system's database. This will permanently remove the crawl source and all of its associations including crawl schedules, crawl jobs, raw data, and related processing linked to this source.\n\nOnly users with the admin role are authorized to perform this destructive operation.\n\nErrors for non-existent or invalid UUIDs are handled with proper HTTP error codes.\n\nNo request body is required. No response content is returned on successful deletion.","summary":"Delete a crawl source by ID","parameters":[{"name":"id","description":"Unique identifier of the crawl source to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":["admin"],"path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSources/{id}","method":"delete","name":"erase","authorizationRole":"admin","authorizationType":null},{"specification":"Retrieve a paginated list of crawl policies stored in the political_news_crawler_crawl_policies table. This operation supports filtering by policy attributes along with pagination and sorting, enabling administrators to search and manage crawl policies effectively.\n\nAccess is restricted to users with admin role to protect sensitive crawling configuration data.","description":"Retrieves a filtered, paginated list of crawl policies from the database. Supports advanced search parameters including max crawl frequency, retry attempts, and ban detection flags.\n\nThis operation returns crawl policy summaries optimized for list display in administrative interfaces.\n\nOnly admin users are authorized for this operation.\n\nThe response contains a pageable data set with key policy attributes.\n\nErrors such as invalid filters or pagination parameters will be returned as appropriate HTTP errors.\n\nThe request body specifies complex search and pagination criteria.","summary":"Search and list crawl policies","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for crawl policies filtering","typeName":"IPoliticalNewsCrawlerCrawlPolicy.IRequest"},"responseBody":{"description":"Paginated list of crawl policy summaries matching search criteria","typeName":"IPageIPoliticalNewsCrawlerCrawlPolicy.ISummary"},"authorizationRoles":["admin"],"path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlPolicies","method":"patch","name":"index","authorizationRole":"admin","authorizationType":null},{"specification":"Retrieve detailed information for a single crawl policy by its UUID from the political_news_crawler_crawl_policies table.\n\nAccess is restricted to admin users since this exposes complete crawl policy configuration details.\n\nReturns full crawl policy entity including all parameters and timestamps.\n\nErrors for invalid or missing IDs yield appropriate HTTP error codes.","description":"Retrieves detailed crawl policy information identified by its UUID.\n\nThis operation supports administrative usage for managing crawl policies.\n\nOnly users with admin role are authorized.\n\nResponse returns full crawl policy entity.\n\nInvalid IDs return error responses.","summary":"Retrieve crawl policy details by ID","parameters":[{"name":"id","description":"Unique identifier of the crawl policy to retrieve","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed crawl policy entity","typeName":"IPoliticalNewsCrawlerCrawlPolicy"},"authorizationRoles":["admin"],"path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlPolicies/{id}","method":"get","name":"at","authorizationRole":"admin","authorizationType":null},{"specification":"Creates a new crawl policy record in the political_news_crawler_crawl_policies table. The request body must provide the policy data following the ICreate schema including policy name, max crawl frequency, retry attempts, backoff multiplier and ban detection flag.\n\nAuthorization is limited to admin users due to the impact on system crawling behavior.\n\nOn success, returns the created crawl policy entity including its ID and timestamps.\n\nValidation errors, such as duplicate policy names or out-of-range parameters, trigger detailed error responses.","description":"Creates a new crawl policy with the provided details.\n\nThe request body must include fields such as policy_name, max crawl frequency (minutes), max retry attempts, backoff multiplier and ban detection enabled flag.\n\nOnly admin users are authorized to create new crawl policies.\n\nReturns the created crawl policy entity.\n\nErrors in input data validation return detailed messages.\n\nNo path parameters are required.","summary":"Create new crawl policy","parameters":[],"requestBody":{"description":"New crawl policy data","typeName":"IPoliticalNewsCrawlerCrawlPolicy.ICreate"},"responseBody":{"description":"Created crawl policy entity","typeName":"IPoliticalNewsCrawlerCrawlPolicy"},"authorizationRoles":["admin"],"path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlPolicies","method":"post","name":"create","authorizationRole":"admin","authorizationType":null},{"specification":"This API operation updates an existing crawl policy in the politicalNewsCrawler backend system. It allows modifying the maximum crawling frequency in minutes, maximum retry attempts after failures, exponential backoff multiplier, and ban detection enforcement flag for crawl scheduling. The operation updates a specific record in the political_news_crawler_crawl_policies table identified by its UUID primary key. It expects the updated crawl policy data in a structured request body matching the IPoliticalNewsCrawlerCrawlPolicy.IUpdate DTO type, and returns the updated crawl policy entity upon success. This supports backend administrators in managing crawling frequency and retry behavior dynamically to maintain optimal crawling performance while respecting source limitations.","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlPolicies/{id}","method":"put","summary":"Update an existing crawl policy in politicalNewsCrawler","description":"This endpoint updates the details of an existing crawl policy used by the politicalNewsCrawler system to govern crawl scheduling and error handling. Updating crawl policies enables administrators to adjust parameters including:\n\n- Maximum crawl frequency in minutes to control how often crawls are scheduled.\n- Maximum retry attempts to specify how many times to retry failed crawling operations before aborting.\n- Backoff multiplier controlling the exponential delay applied between retries.\n- Ban detection enabled flag indicating whether the system should actively detect and handle bans.\n\nSecurity considerations: Only authorized backend administrators should have access to update crawl policies to prevent disruption of crawling behavior. The operation updates a single policy record identified by the provided UUID path parameter, ensuring precise targeting.\n\nThe operation interfaces with the underlying political_news_crawler_crawl_policies table, modifying fields according to the request payload. Proper validation is performed to ensure data integrity.\n\nIn case of errors (e.g., invalid ID or data), the system returns appropriate HTTP error status codes. Successful updates return the modified crawl policy entity reflecting the new parameters.","parameters":[{"name":"id","description":"Unique identifier of the crawl policy to update","schema":{"type":"string","format":"uuid"},"in":"path"}],"requestBody":{"description":"Updated crawl policy data for modification","typeName":"IPoliticalNewsCrawlerCrawlPolicy.IUpdate"},"responseBody":{"description":"The updated crawl policy entity","typeName":"IPoliticalNewsCrawlerCrawlPolicy"},"authorizationRoles":["guest"],"authorizationType":null,"authorizationRole":"guest","name":"updateCrawlPolicy"},{"specification":"This API operation deletes an existing crawl policy record permanently from politicalNewsCrawler backend. It removes the entry from the political_news_crawler_crawl_policies table identified by the UUID primary key. This operation is intended for administrators to manage crawl policies lifecycle by removing deprecated or invalid policies. This delete action permanently removes the record and cannot be undone; no soft deletion is indicated for this entity.","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlPolicies/{id}","method":"delete","summary":"Delete a crawl policy by ID permanently","description":"Deletes the specified crawl policy record from the politicalNewsCrawler system permanently. \n\nThis operation requires administrative privileges due to its impact on crawling behavior and system stability. Careful consideration is necessary before invoking the delete to avoid disruption.\n\nThe API removes the crawl policy identified by the UUID path parameter from the underlying database table political_news_crawler_crawl_policies.\n\nNo request body or response body is returned. Successful invocation results in a confirmation of deletion with no content. Failure cases return an appropriate error code.","parameters":[{"name":"id","description":"Unique identifier of the crawl policy to delete","schema":{"type":"string","format":"uuid"},"in":"path"}],"requestBody":null,"responseBody":null,"authorizationRoles":["guest"],"authorizationType":null,"authorizationRole":"guest","name":"eraseCrawlPolicy"},{"specification":"Retrieve a filtered and paginated list of crawl schedules used in politicalNewsCrawler backend to manage crawl execution timing for different sources. This operation supports filtering by source, policy, and enabled status, with pagination parameters to handle large data sets. The schedules are retrieved from the political_news_crawler_crawl_schedules table and provide details including cron expression, last and next crawl times, and enablement flag.","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSchedules","method":"patch","summary":"Search and list crawl schedules with filtering and pagination","description":"This endpoint provides a paginated list of crawl schedules in the politicalNewsCrawler system. Crawl schedules define when and how often crawling occurs for each configured source, making this information critical for managing crawl frequency and timing.\n\nUsers can filter results by various criteria including source, policy, and whether the schedule is enabled. Extensive pagination support allows clients to manage large result sets efficiently.\n\nSecurity considerations: Access to crawl schedule listings might be restricted to authorized personnel to prevent information disclosure about crawling operations.\n\nThe operation interfaces with the underlying political_news_crawler_crawl_schedules table, projecting relevant fields for API clients including schedule expression, last and next crawl timestamps, and enablement status.\n\nResponse contains a paginated array of crawl schedule summary information for client consumption.","parameters":[],"requestBody":{"description":"Search and pagination filter criteria for crawl schedules","typeName":"IPoliticalNewsCrawlerCrawlSchedule.IRequest"},"responseBody":{"description":"Paginated list of crawl schedules","typeName":"IPageIPoliticalNewsCrawlerCrawlSchedule.ISummary"},"authorizationRoles":["guest"],"authorizationType":null,"authorizationRole":"guest","name":"searchCrawlSchedules"},{"specification":"Retrieve detailed crawl schedule information by its ID to provide complete scheduling and policy data. This operation fetches a single crawl schedule record from politicalNewsCrawler backend's political_news_crawler_crawl_schedules table and presents all fields including cron expression, last and next crawl times, enablement flag, and associated crawl source and policy details.","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSchedules/{id}","method":"get","summary":"Retrieve detailed crawl schedule by ID","description":"This endpoint retrieves a detailed crawl schedule record identified by its unique UUID from the politicalNewsCrawler system. Crawl schedules configure how crawling jobs are scheduled for specific sources and policies.\n\nThe operation returns full details of the schedule including its cron expression, timestamps of last and next crawls, enablement state, creation and update times, and links to the associated crawl source and crawl policy.\n\nSecurity considerations: Access may be limited to administrative users due to operational sensitivity of crawl schedule data.\n\nSuccessful retrieval provides a comprehensive data structure representing the specific crawl schedule for backend monitoring or management use cases. Errors such as invalid IDs return appropriate HTTP status codes indicating not found or unauthorized access.","parameters":[{"name":"id","description":"Unique identifier of the crawl schedule to retrieve","schema":{"type":"string","format":"uuid"},"in":"path"}],"requestBody":null,"responseBody":{"description":"Detailed crawl schedule information","typeName":"IPoliticalNewsCrawlerCrawlSchedule"},"authorizationRoles":["guest"],"authorizationType":null,"authorizationRole":"guest","name":"getCrawlSchedule"},{"specification":"This operation creates a new crawling schedule in the political_news_crawler_crawl_schedules table. It enables backend administrators or automated systems to define when, how often, and under which policies a specific crawl source should be crawled. This includes specifying the cron schedule expression, enabling or disabling the schedule, and linking to the related crawl source and crawl policy for adaptive crawling behavior. It is essential for orchestrating crawling tasks and ensuring compliance with crawling rate limits and bans. The creation operation respects the database schema constraints and relationships, establishing foreign key references to the crawl source and crawl policy tables.","description":"Create a new crawling schedule for political news crawling sources.\n\nThis operation allows authorized systems or administrators to define when and how often a crawl source should be crawled. The crawl schedule includes a cron expression specifying the timing, links to the crawl source and crawl policy to control crawl frequency and backoff strategies, and flags to enable or disable the schedule.\n\nSecurity considerations restrict this operation to authorized roles managing crawling infrastructure.\n\nThe created schedule is registered in the political_news_crawler_crawl_schedules table, with all required validations for data integrity and foreign key constraints against crawl_sources and crawl_policies.\n\nExpected behavior includes returning the newly created schedule with all relevant metadata.\n\nErrors are raised for invalid references or conflicting schedules. The operation never supports soft-deletion here, and data is persistently stored.","summary":"Create a new political news crawler schedule","parameters":[],"requestBody":{"description":"Creation info of the crawl schedules","typeName":"IPoliticalNewsCrawlerCrawlSchedules.ICreate"},"responseBody":{"description":"Created crawl schedule information","typeName":"IPoliticalNewsCrawlerCrawlSchedules"},"authorizationType":null,"authorizationRole":"guest","method":"post","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSchedules","name":"create"},{"specification":"This operation updates an existing crawling schedule record in the political_news_crawler_crawl_schedules table. It enables modification of schedule parameters such as the cron expression, enable flag, and references to crawl source and policy. This update allows maintaining crawl frequency and timing dynamically according to system needs or source behavior changes. Validations ensure foreign keys exist and the schedule record is identified correctly.\n\nThe operation corresponds to a standard update on a resource identified by its id, providing the ability to maintain crawling infrastructure consistently.\n\nThis update modifies the scheduling metadata while preserving the temporal consistency of crawl start and end timestamps not affected here.\n\nErrors are managed for invalid IDs or data violations.","description":"Update an existing crawling schedule identified by its ID.\n\nAllows modification of cron schedule expression, crawl source and policy references, and enabled status.\n\nOnly authorized roles can perform update operations.\n\nThe operation updates the record in the political_news_crawler_crawl_schedules table and returns the updated entity.\n\nRobust validation of the input is performed to prevent data inconsistencies.\n\nNo soft delete behavior is relevant.\n\nErrors include not found schedule or invalid foreign keys.","summary":"Update specified political news crawler schedule","parameters":[{"name":"id","description":"Target crawl schedule's ID","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update info for crawl schedule","typeName":"IPoliticalNewsCrawlerCrawlSchedules.IUpdate"},"responseBody":{"description":"Updated crawl schedule information","typeName":"IPoliticalNewsCrawlerCrawlSchedules"},"authorizationType":null,"authorizationRole":"guest","method":"put","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSchedules/{id}","name":"update"},{"specification":"This operation permanently removes a crawling schedule from the political_news_crawler_crawl_schedules table by its unique ID. Removal is a hard delete; there are no soft delete mechanisms for this entity's API. Deleting a schedule stops all future crawling tasks associated with it.\n\nIt requires appropriate administrative roles for authorization due to the impact on crawling operations. This endpoint does not take a request body and returns no data upon success.\n\nErrors are returned if the specified schedule ID does not exist.","description":"Delete a crawling schedule by its ID.\n\nThis operation permanently removes the schedule and disables associated crawl jobs.\n\nAuthorization is restricted to admin roles to ensure controlled operations.\n\nNo response body is returned. Errors occur if the ID does not exist.\n\nThis is a hard delete operation reflecting the physical removal of the schedule record in the database.","summary":"Delete specified political news crawler schedule","parameters":[{"name":"id","description":"Target crawl schedule's ID","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","method":"delete","path":"/politicalNewsCrawler/guest/politicalNewsCrawler/crawlSchedules/{id}","name":"erase"},{"specification":"This operation performs complex filtering, searching, and paginated retrieval of guest users accessing the political news crawler API. Guests represent unauthenticated users identified by IP and user agent, tracked for analytics and monitoring. This search endpoint supports advanced query criteria and pagination controls for efficient data retrieval.\n\nThe guest data is stored in the political_news_crawler_guests table. Only read operations are permitted by guest users themselves, while admin roles can utilize this endpoint for monitoring and analysis.\n\nThe request body defines search filters and pagination parameters. Responses include paginated summary records of guests matching the criteria.\n\nThis endpoint requires admin authorization to ensure privacy and proper access control.","description":"Retrieve a filtered, sorted, and paginated list of political news crawler guest users.\n\nGuests are unauthenticated visitors recorded by IP, user-agent, and timestamps. This operation allows search filtering on these attributes and supports efficient pagination and sort ordering.\n\nSecurity restricts this to administrator-level access to protect user privacy.\n\nThe response contains a paginated list of guest summaries matching the filters.\n\nErrors may arise from invalid criteria or pagination parameters.\n\nNo modifications are made to guest records here, this is a read-only operation.","summary":"Search and retrieve filtered, paginated political news crawler guests","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for guests","typeName":"IPoliticalNewsCrawlerGuests.IRequest"},"responseBody":{"description":"Paginated list of guest user summary records","typeName":"IPageIPoliticalNewsCrawlerGuests.ISummary"},"authorizationType":null,"authorizationRole":"guest","method":"patch","path":"/politicalNewsCrawler/guest/guests","name":"search"},{"specification":"This operation retrieves detailed information about a specific guest user of the political news crawler system. It accesses the political_news_crawler_guests table which stores IP address and user agent data of unauthenticated guest users. The operation supports identification of guests by their unique UUID. It is read-only and accessible publicly without authentication as guests have no write permissions.","description":"Retrieve information about a specific guest user identified by guestId in the political_news_crawler_guests table.\n\nThis operation returns guest details including IP address and user agent string, as well as timestamps for creation and last update.\n\nSecurity considerations: this endpoint is public and provides read-only access to guest metadata only. No sensitive data or authentication details are exposed.\n\nRelated operations include listing all guest users and managing crawl jobs.\n\nErrors will be returned if guestId does not exist.","summary":"Retrieve a specific guest user by ID","parameters":[{"name":"guestId","description":"Unique identifier of the guest user","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed information of the specified guest user","typeName":"IPoliticalNewsCrawlerGuests"},"authorizationRoles":["guest"],"name":"at","path":"/politicalNewsCrawler/guest/guests/{guestId}","method":"get","authorizationRole":"guest","authorizationType":null},{"specification":"This operation retrieves a paginated list of crawl jobs. It operates on the political_news_crawler_crawl_jobs table which manages scheduled crawling tasks for political news sources. The operation supports complex search and filtering parameters to query crawl jobs efficiently.","description":"Retrieve a paginated and filtered list of crawl jobs managed by the political_news_crawler_crawl_jobs table.\n\nSupports search parameters including schedule ID and active status, allowing clients to find specific crawl jobs.\n\nSecurity: This endpoint is publicly accessible, providing read-only access to crawl job status information.\n\nRelated operations include querying specific crawl job details and creating new crawl jobs.\n\nErrors will be returned if search criteria are invalid.","summary":"Search and list crawl jobs","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for crawl job filtering","typeName":"IPoliticalNewsCrawlerCrawlJobsIRequest"},"responseBody":{"description":"Paginated list of crawl jobs matching criteria","typeName":"IPageIPoliticalNewsCrawlerCrawlJobs"},"authorizationRoles":["guest"],"name":"index","path":"/politicalNewsCrawler/guest/crawlJobs","method":"patch","authorizationRole":"guest","authorizationType":null},{"specification":"This operation retrieves detailed information of a specific crawl job identified by its ID. The crawl job represents a scheduled crawling operation for political news sources. It provides metadata about the job status, schedule, and activity.","description":"Retrieve detailed information about a crawl job specified by its unique ID.\n\nReturns complete crawl job information including schedule, activity status, and timestamps.\n\nSecurity: Requires user-level authorization.\n\nRelated operations include listing crawl jobs and creating new crawl jobs.\n\nErrors occur if the crawl job ID does not exist.","summary":"Retrieve a crawl job by ID","parameters":[{"name":"id","description":"Unique identifier of the crawl job","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed crawl job information","typeName":"IPoliticalNewsCrawlerCrawlJobs"},"authorizationRoles":["guest"],"name":"at","path":"/politicalNewsCrawler/guest/crawlJobs/{id}","method":"get","authorizationRole":"guest","authorizationType":null},{"specification":"This operation creates a new crawl job record that schedules crawling tasks for political news sources. The crawl job links to specific crawl sources and schedules, defines its active state, and initializes timing metadata. This enables dynamic control over crawl task execution.","description":"Create a new crawl job to schedule crawling operations for a political news source.\n\nRequires specifying the associated crawl source, crawl schedule, and active status.\n\nReturns the detailed crawl job record after creation.\n\nSecurity: Requires user role authorization.\n\nRelated operations include listing crawl jobs and retrieving crawl job details.\n\nErrors occur if required fields are missing or references invalid IDs.","summary":"Create a new crawl job","parameters":[],"requestBody":{"description":"Information to create a new crawl job","typeName":"IPoliticalNewsCrawlerCrawlJobsICreate"},"responseBody":{"description":"Created crawl job information","typeName":"IPoliticalNewsCrawlerCrawlJobs"},"authorizationRoles":["guest"],"name":"create","path":"/politicalNewsCrawler/guest/crawlJobs","method":"post","authorizationRole":"guest","authorizationType":null},{"specification":"This operation updates an existing crawling job resource in the political_news_crawler_crawl_jobs table in the Prisma DB schema. Users can modify the job's active status and metadata such as the last run start and completion timestamps. This update allows controlling the scheduling and execution state of a specific crawl job associated with a given crawl source and schedule.","description":"Update the details of an existing crawl job identified by its unique ID. This enables controlling whether the crawl job is active, and can adjust timing metadata such as last run start and completion times. The operation is crucial for administrators or backend systems managing crawl schedules and execution state for political news data retrieval.\n\nSecurity considerations: this endpoint requires guest role authorization as per system role configuration.\n\nThe operation corresponds to the political_news_crawler_crawl_jobs table and updates mutable fields while preserving relational integrity.\n\nBusiness logic includes validation of input timestamps and ensuring the referenced crawl source and schedule remain valid during the update process.\n\nRelated operations include endpoints for retrieving crawl job details and managing crawl attempts.\n\nExpected behavior includes returning the updated crawl job resource upon success or appropriate error messages on failure.","summary":"Update an existing crawl job by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the crawl job to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update data for crawl job","typeName":"IPoliticalNewsCrawlerCrawlJob.IUpdate"},"responseBody":{"description":"Updated crawl job information","typeName":"IPoliticalNewsCrawlerCrawlJob"},"authorizationType":null,"authorizationRole":"guest","name":"update","path":"/politicalNewsCrawler/guest/crawlJobs/{id}","method":"put"},{"specification":"This operation performs a permanent deletion (hard delete) of a crawl job identified by its ID from the political_news_crawler_crawl_jobs table. It removes the job record and all references from the system, disabling scheduled crawling tasks accordingly.","description":"Permanently remove a crawl job by its unique ID. This operation deletes the record from the database and disables further scheduled crawling for the associated crawl source and schedule. The deletion is irreversible.\n\nSecurity considerations: this endpoint requires guest role authorization as per system role configuration.\n\nRelated operations include updating crawl job details and managing crawl attempts.\n\nExpected behavior is the complete removal of the job and returning no response body upon success.","summary":"Erase a crawl job by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the crawl job to remove","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","name":"erase","path":"/politicalNewsCrawler/guest/crawlJobs/{id}","method":"delete"},{"specification":"This operation retrieves a paginated list of crawl attempts for a specific crawl job identified by crawlJobId. It supports search, filtering, and sorting of crawl attempts associated with the job.","description":"Retrieve a filtered and paginated list of crawl attempts performed under a specific crawl job. This enables monitoring and auditing of individual execution runs, including success status, run times, and error details.\n\nThe operation accepts the crawl job ID as a path parameter and supports complex search criteria in the request body.\n\nIt returns a paginated list of crawl attempt summaries with metadata.\n\nAccess to this operation requires guest role authorization as per system role configuration.\n\nRelated operations include retrieving individual crawl attempt details.\n\nExpected behavior is to supply crawl attempt data efficiently and support pagination for large datasets.","summary":"List crawl attempts for a specific crawl job","parameters":[{"name":"crawlJobId","in":"path","description":"Unique identifier of the crawl job whose attempts to retrieve","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Search parameters for crawl attempts","typeName":"IPoliticalNewsCrawlerCrawlAttempt.IRequest"},"responseBody":{"description":"Paginated list of crawl attempt summaries","typeName":"IPageIPoliticalNewsCrawlerCrawlAttempt.ISummary"},"authorizationType":null,"authorizationRole":"guest","name":"index","path":"/politicalNewsCrawler/guest/crawlJobs/{crawlJobId}/crawlAttempts","method":"patch"},{"specification":"This operation retrieves detailed information about a specific crawl attempt by its unique ID linked to a crawl job.","description":"Fetch detailed information for a crawl attempt identified by its unique ID, linked to a specific crawl job. This allows inspection of individual crawl execution details including timestamps, success status, and error messages.\n\nThe operation requires both the crawl job ID and the crawl attempt ID to locate the record.\n\nAccess to this operation requires guest role authorization as per system role configuration.\n\nRelated operations include listing crawl attempts for a crawl job.\n\nExpected behavior is to return the crawl attempt resource or an error if not found.","summary":"Get details of a crawl attempt by ID","parameters":[{"name":"crawlJobId","in":"path","description":"Unique identifier of the crawl job containing the attempt","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Unique identifier of the crawl attempt to retrieve","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed crawl attempt information","typeName":"IPoliticalNewsCrawlerCrawlAttempt"},"authorizationType":null,"authorizationRole":"guest","name":"at","path":"/politicalNewsCrawler/guest/crawlJobs/{crawlJobId}/crawlAttempts/{id}","method":"get"},{"specification":"This operation creates a new crawl attempt for a given crawl job, allowing the system to initiate a crawling task for political news data collection. It operates on the political_news_crawler_crawl_attempts table, which tracks individual executions of crawl jobs including start and completion timestamps, success status, and associated raw data references. The create action requires a POST method to the /crawlJobs/{crawlJobId}/crawlAttempts path, where crawlJobId specifies the target crawl job. This operation supports recording the initiation of new crawl attempt activities by authenticated system components responsible for managing crawler executions.","description":"Create a new crawl attempt record associated with a specific crawl job. The operation logs the start of a crawl process for a political news source, enabling tracking of individual crawl executions.\n\nSecurity considerations: This operation is restricted to system components or users with a role capable of scheduling and managing crawling tasks. It requires ownership or management privileges for the referenced crawl job.\n\nThe API request must include details such as the crawl job ID and initial crawl attempt metadata like the start time. The response confirms creation with complete crawl attempt details.\n\nThis operation interacts directly with the political_news_crawler_crawl_attempts database table, ensuring each crawl attempt is logged accurately for audit and operational monitoring.\n\nValidation rules prevent creation with nonexistent crawl job IDs and ensure timestamp formats conform to ISO 8601 standards.\n\nThis API is typically called by backend scheduler services or crawling orchestrators to start crawl attempts.","summary":"Create a crawl attempt for a specified crawl job","parameters":[{"name":"crawlJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl job"}],"requestBody":{"description":"Creation information for a new crawl attempt","typeName":"IPoliticalNewsCrawlerCrawlAttempt.ICreate"},"responseBody":{"description":"Information about the created crawl attempt","typeName":"IPoliticalNewsCrawlerCrawlAttempt"},"authorizationType":null,"authorizationRole":"guest","name":"createCrawlAttempt","path":"/politicalNewsCrawler/guest/crawlJobs/{crawlJobId}/crawlAttempts","method":"post"},{"specification":"This operation updates an existing crawl attempt, providing the ability to modify status, timestamps, and error messages after a crawling execution has started or completed. It operates on the political_news_crawler_crawl_attempts table, uniquely identified by the crawl attempt ID and crawl job ID in the URL path. The HTTP method is PUT, and the path is /crawlJobs/{crawlJobId}/crawlAttempts/{id}.\n\nThe API allows adjusting the details of a crawl attempt, including marking success or failure, updating completion times, and recording error messages for audit and debugging.\n\nSecurity considerations ensure only authorized system roles can update crawl attempt records, typically crawler orchestrators or administrators.\n\nThe data model requires timestamps in ISO 8601 format and validates existence of the referenced crawl job and attempt ID.\n\nThis capability is essential for managing and tracking crawling operations' lifecycle and health.","description":"Update details of an existing crawl attempt record associated with a specified crawl job. This operation enables modification of crawl attempt state, success indication, completion time, and error information.\n\nSecurity: Restricted to roles with permission to manage crawling tasks. This operation enforces validation of both crawl job and crawl attempt identifiers.\n\nThe operation interacts with the political_news_crawler_crawl_attempts database table, ensuring consistent updates to crawl execution logs.\n\nCompleted timestamps must be later than the started timestamp. Error message updates may be used for debugging failed attempts.\n\nThis API is typically called by crawler system components upon completion or error detection in crawling.","summary":"Update details of a crawl attempt by ID under a given crawl job","parameters":[{"name":"crawlJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl job"},{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl attempt"}],"requestBody":{"description":"Updated crawl attempt information","typeName":"IPoliticalNewsCrawlerCrawlAttempt.IUpdate"},"responseBody":{"description":"Updated crawl attempt details","typeName":"IPoliticalNewsCrawlerCrawlAttempt"},"authorizationType":null,"authorizationRole":"guest","name":"updateCrawlAttempt","path":"/politicalNewsCrawler/guest/crawlJobs/{crawlJobId}/crawlAttempts/{id}","method":"put"},{"specification":"This operation deletes an existing crawl attempt identified by both crawl job ID and crawl attempt ID in the URL. It acts on the political_news_crawler_crawl_attempts table and performs a hard delete because the schema does not contain soft delete fields preventing deletion. This HTTP DELETE endpoint at /crawlJobs/{crawlJobId}/crawlAttempts/{id} removes the crawl attempt permanently from the database.\n\nThis delete operation requires precise matching of identifiers for safety. It is restricted to administrative or system components authorized to manage crawl operations.\n\nThis capability is used to clean up crawl attempt logs, typically for maintenance or removing invalid entries.\n\nNo request body is needed for this operation, and it returns no content upon success.\n\nValidation ensures both crawl job and crawl attempt exist before deletion.","description":"Permanently delete a crawl attempt record by specifying its crawl job ID and crawl attempt ID.\n\nThis is a hard delete operation with no support for soft deletion or recovery.\n\nAccess is restricted to users with administrative rights or system-level permissions.\n\nThe operation directly impacts the political_news_crawler_crawl_attempts table.\n\nNo response body is returned upon success.\n\nUse with caution due to permanent data removal.","summary":"Delete a crawl attempt record by crawl job ID and crawl attempt ID","parameters":[{"name":"crawlJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl job"},{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl attempt"}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","name":"eraseCrawlAttempt","path":"/politicalNewsCrawler/guest/crawlJobs/{crawlJobId}/crawlAttempts/{id}","method":"delete"},{"specification":"Retrieves a paginated and filtered list of crawled news items associated with a specified crawl attempt. It operates on the political_news_crawler_crawled_news table, which contains metadata of individual news articles obtained from crawl attempts.\n\nThe PATCH method at /crawlAttempts/{crawlAttemptId}/crawledNews supports advanced query parameters in the request body for filtering, sorting, and pagination.\n\nThe response includes a paginated collection of summary crawled news matching the query.\n\nThis endpoint is publicly accessible with no authentication to support read-only access to harvested news data.\n\nIt enables clients to retrieve detailed news items tied to particular crawl executions for display, analysis, or monitoring.\n\nError handling includes returning empty results for queries with no matching data and clear messages for malformed queries.","description":"Retrieve a paginated, filtered list of crawled news linked to a specific crawl attempt identified by crawlAttemptId.\n\nSupports complex querying including search filters, sorting, and pagination controls via request body.\n\nThis publicly accessible endpoint requires no authentication, allowing open read access to crawl results.\n\nThe operation queries the political_news_crawler_crawled_news database table and returns data optimized for UI display and analysis.\n\nTypical usage includes news aggregation views, crawl session detail pages, and backend analytics.\n\nProper error handling returns empty pages if no data matches the query and validation errors on bad requests.","summary":"List crawled news filtered by crawl attempt ID","parameters":[{"name":"crawlAttemptId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the crawl attempt"}],"requestBody":{"description":"Search and pagination parameters for filtering crawled news","typeName":"IPoliticalNewsCrawlerCrawledNews.IRequest"},"responseBody":{"description":"Paginated crawled news summaries matching filters","typeName":"IPageIPoliticalNewsCrawlerCrawledNews.ISummary"},"authorizationType":null,"authorizationRole":null,"name":"indexCrawledNews","path":"/politicalNewsCrawler/crawlAttempts/{crawlAttemptId}/crawledNews","method":"patch"},{"specification":"This operation manages the detailed CRUD (Create, Read, Update, Delete) operations for the CrawledNews entity associated with specific CrawlAttempts within the politicalNewsCrawler backend system. The CrawledNews entity represents metadata about individual political news articles retrieved during crawling attempts, including essential attributes such as the article's URL, title, and publish date. This operation interfaces specifically with the political_news_crawler_crawled_news table in the Prisma schema, ensuring data integrity and referential integrity by requiring the associated crawlAttemptId parameter. It enables precise management of news article metadata linked to crawl executions, providing mechanisms for creating new articles, retrieving existing article details, updating article information, or permanently deleting articles if no longer relevant. Each method carefully validates the crawlAttemptId and id path parameters to ensure API operations target the correct hierarchical resource and maintain system consistency.","description":"This operation retrieves detailed metadata of a specific political news article within a given crawl attempt. It returns the article's unique identifier, URL, title, and publishing date.\n\nAccess to this operation requires no authentication, reflecting the system's public API design. The operation references the political_news_crawler_crawled_news table, enforcing relationship integrity by requiring the crawlAttemptId path parameter that links the article to its crawl attempt.\n\nThe API throws an error if the specified article or crawl attempt does not exist. It is designed to provide comprehensive data for client applications that need access to individual news items.\n\nThis operation works closely with create, update, and delete endpoints providing full lifecycle management of news articles within crawl attempts.","summary":"Retrieve a specific crawled news article by crawlAttemptId and article id","parameters":[{"name":"crawlAttemptId","in":"path","description":"Unique identifier of the crawl attempt containing the news article","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Unique identifier of the crawled news article","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed information of the requested crawled news article","typeName":"IPoliticalNewsCrawlerCrawledNews"},"authorizationRoles":[],"name":"at","path":"/politicalNewsCrawler/crawlAttempts/{crawlAttemptId}/crawledNews/{id}","method":"get","authorizationRole":null,"authorizationType":null},{"specification":"This operation creates a new CrawledNews record within a specified CrawlAttempt. CrawledNews records represent metadata about political news articles retrieved during a crawl execution.\n\nThe endpoint accepts a creation DTO containing required properties such as URL, optional title, and optional publish timestamp.\n\nThe operation ensures the new article is correctly linked to the specified CrawlAttempt identified by crawlAttemptId to maintain referential integrity.\n\nSuccessful creation returns the newly created CrawledNews data.\n\nThis operation is intended for backend administration or workflow automation scenarios where new news articles may be added to a crawl attempt post-crawl.\n\nIt references the political_news_crawler_crawled_news table and enforces correct entity associations.","description":"Create a new crawled news article associated with the specified crawl attempt.\n\nThe request body must contain the minimum required information for the crawler news metadata, including a valid URL. Title and published date are optional but recommended for completeness.\n\nNo authentication is required for this operation. On success, the newly created news article's full details are returned.\n\nThis operation complements retrieval and management endpoints allowing clients to add new news entries for a crawl attempt.","summary":"Create a new crawled news article for a given crawlAttemptId","parameters":[{"name":"crawlAttemptId","in":"path","description":"Unique identifier of the crawl attempt to which the news article belongs","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Information needed to create a new crawled news article","typeName":"IPoliticalNewsCrawlerCrawledNews.ICreate"},"responseBody":{"description":"Full details of the created crawled news article","typeName":"IPoliticalNewsCrawlerCrawledNews"},"authorizationRoles":[],"name":"create","path":"/politicalNewsCrawler/crawlAttempts/{crawlAttemptId}/crawledNews","method":"post","authorizationRole":null,"authorizationType":null},{"specification":"This operation updates an existing CrawledNews entity associated with a specific CrawlAttempt. It requires the crawlAttemptId and the unique id of the CrawledNews record to ensure the correct resource is targeted.\n\nThe update payload accepts partial or full modifications to properties such as the URL, title, and published date.\n\nUpon successful update, the operation returns the modified CrawledNews record.\n\nThis operation references the political_news_crawler_crawled_news table and keeps strict referential consistency by requiring valid identifiers for both the crawl attempt and news article.\n\nIt supports backend and client scenarios where crawled news metadata needs amendments after creation.","description":"Update metadata of a crawled news article within the specified crawl attempt.\n\nOnly provided fields in the request body will be updated; others remain unchanged.\n\nThe operation validates the existence of the target article linked to the crawl attempt.\n\nNo authentication is required. Responses include the updated entity details.\n\nThis operation works in conjunction with create, retrieve, and delete endpoints to provide full management capabilities.","summary":"Update an existing crawled news article by crawlAttemptId and article id","parameters":[{"name":"crawlAttemptId","in":"path","description":"Unique identifier of the crawl attempt containing the news article","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Unique identifier of the crawled news article","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Fields to update for the crawled news article","typeName":"IPoliticalNewsCrawlerCrawledNews.IUpdate"},"responseBody":{"description":"Updated details of the crawled news article","typeName":"IPoliticalNewsCrawlerCrawledNews"},"authorizationRoles":[],"name":"update","path":"/politicalNewsCrawler/crawlAttempts/{crawlAttemptId}/crawledNews/{id}","method":"put","authorizationRole":null,"authorizationType":null},{"specification":"This operation permanently removes a CrawledNews record identified by its ID and associated CrawlAttempt ID from the database.\n\nIt requires both crawlAttemptId and article ID path parameters to uniquely identify the resource.\n\nThis is a hard delete operation since the Prisma schema for political_news_crawler_crawled_news does not specify soft delete timestamp.\n\nUpon successful execution, the news article is completely removed and cannot be recovered.\n\nNo response body is returned, and no authentication is required.\n\nThis operation must be used carefully as it irreversibly deletes article metadata tied to crawling attempts.","description":"Permanently delete a specific crawled news article associated with the given crawl attempt.\n\nThis operation removes the record from the database permanently.\n\nIt requires valid and existing identifiers for both the crawl attempt and the article.\n\nNo authentication is required to perform this operation.\n\nThere is no response body upon success.\n\nDeletion is irreversible, so caution is advised.","summary":"Delete a crawled news article by crawlAttemptId and article id","parameters":[{"name":"crawlAttemptId","in":"path","description":"Unique identifier of the crawl attempt containing the news article","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Unique identifier of the crawled news article to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":[],"name":"erase","path":"/politicalNewsCrawler/crawlAttempts/{crawlAttemptId}/crawledNews/{id}","method":"delete","authorizationRole":null,"authorizationType":null},{"specification":"This operation retrieves a paginated list of raw crawled political news data storage entries from the political_news_crawler_raw_data_storage table in the Prisma schema. It enables filtering, sorting, and pagination based on various criteria such as crawl source, crawl job, crawl timestamp, and file metadata. This operation supports efficient retrieval of raw data files metadata for further processing or audit purposes.","description":"Retrieve a filtered and paginated list of raw data storage metadata entries for political news crawling. The operation supports filtering by crawl source and crawl job identifiers, file format types, crawl timestamps, and file sizes. Sorting and pagination options enable efficient browsing through large datasets stored in cloud object storage.\n\nSecurity considerations include limited access to authenticated users with appropriate read privileges, as raw data files may contain sensitive or proprietary information. \n\nThis operation is tightly integrated with the political_news_crawler_raw_data_storage table defined in the Prisma schema, encompassing all relevant fields and relationships. The response returns simplified summary information suited for list displays.\n\nThere is no request body since this is a PATCH method designed for complex search and filtering inputs. The response contains paginated summary records that can be used to locate and verify raw data files before further processing or download.","summary":"Search and retrieve a filtered, paginated list of raw data storage entries","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for raw data storage filtering","typeName":"IPoliticalNewsCrawlerRawDataStorage.IRequest"},"responseBody":{"description":"Paginated list of raw data storage summary matching the search criteria","typeName":"IPageIPoliticalNewsCrawlerRawDataStorage.ISummary"},"authorizationType":null,"authorizationRole":"guest","name":"index","path":"/politicalNewsCrawler/guest/rawDataStorage","method":"patch"},{"specification":"This operation retrieves detailed information about a specific raw data storage entry using its unique identifier from the political_news_crawler_raw_data_storage table in the Prisma schema. It returns all details including storage key, file format, size, crawl timestamp, and associated crawl source and job.\n\nAccess control restricts this operation to authenticated users with appropriate permissions to view sensitive raw data metadata. \n\nThe response provides a full detailed entity representation allowing clients to inspect or validate a specific raw data item.","description":"Retrieve detailed information about a specific raw data storage entry identified by rawDataStorageId. This operation fetches all fields from political_news_crawler_raw_data_storage and related references to crawl source and crawl job.\n\nSecurity requires authenticated user access to prevent unauthorized raw data exposure.\n\nClients can use this operation to access raw data file details necessary for processing or manual inspection.","summary":"Retrieve detailed raw data storage entry by ID","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the target raw data storage entry","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed raw data storage entry information","typeName":"IPoliticalNewsCrawlerRawDataStorage"},"authorizationType":null,"authorizationRole":"guest","name":"at","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}","method":"get"},{"specification":"This operation creates a new raw data storage record in the political_news_crawler_raw_data_storage table in the Prisma schema. It accepts creation details such as crawl source ID, optional crawl job ID, storage key, file format, file size, checksum, and crawl timestamp.\n\nCreation operations require authenticated user roles to ensure data integrity and traceability. \n\nUpon successful creation, the response returns the full entity including assigned ID and timestamps.","description":"Create a new raw data storage record representing a raw crawled data file stored in cloud object storage. The request body must contain all required fields such as crawlSourceId, storageKey, fileFormat, fileSizeBytes, and crawlTimestamp, with optional fields for crawlJobId and checksum.\n\nSecurity requires authenticated access to prevent unauthorized data addition.\n\nThis operation enables the persistence of raw crawl data metadata critical for downstream processing and archival.","summary":"Create new raw data storage entry","parameters":[],"requestBody":{"description":"Creation information of the raw data storage entry","typeName":"IPoliticalNewsCrawlerRawDataStorage.ICreate"},"responseBody":{"description":"Created raw data storage entry information","typeName":"IPoliticalNewsCrawlerRawDataStorage"},"authorizationType":null,"authorizationRole":"guest","name":"create","path":"/politicalNewsCrawler/guest/rawDataStorage","method":"post"},{"specification":"This operation updates an existing raw data storage record identified by rawDataStorageId in the political_news_crawler_raw_data_storage table of the Prisma schema. It accepts update data such as crawl source reference, crawl job reference, storage key, file format, file size, checksum, and crawl timestamp.\n\nThe operation enforces authorization restricting updates to authenticated users only.\n\nUpon successful update, the full updated raw data storage entity is returned for client confirmation and further processing.","description":"Update an existing raw data storage record representing raw crawled political news data. The request body must contain updatable fields such as crawlSourceId, optional crawlJobId, storageKey, fileFormat, fileSizeBytes, checksum, and crawlTimestamp.\n\nThis operation requires authenticated user roles to ensure secure modification of raw data metadata.\n\nClients can use this endpoint to correct or enhance metadata related to raw data files post-crawling or during data reconciliation.","summary":"Update raw data storage entry by ID","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the target raw data storage entry","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update information of the raw data storage entry","typeName":"IPoliticalNewsCrawlerRawDataStorage.IUpdate"},"responseBody":{"description":"Updated raw data storage entry information","typeName":"IPoliticalNewsCrawlerRawDataStorage"},"authorizationType":null,"authorizationRole":"guest","name":"update","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}","method":"put"},{"specification":"This operation permanently deletes a raw data storage record identified by its UUID from the political_news_crawler_raw_data_storage table. This table stores metadata and references for raw political news data collected from various crawling sources, including storage keys for cloud object storage locations, file formats, file sizes, checksums, and crawl timestamps. Deleting a record here removes the raw data metadata and all associated local cache files and processed content entries due to cascading. The delete operation is a hard delete as no explicit soft delete is handled via this endpoint. This is intended for backend administrative cleanup or data lifecycle management.","description":"This DELETE operation removes a specific raw data storage record by its unique identifier. The targeted record belongs to the political_news_crawler_raw_data_storage database entity, which contains critical metadata linking crawled news data to their storage locations.\n\nBy deleting this record, all associated local cache files and processed content linked through foreign keys will also be deleted as cascades are enabled, ensuring referential integrity.\n\nSecurity considerations mandate that only authorized administrative roles can execute this deletion due to the potential for data loss.\n\nUsage of this endpoint requires precise identification of the record to avoid unintended data removal. The API does not soft delete; the resource is permanently removed.\n\nErrors such as attempting to delete a non-existent ID should return a 404 Not Found response.\n\nThis operation does not require a request body and does not yield a response body upon success.","summary":"Delete a raw data storage record permanently by ID","parameters":[{"name":"rawDataStorageId","in":"path","description":"Unique identifier of the raw data storage record to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":["admin"],"name":"erase","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}","method":"delete","authorizationRole":"admin","authorizationType":null},{"specification":"This operation retrieves a paginated list of local cache files associated with a given raw data storage record from the political_news_crawler_local_cache_files table. This table holds metadata about local filesystem cache copies of raw crawled political news data, including file paths, file sizes, TTL expiration times, and deletion timestamps. Listing local cache files provides fast retrieval and backup verification for cached raw data. The API supports pagination and filtering by the associated raw data storage ID.\n\nSecurity considerations include ensuring only authorized administrators or backend services access these listings due to potential sensitive file path exposures.\n\nThis operation requires the raw data storage ID as a path parameter and supports optional search and pagination filters in the request body.","description":"Retrieve a paginated list of local cache file metadata records linked to a specific raw data storage record. Each local cache file entry includes details such as the filesystem path, file size, TTL expiration for automatic deletion, and deletion status.\n\nThe response supports pagination to handle potentially large numbers of cached files efficiently.\n\nThis operation is intended for backend management and is not publicly accessible.\n\nErrors in specifying a non-existent raw data storage ID should produce a 404 Not Found error.\n\nRequest body supports filtering and pagination parameters according to the IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.IRequest definition.\n\nResponse includes a paginated collection of local cache file summaries.","summary":"List local cache files for a raw data storage record","parameters":[{"name":"rawDataStorageId","in":"path","description":"Unique identifier of the raw data storage record","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Filtering and pagination parameters for listing local cache files","typeName":"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.IRequest"},"responseBody":{"description":"Paginated list of local cache file summaries linked to the raw data storage","typeName":"IPageIPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ISummary"},"authorizationRoles":["guest"],"name":"index","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/localCacheFiles","method":"patch","authorizationRole":"guest","authorizationType":null},{"specification":"This operation retrieves a single local cache file metadata record by its unique identifier associated with a given raw data storage record. The local cache file entity represents a local filesystem cached copy of raw crawled political news data, including fields for file path, file size, TTL expiration, and deletion timestamp.\n\nAccessing individual local cache files supports management and auditing of cached copies for data recovery and integrity checks.\n\nThis API endpoint requires both the raw data storage ID and local cache file ID as path parameters to uniquely identify the cache file record.\n\nSecurity restrictions limit access to authorized administrative or backend roles.\n\nResponses return detailed information about the local cache file or 404 if no matching record is found.","description":"Retrieve a single local cache file metadata entry associated with a specific raw data storage record by its unique ID. The response includes all critical details of the cached file including local file path, size, TTL expiration, and deletion status.\n\nThis operation supports backend cache management tasks and data integrity verification.\n\nIf the specified IDs do not exist, a 404 response is expected.\n\nThis endpoint provides detailed, actionable metadata for local cached raw data file inspection.","summary":"Retrieve a specific local cache file record by ID","parameters":[{"name":"rawDataStorageId","in":"path","description":"Unique identifier of raw data storage record","schema":{"type":"string","format":"uuid"}},{"name":"localCacheFileId","in":"path","description":"Unique identifier of local cache file record","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed local cache file information","typeName":"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile"},"authorizationRoles":["guest"],"name":"at","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/localCacheFiles/{localCacheFileId}","method":"get","authorizationRole":"guest","authorizationType":null},{"specification":"This operation creates a new local cache file record representing a cached copy of raw crawled political news data stored locally. It requires specifying the raw data storage reference, local file path, file size in bytes, TTL expiration datetime for automated deletion, and creation/update timestamps.\n\nThis API enables backend systems to track and manage cached files alongside the authoritative cloud object storage records.\n\nSecurity considerations limit creation capability to administrative roles to prevent unauthorized cache entries.\n\nProper validation includes verifying the referenced raw data storage ID exists and the TTL expiration is a valid future datetime.\n\nThe response returns the created local cache file record including all specified fields plus generated identifiers.","description":"Create a new local cache file metadata record for a raw data storage entry. The record includes the local file path, file size, TTL expiration datetime, and timestamps.\n\nThis operation facilitates tracking of local cached copies supporting data redundancy and faster access.\n\nStrict access control limits use to admin roles.\n\nValidation ensures payload completeness and data integrity.\n\nErrors include 400 for invalid data and 404 if referenced raw data storage record is not found.\n\nReturns the newly created local cache file record with its unique identifier.","summary":"Create a new local cache file record","parameters":[{"name":"rawDataStorageId","in":"path","description":"Unique identifier of the associated raw data storage record","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Information needed to create a new local cache file record","typeName":"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ICreate"},"responseBody":{"description":"The created local cache file record","typeName":"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile"},"authorizationRoles":["admin"],"name":"create","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/localCacheFiles","method":"post","authorizationRole":"admin","authorizationType":null},{"specification":"This operation updates an existing local cache file record that duplicates the raw crawled data for faster access and resiliency against cloud storage outages. It belongs to the political_news_crawler_local_cache_files table. Users of this API provide the new local file path, file size, TTL expiration timestamp to extend caching retention, and optionally the deletion timestamp to mark soft deletion. This enables management of local copies of raw data with a strict TTL policy of one month, ensuring timely cache expiration and clean-up as per business rules. The operation requires the rawDataStorageId and localCacheFileId path parameters matching the records to be updated.","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/localCacheFiles/{localCacheFileId}","method":"put","summary":"Update a specific local cache file metadata for a raw data storage record","description":"This API endpoint allows updating the metadata of a specific local cache file associated with raw data storage. This endpoint supports modifying key properties such as the local file path (location of the cached file), file size in bytes for accurate storage accounting, the TTL expiration timestamp which determines when the local cache should be deleted, and an optional deleted_at timestamp which marks the record as soft deleted.\n\nUsers must supply valid path parameters identifying the raw data storage and the local cache file for targeting the update. The input body must conform to the properties defined in IPoliticalNewsCrawlerLocalCacheFiles.IUpdate.\n\nSoft deletion is handled by setting the deleted_at timestamp; if null, the cache file record is considered active. This operation enforces TTL policies to manage local cache lifecycle and assists in maintaining data consistency between cloud storage and local caches.\n\nOnly authorized roles (guest) can update these records, ensuring control and security over cache metadata management.\n\nErrors on invalid IDs or permission violations will result in appropriate HTTP error responses.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record","schema":{"type":"string","format":"uuid"}},{"name":"localCacheFileId","description":"Unique identifier of the local cache file record to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Local cache file update payload","typeName":"IPoliticalNewsCrawlerLocalCacheFiles.IUpdate"},"responseBody":{"description":"Updated local cache file record","typeName":"IPoliticalNewsCrawlerLocalCacheFiles"},"authorizationType":null,"authorizationRole":"guest","name":"updateLocalCacheFile"},{"specification":"This operation deletes a specific local cache file record linked to raw data storage. It performs a soft delete by setting a deleted_at timestamp, consistent with the political_news_crawler_local_cache_files table design supporting soft deletion. Users must specify both rawDataStorageId and localCacheFileId path parameters to identify the record to delete. The operation does not require a request body and does not return response body content. The operation is protected by guest role authorization to prevent unauthorized cache record deletions.","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/localCacheFiles/{localCacheFileId}","method":"delete","summary":"Soft delete a specific local cache file record linked to raw data storage","description":"This API endpoint soft deletes a local cache file record associated with raw crawled data by setting the deleted_at timestamp. This marks the record as logically deleted without physically removing the data from the database. Users need to provide the raw data storage identifier and the local cache file identifier in the path parameters to target the deletion.\n\nSoft deletion enables the system to keep historical cache data records for audit and recovery while excluding logically deleted entries from active queries. This helps enforce the TTL deletion policy and supports clean cache lifecycle management.\n\nThe endpoint is accessible only to users with guest privileges to ensure operational security. If the record does not exist or the user lacks permission, the system returns appropriate errors.\n\nNo request or response body is required for this operation.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record","schema":{"type":"string","format":"uuid"}},{"name":"localCacheFileId","description":"Unique identifier of the local cache file record to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","name":"eraseLocalCacheFile"},{"specification":"This operation retrieves a list of processed content records associated with a specific raw data storage record. It supports filtering, pagination, and sorting via the request body of type IPoliticalNewsCrawlerProcessedContent.IRequest. The processed content represents LLM-generated data such as summaries, highlights, or analyses corresponding to raw political news. The API path includes rawDataStorageId as a path parameter to identify the target raw data record. The response returns a paginated list of processed content summaries for client consumption. Public access is allowed without authentication roles to ensure broad API availability for news clients.","path":"/politicalNewsCrawler/rawDataStorage/{rawDataStorageId}/processedContent","method":"patch","summary":"Retrieve filtered, paginated list of processed content for a raw data record","description":"This endpoint allows clients to search and retrieve processed LLM-generated content linked to a specific raw data storage record. The input request body includes filtering and pagination parameters to control the result set. Processed content types include summaries, highlights, and analysis results. Clients use this endpoint to obtain enriched news content derived from raw crawled data.\n\nThe response returns a paginated list of processed content summaries including essential metadata suitable for display purposes. This operation is publicly accessible requiring no authentication.\n\nPath parameter rawDataStorageId identifies the raw data record to which the processed content belongs. Filters in the request body allow clients to target specific content types, date ranges, and pagination preferences.\n\nErrors for invalid IDs or malformed requests will be returned appropriately.\n\nThis operation links directly to the political_news_crawler_processed_content table.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Search criteria and pagination for processed content filtering","typeName":"IPoliticalNewsCrawlerProcessedContent.IRequest"},"responseBody":{"description":"Paginated list of processed content summaries matching criteria","typeName":"IPageIPoliticalNewsCrawlerProcessedContent.ISummary"},"authorizationType":null,"authorizationRole":null,"name":"searchProcessedContent"},{"specification":"This operation retrieves detailed information for a specific processed content record linked to a raw data storage entry. It requires both rawDataStorageId and processedContentId path parameters identifying the exact processed content item. The response returns full entity details including the content body, generation timestamp, and related metadata. This detailed view supports client applications displaying extended processed news content. The operation is publicly accessible requiring no authorization roles, promoting open data consumption.","path":"/politicalNewsCrawler/rawDataStorage/{rawDataStorageId}/processedContent/{processedContentId}","method":"get","summary":"Retrieve detailed processed content information by raw data and content IDs","description":"This API endpoint fetches complete details of a specified processed content item associated with raw crawled data. It requires valid path parameters to identify the raw data record and the processed content record individually.\n\nThe response includes comprehensive information such as the full text of the processed content, content type (summary, highlight, analysis), generation timestamp, and audit timestamps. This detailed data supports rich client display and further data processing.\n\nThe endpoint is publicly accessible with no authentication requirements.\n\nErrors such as missing or invalid IDs are handled via standard HTTP error codes.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record","schema":{"type":"string","format":"uuid"}},{"name":"processedContentId","description":"Unique identifier of the processed content record","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed processed content information","typeName":"IPoliticalNewsCrawlerProcessedContent"},"authorizationType":null,"authorizationRole":null,"name":"atProcessedContent"},{"specification":"This operation creates new processed content for a specific raw data storage record in the politicalNewsCrawler backend system. It accepts content type and the full content body generated by LLM post-processing and links the processed content to the raw data storage entity. This functionality enables the efficient addition of summaries, highlights, or analytical content associated with raw crawled political news data. The operation interfaces with the political_news_crawler_processed_content table in the Prisma schema to insert new records with appropriate metadata timestamps.","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/processedContent","method":"post","summary":"Create processed content for a specific raw data storage record","description":"Create a new processed content entry linked to a specified raw data storage record. This endpoint allows clients to submit new LLM-generated content such as summaries, highlights, or analyses for political news data.\n\nThe operation requires the rawDataStorageId path parameter to identify the raw data record the content belongs to.\n\nUsers must provide the content_type to specify the nature of the content (e.g., summary, highlight, analysis) and the content_body containing the actual text content.\n\nUpon successful creation, the new processed content entity including its unique ID, timestamps, and linkage to the raw data storage is returned.\n\nSecurity considerations: This operation accepts public (guest) access as per the project's role definitions.\n\nThis operation is associated with the political_news_crawler_processed_content database table, which stores processed textual content generated by LLM models linked to raw crawl data.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record to link processed content","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Processed content creation payload","typeName":"IPoliticalNewsCrawlerProcessedContent.ICreate"},"responseBody":{"description":"Created processed content record","typeName":"IPoliticalNewsCrawlerProcessedContent"},"authorizationType":null,"authorizationRole":"guest","name":"createProcessedContent"},{"specification":"This operation updates an existing processed content entry associated with a specific raw data storage record. It allows modification of content type and content body fields for political news post-processing results managed within the politicalNewsCrawler system. The operation accepts processed content ID and raw data storage record ID as path parameters to identify the target resource.","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/processedContent/{processedContentId}","method":"put","summary":"Update processed content for a given raw data storage record","description":"Update details of a processed content entry for a specific raw data storage record. The endpoint requires both rawDataStorageId and processedContentId path parameters to precisely identify the record to be updated.\n\nClients can modify fields such as content_type and content_body to correct or enhance the processed textual contents derived from LLM processing.\n\nProper user authentication is not required for this operation as per project role definitions.\n\nThis operation accesses the political_news_crawler_processed_content table for persistent updates.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record linked to the processed content","schema":{"type":"string","format":"uuid"}},{"name":"processedContentId","description":"Unique identifier of the processed content record to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Processed content update payload","typeName":"IPoliticalNewsCrawlerProcessedContent.IUpdate"},"responseBody":{"description":"Updated processed content record","typeName":"IPoliticalNewsCrawlerProcessedContent"},"authorizationType":null,"authorizationRole":"guest","name":"updateProcessedContent"},{"specification":"This operation deletes a processed content record linked to a specific raw data storage identifier. It permanently removes the processed content from the politicalNewsCrawler system database. The operation is performed via path parameters identifying the raw data storage and the processed content to remove.\n\nNo request body is required. The platform user initiating this deletion does not require authentication as per system role definitions.\n\nThis operation interacts with the political_news_crawler_processed_content table, permanently erasing the specified record.","path":"/politicalNewsCrawler/guest/rawDataStorage/{rawDataStorageId}/processedContent/{processedContentId}","method":"delete","summary":"Delete a processed content record permanently","description":"Delete (erase) a processed content record associated with a given raw data storage item identified by rawDataStorageId and processedContentId. This operation permanently removes the processed content data and cannot be undone.\n\nClients do not need authentication to delete processed content as per current system roles.\n\nThis action does not accept a request body and does not return a response body.\n\nIt directly removes the referenced record within the political_news_crawler_processed_content table.","parameters":[{"name":"rawDataStorageId","description":"Unique identifier of the raw data storage record linked to processed content","schema":{"type":"string","format":"uuid"}},{"name":"processedContentId","description":"Unique identifier of the processed content record to be deleted","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","name":"eraseProcessedContent"},{"specification":"Retrieve a paginated list of LLM jobs related to the politicalNewsCrawler system. This operation offers filtering, sorting, and pagination capabilities. It is used to query the history and status of LLM post-processing jobs that generate textual content from raw political news data.\n\nThis operation is mapped to the political_news_crawler_llm_jobs Prisma table, which stores asynchronous processing jobs with statuses and parameters.","path":"/politicalNewsCrawler/llmJobs","method":"patch","summary":"List and search LLM post-processing jobs with filters and pagination","description":"Retrieve a filtered and paginated list of large language model (LLM) jobs in the politicalNewsCrawler backend. This endpoint supports query parameters for filtering jobs by status, creation date, and related crawl source.\n\nThe operation returns a pageable list of job summaries including job ID, status, parameters, and timestamps.\n\nThis is a read-only public endpoint that allows monitoring of LLM processing activities.\n\nNo authentication required for this endpoint.\n\nIt is primarily used for administrative or monitoring purposes to track asynchronous processing status and history.","parameters":[],"requestBody":{"description":"Filtering and pagination request for LLM jobs","typeName":"IPoliticalNewsCrawlerLlmJobs.IRequest"},"responseBody":{"description":"Paginated list of LLM job summaries","typeName":"IPageIPoliticalNewsCrawlerLlmJobs.ISummary"},"authorizationType":null,"authorizationRole":null,"name":"indexLlmJobs"},{"specification":"This operation retrieves the detailed information of a specific LLM (Large Language Model) job by its unique identifier. The LLM job entity represents individual processing tasks related to political news data, including status, parameters, creation, and update timestamps, and soft deletion state. This GET operation enables clients to obtain the current state and configuration of the specified LLM job from the political_news_crawler_llm_jobs table in the database.","description":"Retrieve detailed information about a specific LLM job identified by the given UUID. The LLM job entity tracks processing tasks for political news data, including job status such as 'pending', 'running', 'completed', or 'failed'. It also stores the JSON-formatted parameters or prompts used for the job.\n\nSecurity considerations: The endpoint is publicly accessible without role restrictions as per system design; no authentication or authorization is required.\n\nThis operation relates to the political_news_crawler_llm_jobs table in the Postgres database schema, ensuring full coverage of that core entity responsible for large language model processing tasks.\n\nValidation: The UUID parameter must strictly comply with the UUID format.\n\nUsage: This endpoint is typically used by clients or monitoring services to track the status and details of ongoing or completed LLM processing jobs.\n\nError Handling: Accessing a non-existent LLM job ID should return an appropriate error response indicating the resource was not found.","summary":"Retrieve specific LLM job information by ID","parameters":[{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"Identifier of the LLM job"}],"requestBody":null,"responseBody":{"description":"Detailed information of the requested LLM job","typeName":"IPoliticalNewsCrawlerLlmJobs"},"authorizationType":null,"authorizationRole":null,"name":"at","path":"/politicalNewsCrawler/llmJobs/{id}","method":"get"},{"specification":"This operation creates a new LLM job record for processing political news data. The LLM job entity includes the crawl source reference, status, JSON parameters, and timestamps. The client must provide the creation information following the IPoliticalNewsCrawlerLlmJobs.ICreate schema, which includes the source ID, status, and parameter details.\n\nOnce created, the LLM job status will typically be 'pending' until processed.\n\nThis operation is exposed without authorization restrictions to allow system components or external clients to add processing jobs.\n\nThe operation corresponds to the political_news_crawler_llm_jobs table in the database, enabling insertion of new asynchronous LLM processing tasks.\n\nValidation and business logic ensure all mandatory fields are present and correctly formatted.","description":"Create a new LLM job record to enqueue political news data processing tasks for LLM post-processing. Input must include crawl source ID, job status, and parameters in JSON string format.\n\nSecurity considerations: The endpoint is publicly accessible without restrictions.\n\nThis operation affects the political_news_crawler_llm_jobs table and enables clients or system components to add new processing jobs.\n\nValidation: Input must conform to the IPoliticalNewsCrawlerLlmJobs.ICreate schema with UUID and status constraints.\n\nExpected Behavior: Upon successful creation, the new job record is returned with its assigned unique identifier.","summary":"Create a new LLM job","parameters":[],"requestBody":{"description":"Information needed to create the LLM job","typeName":"IPoliticalNewsCrawlerLlmJobs.ICreate"},"responseBody":{"description":"The created LLM job entity","typeName":"IPoliticalNewsCrawlerLlmJobs"},"authorizationType":null,"authorizationRole":null,"name":"create","path":"/politicalNewsCrawler/llmJobs","method":"post"},{"specification":"This operation updates an existing LLM job record identified by its unique ID. The LLM job entity allows updating of job status, parameters, and timestamps according to the IPoliticalNewsCrawlerLlmJobs.IUpdate schema.\n\nThis is a full update operation (PUT) replacing the entity's mutable fields.\n\nThe endpoint interacts with the political_news_crawler_llm_jobs table in the database.\n\nSecurity: No authorization roles are required; the endpoint is publicly accessible.\n\nValidation: The ID path parameter must be a valid UUID, and the request body must satisfy the IUpdate schema requirements.\n\nError scenario: Updating a non-existent ID should return an error indicating resource not found.","description":"Update an existing LLM job identified by its ID with new status, parameters, or other mutable information. The request body must conform to the IPoliticalNewsCrawlerLlmJobs.IUpdate schema.\n\nThis operation supports management and correction of LLM job records.\n\nIt relates directly to the political_news_crawler_llm_jobs table.\n\nSecurity considerations: No authentication or authorization required.\n\nValidation: Ensures UUID format for path parameter and schema compliance for request body.\n\nError handling: Non-existent resources result in not found errors.","summary":"Update an existing LLM job by ID","parameters":[{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"ID of the LLM job to update"}],"requestBody":{"description":"Updated information for the LLM job","typeName":"IPoliticalNewsCrawlerLlmJobs.IUpdate"},"responseBody":{"description":"The updated LLM job entity","typeName":"IPoliticalNewsCrawlerLlmJobs"},"authorizationType":null,"authorizationRole":null,"name":"update","path":"/politicalNewsCrawler/llmJobs/{id}","method":"put"},{"specification":"This operation deletes an existing LLM job record permanently from the database by its ID. The political_news_crawler_llm_jobs table does not indicate soft delete capability for this entity, so this operation performs a hard delete.\n\nThis endpoint is publicly accessible without authorization requirements.\n\nValidation requires the ID path parameter to be a valid UUID.\n\nErrors for non-existent resource IDs should return not found status.\n\nUsage: This operation removes processing jobs that are no longer needed or were created in error.","description":"Delete an LLM job by its unique identifier. This operation permanently removes the LLM job record from the database.\n\nSecurity considerations: No authorization required.\n\nRelates to the political_news_crawler_llm_jobs table.\n\nValidation: Requires a UUID path parameter.\n\nError handling: Non-existent IDs result in appropriate not found responses.","summary":"Delete an LLM job by ID","parameters":[{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"ID of the LLM job to delete"}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":null,"name":"erase","path":"/politicalNewsCrawler/llmJobs/{id}","method":"delete"},{"specification":"This operation retrieves a paginated list of LLM job results associated with a specific LLM job. It operates on the political_news_crawler_llm_results table in the Prisma schema. The results correspond to generated content such as summaries or analyses related to the specified LLM job, supporting pagination, filtering, and sorting capabilities for efficient content retrieval.","description":"Retrieve a filtered and paginated list of LLM results generated from the specified LLM job. This operation allows clients to query processed content outputs including summaries, highlights, and analysis produced by the large language model for political news data. The results are scoped to the LLM job identified by the llmJobId path parameter.\n\nThe operation respects user access constraints and returns results with essential content metadata, including content type and timestamps. Pagination and sorting options enable flexible client-side querying.\n\nThis endpoint interacts directly with the political_news_crawler_llm_results table, ensuring that only results belonging to the given LLM job are included. It is primarily a read-only operation supporting data browsing by authorized clients.\n\nClients should first retrieve the list using this endpoint before accessing details of specific results via the detailed get operation. Proper error handling should cover cases where the LLM job does not exist or has no associated results.","summary":"Fetch paginated list of LLM results for a given LLM job","parameters":[{"name":"llmJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the target LLM job"}],"requestBody":{"description":"Search criteria and pagination parameters for LLM results filtering","typeName":"IPoliticalNewsCrawlerLlmJobResult.IRequest"},"responseBody":{"description":"Paginated list of LLM results for the specified LLM job","typeName":"IPageIPoliticalNewsCrawlerLlmJobResult"},"authorizationType":null,"authorizationRole":"guest","name":"index","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/results","method":"patch"},{"specification":"This operation retrieves detailed information for a specific LLM job result by its unique identifier. It accesses the political_news_crawler_llm_results table in the Prisma schema to provide full content details including content type, generated content body, generation timestamp, and audit information.","description":"Retrieve detailed information of a specific LLM job result identified by its unique ID. This provides comprehensive insight into the processed content generated by large language model post-processing tasks for political news data. It includes content metadata, textual content, generation timestamp, and audit timestamps.\n\nThe path parameter llmJobId references the parent LLM job, ensuring context and scope. Authorization restricts access to authenticated users who have permissions to view this detailed processed content.\n\nThis endpoint is typically used after fetching a list of LLM results to obtain full details for display or further analysis.","summary":"Retrieve detailed information of a specific LLM job result","parameters":[{"name":"llmJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the target LLM job"},{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the LLM job result"}],"requestBody":null,"responseBody":{"description":"Detailed LLM job result information","typeName":"IPoliticalNewsCrawlerLlmJobResult"},"authorizationType":null,"authorizationRole":"guest","name":"at","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/results/{id}","method":"get"},{"specification":"This operation creates a new LLM job result entry associated with a specified LLM job. It inserts processed content data such as summary, highlight, or analysis into the political_news_crawler_llm_results table. The operation accepts content type and content text for storage and links the result to the parent LLM job.","description":"Create a new LLM job result for the given LLM job. This operation stores processed textual content generated by the large language model, categorized by content type such as summaries, highlights, or analysis. It accepts the content payload along with metadata linking to the parent LLM job.\n\nPost-creation, clients typically retrieve the new result's details via the detailed get operation. The LLM job identified by llmJobId must exist before successful creation. Validation rules enforce content type adherence and text length constraints.\n\nAuthorization restricts this operation to authenticated users with permission to add processed content results.","summary":"Create a new LLM job result","parameters":[{"name":"llmJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the target LLM job"}],"requestBody":{"description":"Information required to create an LLM job result","typeName":"IPoliticalNewsCrawlerLlmJobResult.ICreate"},"responseBody":{"description":"Details of the newly created LLM job result","typeName":"IPoliticalNewsCrawlerLlmJobResult"},"authorizationType":null,"authorizationRole":"guest","name":"create","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/results","method":"post"},{"specification":"This operation updates an existing LLM job result identified by its unique ID under a specified LLM job. It modifies processed content such as summaries or analysis stored in the political_news_crawler_llm_results table. The operation allows updating of content type and content text fields, preserving links to the parent LLM job.","description":"Update an existing LLM job result specified by its ID for the given LLM job. This endpoint modifies textual processed content, which may include summaries, highlights, or political news analyses generated by LLM processing tasks.\n\nThe operation supports validation of input content type and ensures that the linked LLM job and result exist prior to update. Authorization check ensures only permitted users can modify these processed results.\n\nSuccessful updates return the updated full resource representation.","summary":"Update an existing LLM job result","parameters":[{"name":"llmJobId","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the target LLM job"},{"name":"id","in":"path","schema":{"type":"string","format":"uuid"},"description":"Unique identifier of the LLM job result"}],"requestBody":{"description":"Information required to update an LLM job result","typeName":"IPoliticalNewsCrawlerLlmJobResult.IUpdate"},"responseBody":{"description":"Details of the updated LLM job result","typeName":"IPoliticalNewsCrawlerLlmJobResult"},"authorizationType":null,"authorizationRole":"guest","name":"update","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/results/{id}","method":"put"},{"specification":"This operation deletes a specific LLM result identified by its unique ID and associated with a particular LLM job, removing the record from the political_news_crawler_llm_results table. It is intended to support administrative or maintenance functionality to clean up or remove specific processed content generated by LLM jobs in the politicalNewsCrawler backend.","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/results/{id}","method":"delete","summary":"Delete a specific LLM result by LLM job ID and result ID","description":"This API endpoint allows authorized users to permanently delete a specific LLM generated result record associated with a given LLM job. The resource is identified by the path parameters llmJobId and id corresponding to the LLM job and the result respectively.\n\nThis deletion operation permanently removes the record from political_news_crawler_llm_results table and cannot be undone.\n\nOnly public access (guest role) is granted as per system design.\n\nNo request body is needed as parameters in the path fully specify the target resource.","parameters":[{"name":"llmJobId","in":"path","description":"ID of the associated LLM job","schema":{"type":"string"}},{"name":"id","in":"path","description":"ID of the LLM result to delete","schema":{"type":"string"}}],"requestBody":null,"responseBody":null,"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"erase"},{"specification":"This operation retrieves the list of metadata entries associated with the specified LLM job from the political_news_crawler_processing_metadata table. Metadata entries describe auxiliary information about the processing job, such as parameters, execution context, or other relevant attributes.\n\nThis endpoint allows querying all metadata entries linked to a given LLM job ID.\n\nThe response contains an array of metadata objects. This endpoint is useful for debugging, job management, and understanding LLM processing parameters.\n\nDocumentation references:\n- political_news_crawler_llm_jobs\n- political_news_crawler_processing_metadata","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/metadata","method":"get","summary":"Retrieve metadata list for an LLM job","description":"Retrieve the list of metadata associated with a particular LLM job. This metadata includes key/value pairs that provide additional information about the LLM processing context and parameters used.\n\nOnly public access (guest role) as per system design.\n\nThe request uses no body but uses the LLM job ID as a path parameter.\n\nPagination or filtering may be implemented in the future.\n\nResponse contains an array of metadata entries with fields such as metadata_key and metadata_value.","parameters":[{"name":"llmJobId","in":"path","description":"ID of the target LLM job","schema":{"type":"string"}}],"requestBody":null,"responseBody":{"description":"List of processing metadata entries for the LLM job","typeName":"IPoliticalNewsCrawlerProcessingMetadataArray"},"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"index"},{"specification":"Creates a new metadata entry associated with a specified LLM job in the political_news_crawler_processing_metadata table. This metadata captures key-value pairs representing auxiliary processing information related to the LLM job.\n\nThis endpoint allows adding a single metadata record by linking it to the LLM job identified by the llmJobId path parameter.\n\nIt requires a request body with metadata_key and metadata_value properties.\n\nThe newly created metadata record is returned in the response.\n\nReferences:\n- political_news_crawler_llm_jobs\n- political_news_crawler_processing_metadata","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/metadata","method":"post","summary":"Create new metadata record for LLM job","description":"Create a new processing metadata record associated with the specified LLM job. Metadata records consist of key-value pairs representing additional context or parameters for the LLM processing.\n\nThe LLM job ID is specified as a path parameter.\n\nThe request body must include metadata_key and metadata_value.\n\nUpon success, the full metadata record including creation timestamps is returned.\n\nThe API is publicly accessible (guest role) according to system design.","parameters":[{"name":"llmJobId","in":"path","description":"ID of the target LLM job","schema":{"type":"string"}}],"requestBody":{"description":"Metadata creation data","typeName":"IPoliticalNewsCrawlerProcessingMetadataICreate"},"responseBody":{"description":"Created metadata record","typeName":"IPoliticalNewsCrawlerProcessingMetadata"},"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"create"},{"specification":"Update metadata entries related to a given LLM job. Typically used to modify multiple metadata attributes in batch for a specific LLM job.\n\nThis endpoint accepts a request body containing an array of metadata update entries. Each entry should include the ID of the metadata record and the new values for metadata_key and metadata_value.\n\nReturns a list of updated metadata records after processing.\n\nThis operation maintains data consistency and auditability for LLM processing metadata.\n\nReferences:\n- political_news_crawler_llm_jobs\n- political_news_crawler_processing_metadata","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/metadata","method":"patch","summary":"Update processing metadata records for an LLM job","description":"Update one or multiple processing metadata records associated with the specified LLM job. This operation supports partial or full updates and batch modifications.\n\nUses the LLM job ID as a path parameter.\n\nThe request body should contain an array of metadata entries with their IDs and updated key/value pairs.\n\nReturns the updated list of metadata records.\n\nThis API is accessible publicly under guest role as per system design.","parameters":[{"name":"llmJobId","in":"path","description":"ID of the target LLM job","schema":{"type":"string"}}],"requestBody":{"description":"Metadata update data","typeName":"IPoliticalNewsCrawlerProcessingMetadataIUpdateArray"},"responseBody":{"description":"Updated metadata records","typeName":"IPoliticalNewsCrawlerProcessingMetadataArray"},"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"update"},{"specification":"This operation updates a metadata record for a specific LLM job in the political_news_crawler_processing_metadata table. It accepts the LLM job ID and metadata record ID as path parameters and a request body with the updated key-value pair. It returns the updated metadata record upon success.","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/metadata/{id}","method":"put","summary":"Update a specific LLM job metadata record","description":"This endpoint allows client to update the key-value metadata information for a particular LLM processing job. This operation requires authentication with the role 'guest'. Only the specified metadata record is updated with new key and value provided in the request body.\n\nThis metadata is crucial for enriching the LLM job processing context and supporting downstream analysis.\n\nThe metadata belongs to the political_news_crawler_processing_metadata model, ensuring key uniqueness for the associated LLM job.\n\nProper validation ensures keys and values are appropriately updated.","parameters":[{"name":"llmJobId","in":"path","description":"Target LLM job's ID","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Metadata record ID to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Updated metadata key-value pair","typeName":"IPoliticalNewsCrawlerProcessingMetadata.IUpdate"},"responseBody":{"description":"Updated LLM job metadata record","typeName":"IPoliticalNewsCrawlerProcessingMetadata"},"authorizationType":null,"authorizationRole":"guest","name":"updateMetadata"},{"specification":"This operation deletes a metadata record identified by ID for a specific LLM job in the political_news_crawler_processing_metadata table. It requires the LLM job ID and metadata record ID as path parameters and does not require a request body or return a response body. The deletion is permanent.","path":"/politicalNewsCrawler/guest/llmJobs/{llmJobId}/metadata/{id}","method":"delete","summary":"Delete a specific LLM job metadata record","description":"This endpoint permanently removes a metadata record associated with a given LLM job. Authorization with the 'guest' role is required to perform this delete operation. The operation ensures that the specified metadata entry is erased from the system, cleaning up auxiliary processing information as necessary.\n\nThe operation directly affects the political_news_crawler_processing_metadata model and is critical for metadata lifecycle management.","parameters":[{"name":"llmJobId","in":"path","description":"Target LLM job's ID","schema":{"type":"string","format":"uuid"}},{"name":"id","in":"path","description":"Metadata record ID to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","name":"eraseMetadata"},{"specification":"This operation retrieves a paginated and filtered list of popular political topics from the political_news_crawler_popular_topics table. Supports filtering by title keyword and sorting by creation date or popularity score. This endpoint is publicly accessible without authorization.","path":"/politicalNewsCrawler/popularTopics","method":"patch","summary":"Retrieve a list of popular political topics with filters and pagination","description":"Retrieve a filtered and paginated list of popular political topics. Supports filtering parameters for title keywords and sorting by creation dates or popularity metrics.\n\nThis API endpoint is public and requires no authentication.\n\nData is sourced from the political_news_crawler_popular_topics model, ensuring relevance, currency, and accuracy.\n\nSupports efficient pagination and search to facilitate frontend consumption.","parameters":[],"requestBody":{"description":"Search and pagination parameters for popular topics","typeName":"IPoliticalNewsCrawlerPopularTopics.IRequest"},"responseBody":{"description":"Paginated popular topics matching search criteria","typeName":"IPageIPoliticalNewsCrawlerPopularTopics"},"authorizationType":null,"authorizationRole":null,"name":"searchPopularTopics"},{"specification":"This operation retrieves detailed information by ID for a popular political topic from the political_news_crawler_popular_topics table. This endpoint is publicly accessible with no authorization requirement.","path":"/politicalNewsCrawler/popularTopics/{id}","method":"get","summary":"Retrieve a specific popular political topic by ID","description":"Get detailed information for a single popular political topic by specifying its unique ID in the path parameter.\n\nThe response includes topic code, title, description, and timestamps.\n\nThis API endpoint is public and requires no authentication.\n\nSupports frontend client needs for detailed topic information display.","parameters":[{"name":"id","in":"path","description":"Unique identifier of the popular political topic","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed popular political topic information","typeName":"IPoliticalNewsCrawlerPopularTopics"},"authorizationType":null,"authorizationRole":null,"name":"atPopularTopic"},{"specification":"This operation creates a new popular topic record in the political_news_crawler_popular_topics table. It accepts the topic's unique code, title, and optional description. The table stores current political news topics with computed popularity rankings, ensuring each topic code is unique. This creation enables the topic to be tracked, scored, and referenced by related popularity score snapshots and topic mentions.","description":"Creates a new popular political topic within the system. Users can specify a unique topic code and title to identify trending or important political themes. An optional description provides further context or details about the topic. This operation inserts a new row into political_news_crawler_popular_topics which serves as the primary entity representing popular political topics.\n\nSecurity considerations require that only authorized parties (e.g., system administrators) perform this operation to prevent duplicates or invalid entries. Validation ensures topic_code uniqueness and title presence.\n\nThis operation integrates with related tables: popularity_scores and topic_mentions, which track dynamic scoring and article references respectively. Creating a topic is a prerequisite for recording its popularity and news mentions.\n\nExpected behavior includes returning the full created topic entity upon successful creation. Errors may arise from duplicate topic codes or validation failures.","summary":"Create new popular political topic","parameters":[],"requestBody":{"description":"Creation info of the popular topic","typeName":"IPoliticalNewsCrawlerPopularTopic.ICreate"},"responseBody":{"description":"Created popular topic information","typeName":"IPoliticalNewsCrawlerPopularTopic"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/popularTopics","method":"post","name":"create"},{"specification":"This operation updates an existing popular topic in the political_news_crawler_popular_topics table. It identifies the topic by its unique ID and provides updated values for topic_code, title, and description. This helps maintain accurate and current topic information within the system's popularity tracking framework.","description":"Updates an existing popular political topic by ID. Allows modification of the topic code, title, and optional description to reflect changes in political themes or corrections to entries.\n\nSecurity measures mandate authorized access to prevent unauthorized modifications. Validation includes ensuring continued uniqueness of topic_code if changed.\n\nThis operation affects the main popular_topics entity and indirectly impacts associated popularity scores and topic mentions. Successful updates return the updated topic entity. In case of invalid ID or concurrency issues, appropriate errors are returned.","summary":"Update popular political topic by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the popular political topic to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update info for the popular topic","typeName":"IPoliticalNewsCrawlerPopularTopic.IUpdate"},"responseBody":{"description":"Updated popular topic information","typeName":"IPoliticalNewsCrawlerPopularTopic"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/popularTopics/{id}","method":"put","name":"update"},{"specification":"This operation deletes an existing popular topic from the political_news_crawler_popular_topics table by its unique ID. Since the schema does not explicitly define soft delete columns for this entity, this operation performs a hard delete, permanently removing the record and all associations from the database.","description":"Deletes a popular political topic identified by ID. This performs a permanent removal of the topic record from the database, including removal of all associated popularity scores and topic mentions due to foreign key constraints.\n\nSecurity constraints require administrative privileges to avoid accidental data loss. This operation cannot be undone; use with caution.\n\nErrors are returned if the ID does not exist or foreign key constraints prevent deletion.\n\nThis operation allows system administrators to clean up outdated or invalid topics.","summary":"Delete popular political topic by ID (hard delete)","parameters":[{"name":"id","in":"path","description":"Unique identifier of the popular political topic to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/popularTopics/{id}","method":"delete","name":"erase"},{"specification":"This operation retrieves a paginated list of popularity score snapshots associated with a specific popular topic identified by popularTopicId. The political_news_crawler_popularity_scores table contains time-stamped snapshots of calculated popularity scores for topics, including decay factors for time-based ranking models. This endpoint facilitates analysis and trend tracking of the popularity evolution of a political topic over time.","description":"Retrieve a paginated list of popularity score snapshots for a given popular topic ID. Provides historical and current popularity scores with decay factors applied based on snapshot timestamps.\n\nSecurity considerations include read-only access for authenticated users, ensuring information is not exposed to unauthorized parties.\n\nThis operation links directly to the primary topic entity and supports UI features like time-trend charts or score evolution tracking. Results are paginated to manage potentially large histories.\n\nExpected behavior includes returning summary entries with score, decay, and snapshot time, ordered by snapshot_at descending.","summary":"List popularity score snapshots for a popular topic","parameters":[{"name":"popularTopicId","in":"path","description":"Identifier of the popular topic whose popularity scores are queried","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Request parameters for popularity scores list with filtering, sorting, and pagination","typeName":"IPoliticalNewsCrawlerPopularityScore.IRequest"},"responseBody":{"description":"Paginated list of popularity score snapshots","typeName":"IPageIPoliticalNewsCrawlerPopularityScore.ISummary"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/popularTopics/{popularTopicId}/popularityScores","method":"patch","name":"index"},{"specification":"Retrieve detailed information for a specific popularity score record tied to a specific popular political topic within the political_news_crawler_popularity_scores table. This endpoint operates on the popularity score snapshots associated with political topics, allowing any caller to get the score, decay factor, and timestamps for the specified score record by IDs. It includes full against the primary key (id) of the score and the foreign key reference to the popular topic. No authentication or user roles are required for access.","description":"This operation returns detailed information about a specific popularity score snapshot for a popular political topic. Users can retrieve the calculated popularity score and decay factor applied at a certain snapshot time, enabling insight into topic trend changes.\n\nSecurity and accessibility are unrestricted; this is a public endpoint allowing read-only access. The response returns all stored properties, including creation and update timestamps.\n\nUnderlying data is stored in political_news_crawler_popularity_scores referencing political_news_crawler_popular_topics, thus reflecting a one-to-many relationship (one topic to many scores).\n\nErrors include invalid ID formats or non-existing records, typically resulting in 404 not found responses.\n\nThis GET endpoint does not require a request body and uniquely identifies the score record by both popularTopicId and id path parameters.","summary":"Retrieve a specific popularity score snapshot for a popular political topic","parameters":[{"name":"popularTopicId","in":"path","schema":{"type":"string"},"description":"Target popular topic's ID"},{"name":"id","in":"path","schema":{"type":"string"},"description":"Specific popularity score record ID"}],"requestBody":null,"responseBody":{"description":"Detailed popularity score snapshot information for the specified popular topic","typeName":"IPoliticalNewsCrawlerPopularityScores"},"authorizationType":null,"authorizationRole":null,"name":"at","path":"/politicalNewsCrawler/popularTopics/{popularTopicId}/popularityScores/{id}","method":"get"},{"specification":"Search and retrieve a paginated list of topic mentions associated with a specific popular topic. Provides filtering, sorting, and pagination capabilities to effectively query mentions within political_news_crawler_topic_mentions. Operates under the popularTopicId foreign key reference.\n\nEnables clients to discover contexts where a popular topic was mentioned within various crawled news articles.\n\nSupports advanced search criteria in the request body (e.g., mention_context, created_at ranges).\n\nThis operation returns a paginated list of topic mention summary information with essential properties optimized for efficient data transfer.\n\nThis endpoint is publicly accessible, requiring no authentication roles.","description":"Retrieve a list of topic mentions related to a specific popular political topic. This operation accepts complex search criteria for filtering and pagination to manage potentially large sets of mentions.\n\nThis list enables clients to explore the contexts and individual news items where the topic was referenced.\n\nThe response includes summary information per mention, including mention context snippets when available.\n\nNo authentication or restriction is applied to this read-only endpoint.\n\nThis PATCH endpoint requires a request body specifying search criteria and returns a paginated list with essential summary data.","summary":"Search and retrieve topic mentions for a popular topic","parameters":[{"name":"popularTopicId","in":"path","schema":{"type":"string"},"description":"Target popular topic's ID"}],"requestBody":{"description":"Search criteria and pagination parameters for topic mentions filtering","typeName":"IPoliticalNewsCrawlerTopicMentions.IRequest"},"responseBody":{"description":"Paginated list of topic mention summaries matching search criteria","typeName":"IPageIPoliticalNewsCrawlerTopicMentions.ISummary"},"authorizationType":null,"authorizationRole":null,"name":"index","path":"/politicalNewsCrawler/popularTopics/{popularTopicId}/topicMentions","method":"patch"},{"specification":"Retrieve detailed information about a specific topic mention record related to a popular political topic. Operates on the political_news_crawler_topic_mentions table by unique mention ID and linked popular topic ID.\n\nThis functionality allows viewing the mention context and associated metadata.\n\nThe endpoint is publicly accessible with no required authentication.\n\nErrors include invalid IDs or missing records resulting in 404 status.\n\nThe GET method retrieves a single resource without request body, returning full entity details.","description":"Get detailed data on a particular topic mention for a popular political topic. Provides the mention context snippet and metadata for understanding the mention occurrence.\n\nThis is a read-only public operation without authentication.\n\nThe response returns all properties of the topic mention record, including creation and update timestamps.","summary":"Retrieve a single topic mention detail","parameters":[{"name":"popularTopicId","in":"path","schema":{"type":"string"},"description":"Target popular topic's ID"},{"name":"id","in":"path","schema":{"type":"string"},"description":"Target topic mention record ID"}],"requestBody":null,"responseBody":{"description":"Detailed topic mention data","typeName":"IPoliticalNewsCrawlerTopicMentions"},"authorizationType":null,"authorizationRole":null,"name":"at","path":"/politicalNewsCrawler/popularTopics/{popularTopicId}/topicMentions/{id}","method":"get"},{"specification":"Update the specified topic mention record tied to a popular political topic ID. Enables modifying fields such as mention_context to refine the textual snippet describing the mention instance.\n\nThis operation validates input fields according to the political_news_crawler_topic_mentions table structure.\n\nOperates as a PUT method to perform full update of the mention record.\n\nAccess is unrestricted, consistent with the public reading endpoints. All changes are saved and returned in the response.\n\nErrors include validation failures and non-existing record IDs returning error statuses.\n\nRequest body uses IPublicNewsCrawlerTopicMentions.IUpdate schema. Response returns the updated complete entity data.","description":"Update an existing topic mention record associated with a popular topic. Modifiable fields typically include mention context text.\n\nThis PUT endpoint requires a full update payload matching the entity update schema.\n\nNo authentication or role is required as the endpoints are publicly open read/write.\n\nUpon success, the updated entity is returned.","summary":"Update a topic mention record for a popular topic","parameters":[{"name":"popularTopicId","in":"path","schema":{"type":"string"},"description":"Target popular topic's ID"},{"name":"id","in":"path","schema":{"type":"string"},"description":"Target topic mention record ID"}],"requestBody":{"description":"Updated topic mention data","typeName":"IPoliticalNewsCrawlerTopicMentions.IUpdate"},"responseBody":{"description":"Updated topic mention data","typeName":"IPoliticalNewsCrawlerTopicMentions"},"authorizationType":null,"authorizationRole":null,"name":"update","path":"/politicalNewsCrawler/popularTopics/{popularTopicId}/topicMentions/{id}","method":"put"},{"specification":"This operation deletes a specific topic mention entity from the politicalNewsCrawlerPopularTopics entity related to the popular topics feature. It performs a soft delete by setting the deleted_at timestamp to mark the topic mention as deleted without physically removing the record from the database. The topic mention record is identified by the popularTopicId and id path parameters. This allows logical removal of topic mentions for moderation or data integrity while preserving historical references.\n\nThe operation targets the 'political_news_crawler_topic_mentions' table, which supports soft deletion with a nullable deleted_at field.\n\nNo authentication is required as the API is public, but operational environments may enforce authorization externally. Clients should handle errors gracefully if the mention is already deleted or does not exist.\n\nThis operation is part of the full lifecycle management of popular topic mentions in conjunction with other CRUD endpoints.","description":"This endpoint allows soft deletion of a specific topic mention under a specific popular topic by setting the deleted_at field.\n\nThe 'popularTopicId' identifies the parent popular topic while 'id' identifies the mention to logically remove.\n\nThis operation does not physically delete the record but marks it as deleted for audit and historical integrity.\n\nClients should consider this logical deletion in their data filtering and retrieval.\n\nThe operation complements other endpoints for managing popular topics and mentions, supporting moderation workflows.","summary":"Soft delete a specific topic mention from a popular topic by ID","parameters":[{"name":"popularTopicId","description":"Unique identifier of the popular topic","schema":{"type":"string","format":"uuid"}},{"name":"id","description":"Unique identifier of the topic mention to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":[],"name":"erase","path":"/politicalNewsCrawler/popularTopics/{popularTopicId}/topicMentions/{id}","method":"delete","authorizationRole":null,"authorizationType":null},{"specification":"This operation retrieves a paged list of API access log entries from the 'political_news_crawler_api_access_logs' table. The endpoint supports advanced filtering, sorting, and pagination through the requestBody with IPoliticalNewsCrawlerApiAccessLog.IRequest type. The returned result includes paginated data with summaries of access logs, optimized for analysis of API usage patterns and client monitoring. The endpoint is publicly accessible without authentication, allowing monitoring systems and clients to retrieve access data.\n\nThis operation helps administrators and developers track API usage dynamics, investigate performance issues, and audit request sources without needing full access control. It integrates with other analytics endpoints such as error logs and usage metrics for comprehensive API monitoring.\n\nFiltering supports date ranges, HTTP methods, endpoint paths filtering using search patterns, and client IP/agent constraints. Pagination and sorting provide efficient data access even with large volumes of log entries.","description":"Retrieve a paginated and filtered list of API access log entries.\n\nThis endpoint interacts with the 'political_news_crawler_api_access_logs' table which records detailed logs for every API call.\n\nClients can filter entries using advanced search and pagination parameters included in the request body.\n\nThe search allows narrowing results by HTTP method, path pattern, date range, and other criteria.\n\nThe response includes a paginated list of API access log summaries with metadata designed for analysis and auditing.\n\nThis operation is public and requires no authorization roles.\n\nClients should handle typical pagination and search results because the data volume can be large.","summary":"Search and retrieve a paginated list of API access log entries","parameters":[],"requestBody":{"description":"Search filters and pagination parameters for API access logs","typeName":"IPoliticalNewsCrawlerApiAccessLog.IRequest"},"responseBody":{"description":"Paginated list of API access log summaries matching search criteria","typeName":"IPageIPoliticalNewsCrawlerApiAccessLog.ISummary"},"authorizationRoles":[],"name":"index","path":"/politicalNewsCrawler/api/accessLogs","method":"patch","authorizationRole":null,"authorizationType":null},{"specification":"This operation retrieves a specific API access log entry by ID from the 'political_news_crawler_api_access_logs' table. Used for detailed examination of individual API call information including request method, path, status, client IP, user agent, and timing metrics.\n\nThe resource is publicly accessible without authentication, enabling debugging and support tools to retrieve access details quickly.\n\nThis operation complements the search index endpoint allowing targeted retrieval of single entries.\n\nPath parameters validate the uniqueness of the ID with UUID format.\n\nErrors indicate not found or malformed ID inputs.\n\nReturned data includes the complete access log record as the main entity type IPoliticalNewsCrawlerApiAccessLog.","description":"Retrieve detailed information of a specific API access log entry by its unique ID.\n\nThis endpoint targets individual records in the 'political_news_crawler_api_access_logs' table.\n\nAllows clients to view full details of an access log for diagnostic purposes.\n\nNo authentication or role check is required because this is a public API endpoint.\n\nClients should handle cases where the requested ID does not exist.\n\nThe response includes all fields relevant to the access log data, such as method, path, status code, IP, and user agent.","summary":"Retrieve a specific API access log entry by ID","parameters":[{"name":"id","description":"Unique identifier of the API access log entry","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"API access log entry detailed information","typeName":"IPoliticalNewsCrawlerApiAccessLog"},"authorizationRoles":[],"name":"at","path":"/politicalNewsCrawler/api/accessLogs/{id}","method":"get","authorizationRole":null,"authorizationType":null},{"specification":"This endpoint provides a paginated and filtered search over API error log entries stored in the 'political_news_crawler_api_error_logs' table. It supports query capabilities enabled through a request body of type IPoliticalNewsCrawlerApiErrorLog.IRequest to specify filters such as path patterns, error codes, date ranges, client IPs, and user agents.\n\nThe response includes paginated summaries of error logs optimized for auditing and operational monitoring.\n\nThis endpoint is publicly accessible without authentication, supporting API diagnostics and data analytics tools.\n\nClients must handle pagination and filtering correctly to access large datasets efficiently.\n\nThis operation complements other API monitoring endpoints such as access logs and API usage metrics for thorough system oversight.","description":"Retrieve a paginated and filtered list of API error log entries.\n\nThis endpoint interacts with the 'political_news_crawler_api_error_logs' table which records detailed error log data.\n\nClients use the request body to specify filters and pagination criteria.\n\nThe response includes paginated summaries that aid in identifying and analyzing API error patterns.\n\nPublic access is allowed without any role restrictions.\n\nError entries may highlight issues such as rate limiting, endpoint failures, or spikes in error occurrence.\n\nClients should handle large result sets with efficient paging.","summary":"Search and retrieve a paginated list of API error log entries","parameters":[],"requestBody":{"description":"Filters and pagination parameters for API error logs","typeName":"IPoliticalNewsCrawlerApiErrorLog.IRequest"},"responseBody":{"description":"Paginated list of API error log summaries matching search criteria","typeName":"IPageIPoliticalNewsCrawlerApiErrorLog.ISummary"},"authorizationRoles":[],"name":"index","path":"/politicalNewsCrawler/api/errorLogs","method":"patch","authorizationRole":null,"authorizationType":null},{"specification":"This operation retrieves a specific API error log entry by its unique identifier from the political_news_crawler_api_error_logs table. It allows clients to fetch detailed error information for troubleshooting and analysis of API failures. The operation requires the error log ID to be provided as a path parameter and returns the corresponding error log data with details including path, error code, message, client IP, and user agent.","description":"Retrieve a detailed API error log entry by its unique identifier.\n\nThis operation is designed to fetch detailed information for a single API error event, referenced by its unique ID. It allows administrators or monitoring systems to obtain specifics about a particular error occurrence, including the error code, message, client IP, and user agent.\n\nOnly read access is required, as this operation exposes error logs for diagnostics without modification rights.\n\nThe data corresponds directly to the political_news_crawler_api_error_logs table implementation in the Prisma schema, ensuring all relevant columns are included in the response.\n\nNo request body is needed, and the error log ID is passed as the path parameter. If the ID does not exist, a 404 response can be expected.\n\nThis operation complements other log retrieval functions such as listing all error logs or fetching usage metrics for analysis.","summary":"Retrieve specific API error log by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the API error log entry","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"API error log entry details","typeName":"IPoliticalNewsCrawlerApiErrorLog"},"authorizationType":null,"authorizationRole":null,"name":"at","path":"/politicalNewsCrawler/api/errorLogs/{id}","method":"get"},{"specification":"This operation retrieves aggregate API usage metrics records from the political_news_crawler_api_usage_metrics table, supporting complex filtering, sorting, and pagination. The request body allows specifying search criteria including HTTP method, API path, and time ranges to filter aggregated metric data.\n\nThe operation returns a paginated list of usage metrics summarizing total calls, max and average response times over defined periods, providing insights into API performance and traffic patterns.\n\nThis endpoint is read-only and publicly accessible, facilitating monitoring and analytics for API consumption without authentication requirements.\n\nThe operation supports flexible and complex queries via the PATCH method to enable refined access to usage metrics data for performance monitoring and capacity planning.","description":"Retrieve a filtered and paginated list of API usage metrics with complex search capabilities.\n\nThis operation supports detailed querying of aggregated API usage data, including filtering by HTTP methods, API paths, and time periods. It accepts pagination and sorting parameters to efficiently manage large datasets.\n\nThe data originates from the political_news_crawler_api_usage_metrics table, which records total call counts, maximum, and average response durations over specified aggregation periods.\n\nSecurity and access control are minimal as the endpoint is expected to be publicly accessible for monitoring systems.\n\nThe response returns a paginated collection of summarized API usage metric records.","summary":"Search and retrieve paginated API usage metrics","parameters":[],"requestBody":{"description":"Search criteria and pagination parameters for API usage metrics filtering","typeName":"IPoliticalNewsCrawlerApiUsageMetricRequest"},"responseBody":{"description":"Paginated list of API usage metrics matching search criteria","typeName":"IPageIPoliticalNewsCrawlerApiUsageMetricSummary"},"authorizationType":null,"authorizationRole":null,"name":"index","path":"/politicalNewsCrawler/api/usageMetrics","method":"patch"},{"specification":"This operation retrieves a specific API usage metric record by its unique identifier from the political_news_crawler_api_usage_metrics table.\n\nThis allows detailed inspection of a particular aggregated metric including total calls, max response time, and average response time over the defined time window.\n\nThe request requires the usage metric ID as a path parameter and returns the full usage metric record details.\n\nThe endpoint is publicly accessible and read-only, supporting monitoring and diagnostics without authentication.\n\nThis operation complements the API usage metrics listing by providing detail on individual records.","description":"Retrieve a specific API usage metric record by ID.\n\nThe response includes total calls, max response time, and average response time recorded during the aggregation period.\n\nThis endpoint is public and requires no authentication.\n\nIf the ID is invalid or not found, a 404 error will be returned.\n\nThis operation supports auditing and detailed performance inspection for API usage.","summary":"Retrieve specific API usage metric by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the API usage metric record","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"API usage metric record details","typeName":"IPoliticalNewsCrawlerApiUsageMetric"},"authorizationType":null,"authorizationRole":null,"name":"at","path":"/politicalNewsCrawler/api/usageMetrics/{id}","method":"get"},{"specification":"This operation retrieves crawl alert events from the political_news_crawler_crawl_alerts table, supporting filtering and pagination to monitor crawl failures, bans, or throttle warnings.\n\nThe request body accepts search parameters such as crawl source filters, alert types, severities, and time ranges.\n\nThe response provides paginated crawl alert records that include related crawl source information, alert types, messages, severities, and timestamps.\n\nThis publicly accessible read-only API supports operational monitoring and alerting dashboards to observe crawl health and issues in near real-time.","description":"Retrieve filtered and paginated crawl alert events generated by crawl sources.\n\nFacilitates monitoring of crawl operation health by reporting bans, errors, or warnings.\n\nAccepts search and pagination parameters to manage potentially large alert data volumes.\n\nData is sourced from the political_news_crawler_crawl_alerts table, including alert metadata and associated crawl source references.\n\nSecurity is open for public access, enabling transparent operational insight.\n\nClients can filter alerts by severity, type, and crawl source as needed.\n\nReturns a paginated collection of alert records matching criteria.","summary":"Search and retrieve paginated crawl alert events","parameters":[],"requestBody":{"description":"Search filters and pagination parameters for crawl alerts retrieval","typeName":"IPoliticalNewsCrawlerCrawlAlertRequest"},"responseBody":{"description":"Paginated list of crawl alert records matching filters","typeName":"IPageIPoliticalNewsCrawlerCrawlAlert"},"authorizationType":null,"authorizationRole":null,"name":"index","path":"/politicalNewsCrawler/crawlAlerts","method":"patch"},{"specification":"This operation retrieves a specific crawl alert record from the political_news_crawler_crawl_alerts table by its unique identifier (id). crawl_alerts store operational alert events related to crawling failures, bans, and throttling for political news sources, providing context and severity information essential for monitoring and troubleshooting the crawler subsystem.","description":"Retrieve detailed information about a specific crawl alert by its unique ID. The crawl alert entity logs critical events in the crawling process such as bans, network errors, or throttle warnings related to specific crawl sources. This data helps system operators monitor crawler health and perform diagnostics.\n\nAccess to this endpoint requires appropriate permissions reflecting user roles responsible for viewing operational alerts.\n\nThe response returns all available metadata for the alert including its type, descriptive message, severity level, and timestamps. If the alert has been soft deleted, it will still be retrievable for audit purposes.\n\nThis retrieval operation maps directly to the political_news_crawler_crawl_alerts database table, ensuring consistent data mapping and field representation.\n\nClients should handle not found errors appropriately if the specified alert ID does not exist or has been deleted permanently.","summary":"Retrieve a crawl alert detail by ID","parameters":[{"name":"id","description":"Unique identifier of the target crawl alert","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed crawl alert information","typeName":"IPoliticalNewsCrawlerCrawlAlerts"},"authorizationRoles":["guest"],"path":"/politicalNewsCrawler/guest/crawlAlerts/{id}","method":"get","name":"at","authorizationRole":"guest","authorizationType":null},{"specification":"This operation creates a new crawl alert in the political_news_crawler_crawl_alerts table, which logs events such as bans, network errors, or throttling issues encountered during crawling of political news sources. These alerts include important metadata such as alert type, descriptive message, severity, and timestamps.\n\nThe API enables administrative clients to report new subsystem alerts for monitoring and operational awareness.\n\nAll required fields including crawl_source_id, alert_type, message, and severity must be provided for the alert creation. The system automatically manages audit timestamps on creation.\n\nNo direct user input fields other than essential alert details should be accepted.\n\nSecurity restrictions apply to ensure only authorized administrative roles can create alerts.","description":"Create a new crawl alert entry to record important operational events affecting crawling. This is essential for tracking issues such as bans or errors from crawl sources.\n\nClients must provide the crawl source reference, alert type, descriptive message, and severity level.\n\nNo soft delete or update timestamps are required, as this is strictly for new event logging.\n\nSuccessful creation returns the created crawl alert data with generated IDs and timestamps.","summary":"Create a new crawl alert","parameters":[],"requestBody":{"description":"New crawl alert details for creation","typeName":"IPoliticalNewsCrawlerCrawlAlerts.ICreate"},"responseBody":{"description":"Created crawl alert information","typeName":"IPoliticalNewsCrawlerCrawlAlerts"},"authorizationRoles":["guest"],"path":"/politicalNewsCrawler/guest/crawlAlerts","method":"post","name":"create","authorizationRole":"guest","authorizationType":null},{"specification":"This operation updates an existing crawl alert in the political_news_crawler_crawl_alerts database table by its unique identifier (id). It allows modification of alert details such as alert type, descriptive message, and severity level, reflecting updated operational status or corrections.\n\nOnly non-audit fields can be updated. Audit timestamps such as created_at should remain immutable.\n\nThe operation supports partial updates of provided fields and enforces proper authorization controls.\n\nIf the crawl alert ID does not exist, clients should handle such errors gracefully.\n\nThis update operation is critical for maintaining accurate monitoring records and operational alert data integrity.","description":"Update an existing crawl alert identified by ID. This allows modification of operational alert details including alert type, message, and severity.\n\nAudit timestamps are immutable and managed internally.\n\nClients should ensure to provide only the fields that need to be updated.\n\nProper error handling is required for non-existent IDs.\n\nThis operation maps to the political_news_crawler_crawl_alerts table ensuring data consistency.","summary":"Update crawl alert information by ID","parameters":[{"name":"id","description":"Unique identifier of the target crawl alert","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Updated crawl alert data","typeName":"IPoliticalNewsCrawlerCrawlAlerts.IUpdate"},"responseBody":{"description":"Updated crawl alert detail","typeName":"IPoliticalNewsCrawlerCrawlAlerts"},"authorizationRoles":["guest"],"path":"/politicalNewsCrawler/guest/crawlAlerts/{id}","method":"put","name":"update","authorizationRole":"guest","authorizationType":null},{"specification":"This operation permanently removes a crawl alert record from the political_news_crawler_crawl_alerts database table by its unique identifier (id). This hard delete operation irreversibly deletes all associated data for compliance and housekeeping.\n\nClients must exercise caution as deleted records cannot be recovered.\n\nThe API enforces role-based access control to allow deletions only by authorized administrative users.\n\nThis corresponds directly to the underlying database entity removing the record in a hard delete semantics.\n\nClients should confirm existence and permission before invoking this operation.","description":"Permanently delete a crawl alert record by its ID. This operation irreversibly removes the alert and all associated data from the database.\n\nThis hard delete is definitive and cannot be undone.\n\nOnly authorized users with administrative roles may perform this operation.\n\nClients should confirm the record exists before requesting deletion.\n\nThis action maps to the political_news_crawler_crawl_alerts DB table deletion behavior.","summary":"Delete crawl alert by ID","parameters":[{"name":"id","description":"Unique identifier of the target crawl alert","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":["guest"],"path":"/politicalNewsCrawler/guest/crawlAlerts/{id}","method":"delete","name":"erase","authorizationRole":"guest","authorizationType":null},{"specification":"This operation retrieves a filtered and paginated list of processing alerts related to the politicalNewsCrawler backend system. It returns various alert events such as LLM processing failures, queue overflows, and retry escalations. The data returned includes alert type, severity, and detailed descriptive messages. It enables operators to monitor and diagnose processing pipeline issues efficiently. The underlying database table is political_news_crawler_processing_alerts, which stores alert records with creation and update timestamps as well as soft deletion support.","description":"Retrieve a paginated list of processing alerts for the politicalNewsCrawler service.\n\nThis operation provides filtering capabilities to search alerts by type, severity, time ranges, and message content.\n\nSecurity considerations: this endpoint is publicly accessible without authentication, reflecting the service's open design.\n\nEach alert record originates from the political_news_crawler_processing_alerts table and includes fields such as alert_type, message, severity, created_at, and updated_at.\n\nThe endpoint supports pagination and sorting to efficiently manage potentially large alert data sets.\n\nError handling includes returning appropriate responses for invalid filter parameters or server issues.\n\nRelated operations include retrieving individual alerts by ID for detailed inspection.\n\nThis operation adheres to business rules that ensure alerts accurately reflect backend processing issues and provide operational visibility.","path":"/politicalNewsCrawler/processingAlerts","method":"patch","summary":"Retrieve list of processing alerts with filtering and pagination","parameters":[],"requestBody":{"description":"Filtering criteria and pagination parameters for processing alerts","typeName":"IPoliticalNewsCrawlerProcessingAlert.IRequest"},"responseBody":{"description":"Paginated list of processing alert records","typeName":"IPageIPoliticalNewsCrawlerProcessingAlert"},"authorizationRoles":[],"name":"index","authorizationRole":null,"authorizationType":null},{"specification":"This operation retrieves details of a single processing alert by its unique identifier from the political_news_crawler_processing_alerts table. It returns comprehensive alert information including alert type, message, severity, timestamps, and soft deletion status. This facilitates precise monitoring and troubleshooting of backend processing issues associated with the politicalNewsCrawler service.","description":"Retrieve detailed information for a specific processing alert identified by its unique ID.\n\nThis operation returns a single alert record from the political_news_crawler_processing_alerts table including all descriptive fields.\n\nSecurity considerations: this endpoint is publicly accessible without authentication.\n\nIt allows operators and support personnel to view complete details for diagnosing and responding to alert events.\n\nIf the specified alert ID does not exist, the operation returns a not found error.\n\nRelated operations include the listing endpoint that returns multiple alerts with filtering.\n\nThis operation supports business requirements for transparency and operational health monitoring.","path":"/politicalNewsCrawler/processingAlerts/{id}","method":"get","summary":"Retrieve detailed processing alert information by ID","parameters":[{"name":"id","description":"Unique identifier of the processing alert","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Processing alert detailed information","typeName":"IPoliticalNewsCrawlerProcessingAlert"},"authorizationRoles":[],"name":"at","authorizationRole":null,"authorizationType":null},{"specification":"This operation creates a new processing alert entry in the political_news_crawler_processing_alerts table to record issues identified during backend LLM processing jobs. It accepts alert type, descriptive message, and severity level as input fields. This endpoint enables the system and support teams to log new incidents for timely operational handling.","description":"Create a new processing alert record to log backend processing issues such as LLM failures or queue problems.\n\nThis operation records alert details including alert_type, message, and severity level in the system.\n\nSecurity considerations: this endpoint is publicly exposed without authentication, but typically would be called internally by system components.\n\nValidation rules ensure required fields are provided with allowed severity values like 'info', 'warning', and 'critical'.\n\nThe newly created alert record is returned including timestamps and identifier.\n\nRelated operations include listing processing alerts and retrieving individual alert details.\n\nProper use of this endpoint ensures timely detection and notification of backend processing problems.","path":"/politicalNewsCrawler/processingAlerts","method":"post","summary":"Create a new processing alert record","parameters":[],"requestBody":{"description":"Data required to create a new processing alert","typeName":"IPoliticalNewsCrawlerProcessingAlert.ICreate"},"responseBody":{"description":"Newly created processing alert record","typeName":"IPoliticalNewsCrawlerProcessingAlert"},"authorizationRoles":[],"name":"create","authorizationRole":null,"authorizationType":null},{"specification":"This operation updates an existing processing alert record identified by its unique ID in the political_news_crawler_processing_alerts table. It allows modifying alert attributes such as alert type, message, and severity. This endpoint supports administrative correction or augmentation of alert information to maintain accurate operational data.","description":"Update an existing processing alert identified by its ID.\n\nThis operation accepts updated alert_type, message, and severity fields.\n\nSecurity considerations: publicly accessible with no authentication.\n\nIf the specified ID does not exist, the operation returns an error.\n\nRelated operations include creating new alerts and listing alerts with filters.\n\nUpdating alerts is necessary for accurate status tracking, corrections, or additional information inclusion.","path":"/politicalNewsCrawler/processingAlerts/{id}","method":"put","summary":"Update an existing processing alert record","parameters":[{"name":"id","description":"Unique identifier of the processing alert to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Data required to update processing alert information","typeName":"IPoliticalNewsCrawlerProcessingAlert.IUpdate"},"responseBody":{"description":"Updated processing alert record","typeName":"IPoliticalNewsCrawlerProcessingAlert"},"authorizationRoles":[],"name":"update","authorizationRole":null,"authorizationType":null},{"specification":"This operation deletes an existing processing alert record from the political_news_crawler_processing_alerts table within the database. It allows for permanent removal of the alert record identified by the UUID 'id' path parameter. Processing alerts encapsulate various error or condition logs such as LLM processing failures or queue backlogs. Typically, only users with the role \"guest\" are authorized to access this endpoint, reflecting a controlled permission scenario within the given user role definitions. The deletion here is a hard delete, permanently removing the record from the database without soft delete markings.","description":"This API endpoint provides the capability to erase a specified processing alert by accepting its unique identifier as a path parameter.\n\nThe processing alerts represent system notifications for issues encountered during the LLM post-processing pipeline, such as failures, retries, or other significant events requiring operational attention.\n\nProper authorization is required, typically limited to users with the \"guest\" role to prevent unauthorized data loss.\n\nThe operation deletes the processing alert permanently from the system, meaning the record cannot be recovered after deletion.\n\nUsers must ensure the 'id' corresponds to a valid existing alert record to avoid errors.","summary":"Delete a specific processing alert by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the processing alert to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/processingAlerts/{id}","method":"delete","name":"eraseProcessingAlert"},{"specification":"This operation retrieves a paginated list of API alerts from the political_news_crawler_api_alerts table. API alerts record system notifications such as rate limiting, endpoint errors, and error spikes. The API supports filtering, sorting, and pagination via query parameters in the IRequest request body. Results help identify operational issues and trigger maintenance actions. Alerts include severity level, message, and type timestamped for tracking. Typically accessible by system monitoring users with the \"guest\" role to observe API health and incident trends.","description":"Retrieve a list of API alert records with flexible search and pagination.\n\nAPI alerts correspond to system event notifications for rate limiting, endpoint issues, and spike detection.\n\nThis operation allows monitoring users or admins to filter alerts based on severity, type, or date ranges.\n\nResults are paginated and sorted according to request criteria, enabling effective operational surveillance.\n\nProper authorization ensures only permitted roles can view sensitive alert data, commonly the \"guest\" user role in this context.\n\nAll users of the \"guest\" role are authorized.","summary":"Search and retrieve a filtered, paginated list of API alerts","parameters":[],"requestBody":{"description":"Search and pagination parameters for API alerts","typeName":"IPoliticalNewsCrawlerApiAlert.IRequest"},"responseBody":{"description":"Paginated search results of API alert summaries","typeName":"IPageIPoliticalNewsCrawlerApiAlert.ISummary"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/apiAlerts","method":"patch","name":"indexApiAlerts"},{"specification":"This operation retrieves a single API alert from the political_news_crawler_api_alerts table by its unique UUID identifier. It returns detailed information including alert type, message, severity, and timestamps. This is useful for viewing specific incident details in the system monitoring context. Access is granted to users with the \"guest\" role. The operation returns 404 if the specified alert ID does not exist.","description":"Retrieve detailed information about a specific API alert identified by its unique ID.\n\nThis endpoint is intended for use by users with the \"guest\" role or monitoring tools requiring deep insights into particular alert events.\n\nAPI alert data includes event type, severity level, descriptive message, and timestamps.\n\nAttempting to retrieve a non-existent alert will result in an error response indicating not found.","summary":"Get detailed information of a specific API alert by ID","parameters":[{"name":"id","in":"path","description":"Unique identifier of the API alert to retrieve","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":{"description":"Detailed API alert information","typeName":"IPoliticalNewsCrawlerApiAlert"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/apiAlerts/{id}","method":"get","name":"atApiAlert"},{"specification":"This operation creates a new API alert record in the political_news_crawler_api_alerts table. API alerts capture system events regarding API subsystem errors such as rate limiting, endpoint failures, and error spikes. The request body includes alert type, severity, message, and timestamps. Creation of alerts is typically automated by the backend monitoring system, but can be manually invoked for testing or exceptional cases by authorized users with the \"guest\" role. The operation validates input data and returns the created alert entity with assigned UUID.","description":"Create a new API alert entry to record system-level API issues or notifications.\n\nThis endpoint is generally used by automated monitoring or system components, but manual creation is supported for maintenance purposes.\n\nThe alert includes type, severity, message, and the time it was generated.\n\nOnly users with the \"guest\" role can create API alerts to maintain system integrity.\n\nThe API returns the newly created alert including its unique identifier.","summary":"Create a new API alert record","parameters":[],"requestBody":{"description":"Information required for creating an API alert","typeName":"IPoliticalNewsCrawlerApiAlert.ICreate"},"responseBody":{"description":"Created API alert information","typeName":"IPoliticalNewsCrawlerApiAlert"},"authorizationType":null,"authorizationRole":"guest","path":"/politicalNewsCrawler/guest/apiAlerts","method":"post","name":"createApiAlert"},{"specification":"This operation updates a specific API alert record identified by its unique UUID. It acts on the political_news_crawler_api_alerts table in the Prisma schema which stores alert events related to API errors such as rate limiting and endpoint failures. The operation allows changing alert_type, message, and severity, and ensures monitoring alert accuracy. It requires the alert id as a path parameter and the update request body containing alert details according to IPoliticalNewsCrawlerApiAlert.IUpdate schema. The response returns the updated alert record. Authorized roles: guest.","path":"/politicalNewsCrawler/guest/apiAlerts/{id}","method":"put","summary":"Update a specific API alert by id","description":"Update an existing API alert identified by its unique UUID.\n\nThis operation modifies the alert_type, message, and severity attributes of the API alert stored in the political_news_crawler_api_alerts table. This ensures that monitoring systems maintain correct and timely alert data for API subsystem errors.\n\nThe path parameter id uniquely identifies the API alert record to be updated.\n\nOnly users with 'guest' role authorization can execute this operation as the alert management is publicly accessible read/write controlled.\n\nThe operation will return the updated API alert record upon successful update.\n\nIf the specified id does not exist or payload validation fails, appropriate error responses will be generated.\n\nThis operation is a key part of the alert management subsystem that helps in tracking API error conditions and notifying stakeholders.","parameters":[{"name":"id","in":"path","description":"Unique identifier of the API alert to update","schema":{"type":"string","format":"uuid"}}],"requestBody":{"description":"Update data for the API alert","typeName":"IPoliticalNewsCrawlerApiAlert.IUpdate"},"responseBody":{"description":"Updated API alert record","typeName":"IPoliticalNewsCrawlerApiAlert"},"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"update"},{"specification":"This operation permanently deletes a specified API alert identified by its unique UUID. It operates on the political_news_crawler_api_alerts table which contains API subsystem error alerts. The deletion is a hard delete without soft deletion semantics and does not return a response body. The API alert id must be supplied as a path parameter. Only users with 'guest' authorization role may invoke this endpoint, reflecting public access without authentication. This operation supports managing alert records and cleaning obsolete entries to maintain system hygiene and reduce storage load.","path":"/politicalNewsCrawler/guest/apiAlerts/{id}","method":"delete","summary":"Delete a specific API alert by id","description":"Delete an existing API alert permanently by its unique UUID.\n\nThis operation removes the alert record from the political_news_crawler_api_alerts table irreversibly.\n\nThe path parameter id specifies the alert to remove.\n\nOnly users with 'guest' role authorization can perform this action.\n\nNo response body is returned upon successful deletion.\n\nThis is a hard delete operation; the record is fully removed with no soft delete functionality.\n\nUse this operation cautiously as deleted alert records cannot be recovered.\n\nThis endpoint helps maintain system health by allowing removal of outdated or resolved alerts.","parameters":[{"name":"id","in":"path","description":"Unique identifier of the API alert to delete","schema":{"type":"string","format":"uuid"}}],"requestBody":null,"responseBody":null,"authorizationRoles":["guest"],"authorizationRole":"guest","authorizationType":null,"name":"erase"}],"components":{"authorization":[{"name":"guest","description":"Unauthenticated users who can access public endpoints to retrieve news and popular topics.","kind":"guest"}],"schemas":{"IPage.IPagination":{"type":"object","properties":{"current":{"type":"integer","minimum":0,"description":"Current page number."},"limit":{"type":"integer","minimum":0,"description":"Limitation of records per a page."},"records":{"type":"integer","minimum":0,"description":"Total records in the database."},"pages":{"type":"integer","minimum":0,"description":"Total pages.\n\nEqual to {@link records} / {@link limit} with ceiling."}},"required":["current","limit","records","pages"],"description":"Page information."},"IAuthorizationToken":{"type":"object","properties":{"access":{"type":"string","description":"JWT access token for authenticated requests.\n\nThis token should be included in the Authorization header for subsequent\nauthenticated API requests as `Bearer {token}`."},"refresh":{"type":"string","description":"Refresh token for obtaining new access tokens.\n\nThis token can be used to request new access tokens when the current access\ntoken expires, extending the user's session."},"expired_at":{"type":"string","format":"date-time","description":"Access token expiration timestamp.\n\nISO 8601 date-time string indicating when the access token will expire and\ncan no longer be used for authentication."},"refreshable_until":{"type":"string","format":"date-time","description":"Refresh token expiration timestamp.\n\nISO 8601 date-time string indicating the latest time until which the\nrefresh token can be used to obtain new access tokens."}},"required":["access","refresh","expired_at","refreshable_until"],"description":"Authorization token response structure.\n\nThis interface defines the structure of the authorization token response\nreturned after successful user authentication. It contains both access and\nrefresh tokens along with their expiration information.\n\nThis token structure is automatically included in API schemas when the system\ndetects authorization roles in the requirements analysis phase. It provides a\nstandard format for JWT-based authentication across the generated backend\napplications."},"IPageIPoliticalNewsCrawlerCrawlSources.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlSources.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawlPolicy.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlPolicy.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawlSchedule.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlSchedule.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerGuests.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerGuests.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawlJobs":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlJobs"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawlAttempt.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlAttempt.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawledNews.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawledNews.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerRawDataStorage.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerRawDataStorage.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerProcessedContent.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerProcessedContent.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerLlmJobs.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerLlmJobs.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerLlmJobResult":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerLlmJobResult"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerPopularTopics":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerPopularTopics"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerPopularityScore.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerPopularityScore.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerTopicMentions.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerTopicMentions.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerApiAccessLog.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerApiAccessLog.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerApiErrorLog.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerApiErrorLog.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerApiUsageMetricSummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerApiUsageMetricSummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerCrawlAlert":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlAlert"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerProcessingAlert":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerProcessingAlert"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPageIPoliticalNewsCrawlerApiAlert.ISummary":{"type":"object","properties":{"pagination":{"$ref":"#/components/schemas/IPage.IPagination","description":"Page information."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerApiAlert.ISummary"},"description":"List of records."}},"required":["pagination","data"],"description":"A page.\n\nCollection of records with pagination information."},"IPoliticalNewsCrawlerGuest.IRequest":{"description":"Guest join request payload. For guests, this is typically empty or minimal since no credentials are needed.","type":"object","properties":{},"required":[]},"IPoliticalNewsCrawlerGuest.IAuthorized":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier of the authenticated guest"},"token":{"$ref":"#/components/schemas/IAuthorizationToken","description":"JWT token information for authentication"},"ip_address":{"type":"string","description":"IP address of the guest user"},"user_agent":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"User agent string presented by the guest"},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the guest record was created"},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the guest record was last updated"}},"required":["id","token","ip_address","created_at","updated_at"],"description":"Authorization response containing JWT token.\n\nThis response is returned after successful guest authentication or token refresh operations. It contains guest metadata and token expiry information to support authenticated guest sessions."},"IPoliticalNewsCrawlerGuest.IRefresh":{"description":"Payload containing a refresh token to renew guest JWT tokens securely.","type":"object","properties":{"refresh_token":{"type":"string","description":"The refresh token used to obtain new JWT tokens."}},"required":["refresh_token"]},"IPoliticalNewsCrawlerCrawlSources.IRequest":{"type":"object","properties":{"search_keyword":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional search keyword to filter source code or URL."},"is_active":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Filter active sources only if true, inactive if false, or both if null."},"page":{"oneOf":[{"type":"integer","minimum":1},{"type":"null"}],"description":"Pagination page number, starting at 1."},"limit":{"oneOf":[{"type":"integer","minimum":1},{"type":"null"}],"description":"Records per page limit."},"order_by":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sort by field name, e.g., \"source_code\" or \"created_at\"."},"direction":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sort order direction: \"asc\" or \"desc\"."}},"required":[],"description":"Request interface for crawl sources listing with filtering and pagination.\nSupports criteria to search and paginate crawl sources."},"IPoliticalNewsCrawlerCrawlSources":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key.\n\nUnique identifier of the crawl source."},"source_code":{"type":"string","description":"Unique identifier code for the crawl source.\n\nThis code uniquely identifies each crawl source within the system."},"source_url":{"type":"string","description":"The base URL of the crawl source website or API.\n\nThe URL serves as the source location for crawling news data."},"is_active":{"type":"boolean","description":"Flag indicating whether the crawl source is active and enabled for crawling.\n\nWhen true, crawling operations are allowed for this source."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional description of the crawl source.\n\nProvides additional contextual information about the source."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp.\n\nTimestamp when this crawl source record was created."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp.\n\nTimestamp when this record was last modified."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, if record is deleted.\n\nNullable field indicating if the record was deleted."}},"required":["id","source_code","source_url","is_active","created_at","updated_at"],"description":"Political News Crawler Crawl Sources entity stores the configuration of each news source that the system can crawl. This includes the source's unique code, URL, active status, optional description, and audit timestamps.\n\nThis entity is fundamental for managing the variety of external sites and APIs from which political news is retrieved. Managing these sources centrally allows the crawler to operate on diverse data feeds efficiently and safely.\n\nSecurity note: This entity does not contain sensitive details and is generally exposed publicly.\n\nBusiness logic includes ensuring uniqueness of source_code and source_url and that inactive sources are excluded from crawling cycles."},"IPoliticalNewsCrawlerCrawlSources.ICreate":{"type":"object","properties":{"source_code":{"type":"string","description":"Unique identifier code for the crawl source.\n\nRequired during creation to uniquely identify the source."},"source_url":{"type":"string","description":"The base URL of the crawl source website or API.\n\nThis field is mandatory for crawl source creation."},"is_active":{"type":"boolean","description":"Flag indicating whether the crawl source is active and enabled for crawling.\n\nDefaults to true when creating a new source."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional description of the crawl source.\n\nCan be null if no description is provided."}},"required":["source_code","source_url","is_active"],"description":"Input type for creating a new political news crawler crawl source."},"IPoliticalNewsCrawlerCrawlSources.IUpdate":{"type":"object","properties":{"source_code":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Unique identifier code for the crawl source.\n\nUpdatable field for renaming or correcting source code."},"source_url":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"The base URL of the crawl source website or API.\n\nCan be updated to change the crawl target."},"is_active":{"type":"boolean","description":"Flag indicating whether the crawl source is active and enabled for crawling.\n\nAllows enabling or disabling crawling for this source."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional description of the crawl source.\n\nCan be updated or cleared by setting null explicitly."}},"required":[],"description":"Input type for updating an existing political news crawler crawl source."},"IPoliticalNewsCrawlerCrawlPolicy.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"integer","minimum":0,"description":"Pagination page number.\n\nThe page to retrieve for paginated results."},{"type":"null"}],"description":"Pagination page number.\n\nThe page to retrieve for paginated results."},"limit":{"oneOf":[{"type":"integer","minimum":0,"description":"Pagination page size limit.\n\nThe maximum number of items per page."},{"type":"null"}],"description":"Pagination page size limit.\n\nThe maximum number of items per page."},"policy_name":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter for policy name matching.\n\nCan be used to search for policies by name."},"max_crawl_frequency_minutes":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Filter for maximum crawl frequency.\n\nLimits results to policies with max frequency below this value."},"ban_detection_enabled":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Filter for enabling ban detection.\n\nIf true, only policies with ban detection enabled are returned."}},"required":[],"description":"Request type for searching crawl policies, supports filtering and pagination."},"IPoliticalNewsCrawlerCrawlPolicy":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"policy_name":{"type":"string","description":"Unique name identifier for the crawl policy."},"max_crawl_frequency_minutes":{"type":"integer","format":"int32","description":"Maximum allowed crawl frequency in minutes."},"max_retry_attempts":{"type":"integer","format":"int32","description":"Maximum number of retry attempts after failures."},"backoff_multiplier":{"type":"number","description":"Multiplier factor for exponential backoff on retries."},"ban_detection_enabled":{"type":"boolean","description":"Flag to enable detection and handling of bans during crawling."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, if record is deleted."}},"required":["id","policy_name","max_crawl_frequency_minutes","max_retry_attempts","backoff_multiplier","ban_detection_enabled","created_at","updated_at"],"description":"Configuration for crawl policies governing crawling frequency, retry, and error handling for political news sources. Ensures adaptive and respectful crawling behavior according to source limits and bans."},"IPoliticalNewsCrawlerCrawlPolicy.ICreate":{"type":"object","properties":{"policy_name":{"type":"string","description":"Unique name identifier for the crawl policy."},"max_crawl_frequency_minutes":{"type":"integer","format":"int32","description":"Maximum allowed crawl frequency in minutes."},"max_retry_attempts":{"type":"integer","format":"int32","description":"Maximum number of retry attempts after failures."},"backoff_multiplier":{"type":"number","description":"Multiplier factor for exponential backoff on retries."},"ban_detection_enabled":{"type":"boolean","description":"Flag to enable detection and handling of bans during crawling."}},"required":["policy_name","max_crawl_frequency_minutes","max_retry_attempts","backoff_multiplier","ban_detection_enabled"],"description":"Create info for crawl policy."},"IPoliticalNewsCrawlerCrawlPolicy.IUpdate":{"type":"object","properties":{"policy_name":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Unique name identifier for the crawl policy."},"max_crawl_frequency_minutes":{"oneOf":[{"type":"integer","format":"int32"},{"type":"null"}],"description":"Maximum allowed crawl frequency in minutes."},"max_retry_attempts":{"oneOf":[{"type":"integer","format":"int32"},{"type":"null"}],"description":"Maximum number of retry attempts after failures."},"backoff_multiplier":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Multiplier factor for exponential backoff on retries."},"ban_detection_enabled":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Flag to enable detection and handling of bans during crawling."}},"required":[],"description":"Update info for crawl policy."},"IPoliticalNewsCrawlerCrawlSchedule.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"integer","format":"uint32"},{"type":"null"}],"description":"Page number."},"limit":{"oneOf":[{"type":"integer","format":"uint32"},{"type":"null"}],"description":"Limitation of records per a page."},"sort":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sorting expression with 'propertyName asc' or 'propertyName desc'.\n\nExample: \"created_at desc\""},"crawl_source_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filter by crawl source ID (foreign key to political_news_crawler_crawl_sources)."},"crawl_policy_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filter by crawl policy ID (foreign key to political_news_crawler_crawl_policies)."},"is_enabled":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Filter by enabled flag."},"last_crawled_after":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records last crawled after this timestamp."},"last_crawled_before":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records last crawled before this timestamp."},"next_crawl_after":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records next crawl after this timestamp."},"next_crawl_before":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records next crawl before this timestamp."}},"required":[],"description":"Search data and pagination filter criteria for crawl schedules.","default":{}},"IPoliticalNewsCrawlerCrawlSchedule":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key.\n\nUnique identifier for the crawl schedule record.\n\nFormat: UUID string."},"crawl_source_id":{"type":"string","format":"uuid","description":"Reference to Crawling Source.\n\nForeign key to the political_news_crawler_crawl_sources entity."},"crawl_policy_id":{"type":"string","format":"uuid","description":"Reference to Crawl Policy.\n\nForeign key to the political_news_crawler_crawl_policies entity."},"schedule_expression":{"type":"string","description":"Cron expression defining the crawl schedule timing.\n\nFormat is a standard cron string defining when the crawl runs."},"last_crawled_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the crawl last occurred.\n\nOptional ISO 8601 date-time string."},"next_crawl_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp for the next scheduled crawl.\n\nOptional ISO 8601 date-time string."},"is_enabled":{"type":"boolean","description":"Flag indicating if this schedule is enabled.\n\nTrue if schedule is active."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp.\n\nISO 8601 date-time string."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp.\n\nISO 8601 date-time string."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, if record is deleted.\n\nOptional ISO 8601 date-time string."}},"required":["id","crawl_source_id","crawl_policy_id","schedule_expression","is_enabled","created_at","updated_at"],"description":"Defines when and how often crawling runs for each political news source. References the crawl source and policy to enable adaptive scheduling and coordination."},"IPoliticalNewsCrawlerCrawlSchedules.ICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","format":"uuid","description":"Reference to Crawling Source.\n\nRequired foreign key UUID string."},"crawl_policy_id":{"type":"string","format":"uuid","description":"Reference to Crawl Policy.\n\nRequired foreign key UUID string."},"schedule_expression":{"type":"string","description":"Cron expression defining the crawl schedule timing.\n\nRequired string with cron format."},"is_enabled":{"type":"boolean","description":"Flag indicating if this schedule is enabled.\n\nRequired boolean."}},"required":["crawl_source_id","crawl_policy_id","schedule_expression","is_enabled"],"description":"Creation info of the crawl schedules"},"IPoliticalNewsCrawlerCrawlSchedules":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key identifier of the crawl schedule."},"data":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerCrawlSchedule"},"description":"List of crawl schedule records."},"pagination":{"type":"object","properties":{"current":{"type":"integer","minimum":1,"description":"Current page number."},"limit":{"type":"integer","minimum":1,"description":"Number of records per page."},"records":{"type":"integer","minimum":0,"description":"Total number of records matching the criteria."},"pages":{"type":"integer","minimum":1,"description":"Total number of pages available."}},"required":["current","limit","records","pages"],"description":"Pagination information for the result set."}},"required":["id","data","pagination"],"description":"Response structure for paginated crawl schedules."},"IPoliticalNewsCrawlerCrawlSchedules.IUpdate":{"type":"object","properties":{"crawl_source_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Reference to Crawling Source.\n\nOptional foreign key UUID string."},"crawl_policy_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Reference to Crawl Policy.\n\nOptional foreign key UUID string."},"schedule_expression":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Cron expression defining the crawl schedule timing.\n\nOptional string with cron format."},"last_crawled_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the crawl last occurred.\n\nOptional ISO 8601 date-time string."},"next_crawl_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp for the next scheduled crawl.\n\nOptional ISO 8601 date-time string."},"is_enabled":{"type":"boolean","description":"Flag indicating if this schedule is enabled.\n\nOptional boolean."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, if record is deleted.\n\nOptional ISO 8601 date-time string."}},"required":[],"description":"Update info of the crawl schedules"},"IPoliticalNewsCrawlerGuests.IRequest":{"type":"object","properties":{"ip_address":{"type":"string","description":"Optional string filter for IP address.\n\nSupports exact or partial search."},"user_agent":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional string filter for user agent.\n\nSupports exact or partial search."},"page":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination: page number.\n\nOptional unsigned 32-bit integer."},"limit":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination: number of records per page.\n\nOptional unsigned 32-bit integer.\n\nDefault is typically 100."},"sort":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional sort order specification.\n\nCould be a string indicating sorting field and direction."},"search":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional search string for fuzzy matching or full-text search."}},"description":"Request body schema for searching political news crawler guests with filtering and pagination.","required":[]},"IPoliticalNewsCrawlerGuests":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"ip_address":{"type":"string","description":"IP address of the guest user."},"user_agent":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"User agent string presented by the guest."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the guest record was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the guest record was last updated."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp of soft deletion for the guest record."}},"required":["id","ip_address","created_at","updated_at"],"description":"Stores political news crawler guest user information representing unauthenticated users accessing APIs. Captures identification via IP and user agent, includes timestamps for auditing and soft deletion support.\n\nGuests are limited to read-only access with no password or login credentials."},"IPoliticalNewsCrawlerCrawlJobsIRequest":{"type":"object","properties":{"crawl_schedule_id":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional filter for crawl schedule ID.\n\nIdentifies the schedule to which the job belongs."},"active":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Optional filter for job active status.\n\nBoolean to filter active or inactive jobs."},"page":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination: current page number.\n\nOptional unsigned 32-bit integer."},"limit":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination: limit of records per page.\n\nOptional unsigned 32-bit integer.\n\nDefault value is usually 100."},"sort":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional sort instructions.\n\nSpecifies how results should be ordered."},"search":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional search keyword.\n\nUsed for filtering jobs based on relevant text fields."}},"description":"Request body schema for searching crawl jobs with filtering and pagination.","required":[]},"IPoliticalNewsCrawlerCrawlJobs":{"type":"object","properties":{"id":{"type":"string","description":"Primary Key.\n\nUnique identifier for the crawl job.\n\nStored as UUID string."},"crawl_source_id":{"type":"string","description":"Referenced crawl source identifier.\n\nRelates to the source where crawling is performed.\n\nUUID string."},"crawl_schedule_id":{"type":"string","description":"Referenced crawl schedule identifier.\n\nSpecifies schedule associated with the crawl job.\n\nUUID string."},"active":{"type":"boolean","description":"Boolean flag indicating if the crawl job is active.\n\nTrue if scheduled to run, false otherwise."},"last_run_started_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional timestamp when the last run started.\n\nNullable ISO 8601 date-time string with timezone."},"last_run_completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional timestamp when the last run completed.\n\nNullable ISO 8601 date-time string with timezone."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the crawl job was created.\n\nISO 8601 date-time format."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the crawl job was last updated.\n\nISO 8601 date-time format."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional soft deletion timestamp.\n\nNullable ISO 8601 date-time string with timezone."}},"required":["id","crawl_source_id","crawl_schedule_id","active","created_at","updated_at"],"description":"Entity representing a scheduled crawl job for a political news source."},"IPoliticalNewsCrawlerCrawlJobsICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","format":"uuid","description":"Reference to the crawl source identifier.\nFormat: UUID v4 string.\nIt identifies the source from which the crawl job originates."},"crawl_schedule_id":{"type":"string","format":"uuid","description":"Reference to the crawl schedule identifier.\nFormat: UUID v4 string.\nLinks the job to a predefined crawl schedule."},"active":{"type":"boolean","description":"Indicates whether the crawl job is active and scheduled to run.\ntrue means the job is currently active and enabled."},"last_run_started_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run of the crawl job started.\nOptional, nullable.\nFormat: ISO 8601 date-time string.\nRepresents the start time of the most recent execution."},"last_run_completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run of the crawl job completed.\nOptional, nullable.\nFormat: ISO 8601 date-time string.\nRepresents when the most recent execution finished."}},"required":["crawl_source_id","crawl_schedule_id","active"],"description":"Create request body schema for political_news_crawler_crawl_jobs table representing crawl job creation information."},"IPoliticalNewsCrawlerCrawlJob.IUpdate":{"type":"object","properties":{"crawl_source_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Reference to the crawl source identifier.\nFormat: UUID v4 string.\nIdentifies the source from which this crawl job originates.\nOptional in update, nullable."},"crawl_schedule_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Reference to the crawl schedule identifier.\nFormat: UUID v4 string.\nLinks the job to its crawl schedule.\nOptional in update, nullable."},"active":{"type":"boolean","description":"Boolean indicating if the crawl job is active.\nOptional in update."},"last_run_started_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run of the crawl job started.\nOptional in update, nullable.\nFormat: ISO 8601 date-time string."},"last_run_completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run of the crawl job completed.\nOptional in update, nullable.\nFormat: ISO 8601 date-time string."}},"required":[],"description":"Update request body schema for political_news_crawler_crawl_jobs table representing crawl job update information."},"IPoliticalNewsCrawlerCrawlJob":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier of the crawl job."},"crawl_source_id":{"type":"string","format":"uuid","description":"Reference to the crawl source identifier.\nIdentifies the source of the crawl job."},"crawl_schedule_id":{"type":"string","format":"uuid","description":"Reference to the crawl schedule identifier.\nLinks to the crawl schedule dictating crawl timing."},"active":{"type":"boolean","description":"Boolean indicating if the crawl job is active."},"last_run_started_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run started. Optional, nullable."},"last_run_completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the last run completed. Optional, nullable."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the record was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the record was last updated."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the record was soft deleted. Optional, nullable."}},"required":["id","crawl_source_id","crawl_schedule_id","active","created_at","updated_at"],"description":"Entity schema for political_news_crawler_crawl_jobs table representing a crawl job record."},"IPoliticalNewsCrawlerCrawlAttempt.IRequest":{"type":"object","properties":{"crawl_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filters for crawl job ID to restrict crawl attempts to a specific job. Optional filter."},"success":{"oneOf":[{"type":"boolean"},{"type":"null"}],"description":"Filter for success status of the crawl attempt. Optional."},"started_after":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Start timestamp to filter crawl attempts that started after this time. Optional, nullable."},"started_before":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"End timestamp to filter crawl attempts that started before this time. Optional, nullable."},"page":{"oneOf":[{"type":"integer","minimum":1},{"type":"null"}],"description":"Pagination page number. Optional. Must be positive integer if provided."},"limit":{"oneOf":[{"type":"integer","minimum":1},{"type":"null"}],"description":"Pagination page size limit. Optional. Must be positive integer if provided."}},"required":[],"description":"Request schema for filtering and pagination of crawl attempts."},"IPoliticalNewsCrawlerCrawlAttempt":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"crawl_job_id":{"type":"string","format":"uuid","description":"Associated crawl job identifier."},"raw_data_storage_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Reference to raw data storage entry for the crawl result."},"started_at":{"type":"string","format":"date-time","description":"Timestamp when this crawl attempt started."},"completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when this crawl attempt ended; null if still running."},"success":{"type":"boolean","description":"Indicator whether this crawl attempt was successful."},"error_message":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Error message details if the crawl attempt failed."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp."}},"required":["id","crawl_job_id","started_at","success","created_at","updated_at"],"description":"Records individual execution attempts of crawl jobs, tracking start and completion times, success status, errors, and associated raw data references to enable detailed auditing and failure analysis."},"IPoliticalNewsCrawlerCrawlAttempt.ICreate":{"type":"object","properties":{"crawl_job_id":{"type":"string","format":"uuid","description":"Associated crawl job identifier. {@link political_news_crawler_crawl_jobs.id}"},"raw_data_storage_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Optional reference to raw data storage entry for the crawl result. {@link political_news_crawler_raw_data_storage.id}"},"started_at":{"type":"string","format":"date-time","description":"Timestamp when this crawl attempt started."},"completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional timestamp when this crawl attempt ended; null if still running."},"success":{"type":"boolean","description":"Indicator whether this crawl attempt was successful."},"error_message":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional error message details if the crawl attempt failed."}},"required":["crawl_job_id","started_at","success"],"description":"Input data for creating a new crawl attempt record.\n\nIncludes crawl job association, start time, success indicator, and optionally raw data reference, completion time, and error message."},"IPoliticalNewsCrawlerCrawlAttempt.IUpdate":{"type":"object","properties":{"crawl_job_id":{"type":"string","format":"uuid","description":"Associated crawl job identifier. {@link political_news_crawler_crawl_jobs.id}"},"raw_data_storage_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Optional reference to raw data storage entry for the crawl result. {@link political_news_crawler_raw_data_storage.id}"},"started_at":{"type":"string","format":"date-time","description":"Timestamp when this crawl attempt started."},"completed_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional timestamp when this crawl attempt ended; null if still running."},"success":{"type":"boolean","description":"Indicator whether this crawl attempt was successful."},"error_message":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional error message details if the crawl attempt failed."}},"required":[],"description":"Partial update data for modifying an existing crawl attempt record.\n\nAllows updating any combination of crawl job ID, raw data storage reference, start and end timestamps, success flag, and error message."},"IPoliticalNewsCrawlerCrawledNews.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Page number for pagination."},"limit":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Limitation of records per a page."},"search":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Search term to filter news."},"sort_by":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sorting field and direction."},"crawl_attempt_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filter to specific crawl attempt ID."}},"required":[],"description":"Request parameters for querying crawled news with filtering, sorting, and pagination."},"IPoliticalNewsCrawlerCrawledNews":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"crawl_attempt_id":{"type":"string","format":"uuid","description":"Associated crawl attempt identifier."},"url":{"type":"string","description":"URL of the crawled news article."},"title":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Title of the news article, if available."},"published_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Publish timestamp of the news article, if known."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp."}},"required":["id","crawl_attempt_id","url","created_at","updated_at"],"description":"Contains metadata for crawled political news articles, linking to the crawl attempt that obtained the raw content and providing key attributes for management and filtering."},"IPoliticalNewsCrawlerCrawledNews.ICreate":{"type":"object","properties":{"crawl_attempt_id":{"type":"string","format":"uuid","description":"Associated crawl attempt identifier. {@link\r\n  /// political_news_crawler_crawl_attempts.id}"},"url":{"type":"string","description":"URL of the crawled news article."},"title":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Title of the news article, if available."},"published_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Publish timestamp of the news article, if known."}},"required":["crawl_attempt_id","url"],"description":"Create a new crawled news article associated with the specified crawl attempt.\n\nThe request body must contain the minimum required information for the crawler news metadata, including a valid URL. Title and published date are optional but recommended for completeness.\n\nNo authentication is required for this operation. On success, the newly created news article's full details are returned.\n\nThis operation complements retrieval and management endpoints allowing clients to add new news entries for a crawl attempt."},"IPoliticalNewsCrawlerCrawledNews.IUpdate":{"type":"object","properties":{"crawl_attempt_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Associated crawl attempt identifier. {@link\r\n  /// political_news_crawler_crawl_attempts.id}"},"url":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"URL of the crawled news article."},"title":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Title of the news article, if available."},"published_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Publish timestamp of the news article, if known."},"created_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Record creation timestamp."},"updated_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Record last update timestamp."}},"required":[],"description":"Update metadata of a crawled news article within the specified crawl attempt.\n\nOnly provided fields in the request body will be updated; others remain unchanged.\n\nThe operation validates the existence of the target article linked to the crawl attempt.\n\nNo authentication is required. Responses include the updated entity details.\n\nThis operation works in conjunction with create, retrieve, and delete endpoints to provide full management capabilities."},"IPoliticalNewsCrawlerRawDataStorage.IRequest":{"type":"object","properties":{"crawl_source_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Belonged crawl source's political_news_crawler_crawl_sources.id."},"crawl_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Optional crawl job reference to political_news_crawler_crawl_jobs.id."},"storage_key":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Unique key or path identifying storage location in cloud object storage (e.g., GCP or AWS S3)."},"file_format":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Format of the raw data file such as JSON or XML for processing compatibility."},"file_size_bytes":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Size of the raw data file in bytes."},"checksum":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Checksum hash to verify file integrity."},"crawl_timestamp":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the raw data was crawled, used for data freshness and scheduling."},"created_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Creation timestamp record."},"updated_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Last update timestamp record."}},"required":[],"description":"Retrieve a filtered and paginated list of raw data storage metadata entries for political news crawling. The operation supports filtering by crawl source and crawl job identifiers, file format types, crawl timestamps, and file sizes. Sorting and pagination options enable efficient browsing through large datasets stored in cloud object storage.\n\nSecurity considerations include limited access to authenticated users with appropriate read privileges, as raw data files may contain sensitive or proprietary information. \n\nThis operation is tightly integrated with the political_news_crawler_raw_data_storage table defined in the Prisma schema, encompassing all relevant fields and relationships. The response returns simplified summary information suited for list displays.\n\nThere is no request body since this is a PATCH method designed for complex search and filtering inputs."},"IPoliticalNewsCrawlerRawDataStorage":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"crawl_source_id":{"type":"string","format":"uuid","description":"Belonged crawl source's political_news_crawler_crawl_sources.id."},"crawl_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Optional crawl job reference to political_news_crawler_crawl_jobs.id."},"storage_key":{"type":"string","description":"Unique key or path identifying storage location in cloud object storage (e.g., GCP or AWS S3)."},"file_format":{"type":"string","description":"Format of the raw data file such as JSON or XML for processing compatibility."},"file_size_bytes":{"type":"integer","format":"int32","description":"Size of the raw data file in bytes."},"checksum":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Checksum hash to verify file integrity."},"crawl_timestamp":{"type":"string","format":"date-time","description":"Timestamp when the raw data was crawled, used for data freshness and scheduling."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp record."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp record."}},"required":["id","crawl_source_id","storage_key","file_format","file_size_bytes","crawl_timestamp","created_at","updated_at"],"description":"Stores metadata and references for raw political news data collected from various crawling sources. Ensures durable and consistent storage links to cloud object storage. Tracks source information, crawl job association, and data integrity validations. Includes audit timestamps for traceability."},"IPoliticalNewsCrawlerRawDataStorage.ICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","description":"Belonged crawl source's political_news_crawler_crawl_sources.id.\n\nIdentifier of the crawl source from which this raw data originated."},"crawl_job_id":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional crawl job reference to political_news_crawler_crawl_jobs.id.\n\nIdentifier of the crawl job associated with this raw data, if any."},"storage_key":{"type":"string","description":"Unique key or path identifying storage location in cloud object storage\n(e.g., GCP or AWS S3).\n\nStorage key that identifies the raw data file location within cloud storage."},"file_format":{"type":"string","description":"Format of the raw data file such as JSON or XML for processing\ncompatibility.\n\nString specifying the file format of the stored raw data."},"file_size_bytes":{"type":"integer","description":"Size of the raw data file in bytes.\n\nInteger representing the size of the raw data file in bytes."},"checksum":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Checksum hash to verify file integrity.\n\nOptional checksum string for data integrity verification."},"crawl_timestamp":{"type":"string","format":"date-time","description":"Timestamp when the raw data was crawled, used for data freshness and\nscheduling.\n\nDate-time when the raw data was obtained."}},"required":["crawl_source_id","storage_key","file_format","file_size_bytes","crawl_timestamp"],"description":"Creation info of the crawl source\n\n@author AutoBE - https://github.com/wrtnlabs/autobe"},"IPoliticalNewsCrawlerRawDataStorage.IUpdate":{"type":"object","properties":{"crawl_source_id":{"type":"string","description":"Belonged crawl source's political_news_crawler_crawl_sources.id.\n\nIdentifier of the crawl source from which this raw data originated."},"crawl_job_id":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional crawl job reference to political_news_crawler_crawl_jobs.id.\n\nIdentifier of the crawl job associated with this raw data, if any."},"storage_key":{"type":"string","description":"Unique key or path identifying storage location in cloud object storage\n(e.g., GCP or AWS S3).\n\nStorage key that identifies the raw data file location within cloud storage."},"file_format":{"type":"string","description":"Format of the raw data file such as JSON or XML for processing\ncompatibility.\n\nString specifying the file format of the stored raw data."},"file_size_bytes":{"type":"integer","description":"Size of the raw data file in bytes.\n\nInteger representing the size of the raw data file in bytes."},"checksum":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Checksum hash to verify file integrity.\n\nOptional checksum string for data integrity verification."},"crawl_timestamp":{"type":"string","format":"date-time","description":"Timestamp when the raw data was crawled, used for data freshness and\nscheduling.\n\nDate-time when the raw data was obtained."}},"required":[],"description":"Update info of the crawl source\n\n@author AutoBE - https://github.com/wrtnlabs/autobe"},"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Page number.\n\nOptional page number for pagination (unsigned 32-bit integer)."},"limit":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Limitation of records per a page.\n\nOptional limit to restrict the number of records per page (unsigned 32-bit integer)."},"ttl_expiration_at_from":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records with ttl_expiration_at greater than or equal to this ISO datetime.\n\nOptional filter to include only cache files whose TTL expiration is after or equal to certain date-time."},"ttl_expiration_at_to":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records with ttl_expiration_at less than or equal to this ISO datetime.\n\nOptional filter to include only cache files whose TTL expiration is before or equal to certain date-time."},"created_at_from":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records created after or equal to this ISO datetime.\n\nOptional filter for records created on or after a specific date-time."},"created_at_to":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter records created before or equal to this ISO datetime.\n\nOptional filter for records created on or before a specific date-time."}},"required":[],"description":"Search and pagination filter criteria for crawl schedules\n\n@author AutoBE - https://github.com/wrtnlabs/autobe"},"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile":{"type":"object","description":"Tracks local file cache copies of raw crawled political news data with TTL enforcement and deletion status. Enables fast retrieval during cloud storage outages and manages file lifecycle with audit timestamps.","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"raw_data_storage_id":{"type":"string","format":"uuid","description":"Reference to related raw data storage record, political_news_crawler_raw_data_storage.id."},"local_file_path":{"type":"string","description":"Filesystem path or identifier for the local cached file copy."},"file_size_bytes":{"type":"integer","description":"Size of the local cached file in bytes."},"ttl_expiration_at":{"type":"string","format":"date-time","description":"Datetime when the cached file expires and is due for deletion under TTL policy."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp indicating when the cached file was deleted, if applicable."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp record."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp record."}},"required":["id","raw_data_storage_id","local_file_path","file_size_bytes","ttl_expiration_at","created_at","updated_at"]},"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ICreate":{"type":"object","description":"Create a new local cache file metadata record for a raw data storage entry. The record includes the local file path, file size, TTL expiration datetime, and timestamps.","properties":{"raw_data_storage_id":{"type":"string","format":"uuid","description":"Reference to related raw data storage record, political_news_crawler_raw_data_storage.id."},"local_file_path":{"type":"string","description":"Filesystem path or identifier for the local cached file copy."},"file_size_bytes":{"type":"integer","description":"Size of the local cached file in bytes."},"ttl_expiration_at":{"type":"string","format":"date-time","description":"Datetime when the cached file expires and is due for deletion under TTL policy."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp indicating when the cached file was deleted, if applicable."}},"required":["raw_data_storage_id","local_file_path","file_size_bytes","ttl_expiration_at"]},"IPoliticalNewsCrawlerLocalCacheFiles.IUpdate":{"type":"object","description":"Update the metadata of a specific local cache file associated with raw data storage. Supports modifying path, size, TTL expiration, and deletion timestamp.","properties":{"raw_data_storage_id":{"type":"string","format":"uuid","description":"Reference to related raw data storage record, political_news_crawler_raw_data_storage.id."},"local_file_path":{"type":"string","description":"Filesystem path or identifier for the local cached file copy."},"file_size_bytes":{"type":"integer","description":"Size of the local cached file in bytes."},"ttl_expiration_at":{"type":"string","format":"date-time","description":"Datetime when the cached file expires and is due for deletion under TTL policy."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp indicating when the cached file was deleted, if applicable."}},"required":[]},"IPoliticalNewsCrawlerLocalCacheFiles":{"type":"object","description":"Tracks local file cache copies of raw crawled political news data with TTL enforcement and deletion status. Enables fast retrieval during cloud storage outages and manages file lifecycle with audit timestamps.","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"raw_data_storage_id":{"type":"string","format":"uuid","description":"Reference to related raw data storage record, political_news_crawler_raw_data_storage.id."},"local_file_path":{"type":"string","description":"Filesystem path or identifier for the local cached file copy."},"file_size_bytes":{"type":"integer","description":"Size of the local cached file in bytes."},"ttl_expiration_at":{"type":"string","format":"date-time","description":"Datetime when the cached file expires and is due for deletion under TTL policy."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp indicating when the cached file was deleted, if applicable."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp record."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp record."}},"required":["id","raw_data_storage_id","local_file_path","file_size_bytes","ttl_expiration_at","created_at","updated_at"]},"IPoliticalNewsCrawlerProcessedContent":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"raw_data_storage_id":{"type":"string","format":"uuid","description":"Foreign key to the raw data storage record, political_news_crawler_raw_data_storage.id."},"llm_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Foreign key to associated LLM job, political_news_crawler_llm_jobs.id."},"content_type":{"type":"string","description":"Type of processed content, e.g., summary, highlight, or analysis."},"content_body":{"type":"string","description":"Full textual content produced by LLM processing."},"generation_timestamp":{"type":"string","format":"date-time","description":"Timestamp when this content was generated."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp, typically same or near generation time."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp record."}},"required":["id","raw_data_storage_id","content_type","content_body","generation_timestamp","created_at","updated_at"],"description":"Processed political news content generated by LLM post-processing, including summaries, highlights, and analysis. Links content to raw data storage and optionally to the LLM job that generated it. Contains content type, full text body, generation timestamp, and audit timestamps.\n\nSupports text search through GIN index on content body."},"IPoliticalNewsCrawlerProcessedContent.IRequest":{"type":"object","properties":{"raw_data_storage_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filter by raw data storage id for processed content records"},"content_type":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter by content type, such as \"summary\", \"highlight\", or \"analysis\""},"page":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Pagination current page number."},"limit":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Pagination limit on number of records per page."},"generated_from":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional filter for date range start for generation timestamp."},"generated_to":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Optional filter for date range end for generation timestamp."}},"required":[],"description":"Request type for searching and paginating processed content records linked to raw data storage."},"IPoliticalNewsCrawlerProcessedContent.ICreate":{"type":"object","properties":{"raw_data_storage_id":{"type":"string","format":"uuid","description":"Foreign key to the raw data storage record, political_news_crawler_raw_data_storage.id."},"llm_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Foreign key to associated LLM job, political_news_crawler_llm_jobs.id."},"content_type":{"type":"string","description":"Type of processed content, e.g., summary, highlight, or analysis."},"content_body":{"type":"string","description":"Full textual content produced by LLM processing."},"generation_timestamp":{"type":"string","format":"date-time","description":"Timestamp when this content was generated."}},"required":["raw_data_storage_id","content_type","content_body","generation_timestamp"],"description":"Request type for creating processed content, including raw data reference and LLM job association."},"IPoliticalNewsCrawlerProcessedContent.IUpdate":{"type":"object","properties":{"raw_data_storage_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Foreign key to the raw data storage record, political_news_crawler_raw_data_storage.id."},"llm_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Foreign key to associated LLM job, political_news_crawler_llm_jobs.id."},"content_type":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Type of processed content, e.g., summary, highlight, or analysis."},"content_body":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Full textual content produced by LLM processing."},"generation_timestamp":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when this content was generated."}},"required":[],"description":"Request type for updating processed content with optional fields for patch-like modification."},"IPoliticalNewsCrawlerLlmJobs.IRequest":{"type":"object","properties":{"crawl_source_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Filter by crawl source UUID. Optional."},"status":{"oneOf":[{"const":"pending"},{"const":"running"},{"const":"completed"},{"const":"failed"},{"type":"null"}],"description":"Filter by exact status: 'pending', 'running', 'completed', 'failed'. Optional."},"page":{"oneOf":[{"type":"integer","minimum":1,"format":"uint32"},{"type":"null"}],"description":"Pagination: page number >= 1. Optional. Provides the page number for paginated results."},"limit":{"oneOf":[{"type":"integer","minimum":1,"format":"uint32"},{"type":"null"}],"description":"Pagination: limit per page >= 1. Optional, default can be 100."},"orderBy":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sort key, e.g., 'created_at' or 'updated_at'. Optional."}},"description":"Request parameters for searching and pagination of LLM jobs.","required":[]},"IPoliticalNewsCrawlerLlmJobs":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier of the LLM job."},"crawl_source_id":{"type":"string","format":"uuid","description":"The source channel from which the raw news data originated."},"status":{"oneOf":[{"const":"pending","description":"Processing status of the job."},{"const":"running","description":"Processing status of the job."},{"const":"completed","description":"Processing status of the job."},{"const":"failed","description":"Processing status of the job."}],"description":"Processing status of the job."},"parameters":{"type":"string","description":"JSON string of parameters or prompts used for this LLM job."},"created_at":{"type":"string","format":"date-time","description":"Job creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Job last update timestamp."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, if record is deleted; nullable."}},"required":["id","crawl_source_id","status","parameters","created_at","updated_at"],"description":"Represents a Large Language Model (LLM) processing job entity managing asynchronous processing tasks for political news data."},"IPoliticalNewsCrawlerLlmJobs.ICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","format":"uuid","description":"The source channel from which the raw news data originated."},"status":{"oneOf":[{"const":"pending","description":"Processing status of the job."},{"const":"running","description":"Processing status of the job."},{"const":"completed","description":"Processing status of the job."},{"const":"failed","description":"Processing status of the job."}],"description":"Processing status of the job."},"parameters":{"type":"string","description":"JSON string of parameters or prompts used for this LLM job."}},"required":["crawl_source_id","status","parameters"],"description":"Request body for creating a new LLM processing job."},"IPoliticalNewsCrawlerLlmJobs.IUpdate":{"type":"object","properties":{"crawl_source_id":{"type":"string","format":"uuid","description":"The source channel from which the raw news data originated."},"status":{"oneOf":[{"const":"pending","description":"Processing status of the job."},{"const":"running","description":"Processing status of the job."},{"const":"completed","description":"Processing status of the job."},{"const":"failed","description":"Processing status of the job."}],"description":"Processing status of the job."},"parameters":{"type":"string","description":"JSON string of parameters or prompts used for this LLM job."}},"description":"Request body for updating an existing LLM job.","required":[]},"IPoliticalNewsCrawlerLlmJobResult.IRequest":{"type":"object","properties":{"content_type":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter by content type for the results to return.\n\nOptional string filter."},"created_after":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter by creation date; results created after this timestamp are included.\n\nOptional ISO 8601 date-time string."},"created_before":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter by creation date; results created before this timestamp are included.\n\nOptional ISO 8601 date-time string."},"page":{"oneOf":[{"type":"number","minimum":1},{"type":"null"}],"description":"Pagination: current page number.\n\nDefault: 1"},"limit":{"oneOf":[{"type":"number","minimum":1},{"type":"null"}],"description":"Pagination: number of items per page.\n\nDefault: 20"}},"description":"Request type for searching LLM job results with filters and pagination.","required":[]},"IPoliticalNewsCrawlerLlmJobResult":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"llm_job_id":{"type":"string","format":"uuid","description":"Associated LLM job's political_news_crawler_llm_jobs.id.\n\nUUID format as per Prisma schema."},"content_type":{"type":"string","description":"Type of generated content, e.g., 'summary', 'highlight', 'analysis'.\n\nIndicates the nature of the processed content."},"content_text":{"type":"string","description":"Generated content text by the LLM.\n\nContains the full textual data of the processed content."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the output was created.\n\nDateTime in RFC 3339 / ISO 8601 format with timezone."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the output was last updated.\n\nDateTime in RFC 3339 / ISO 8601 format with timezone."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, null if not deleted.\n\nOptional timestamp indicating logical deletion."}},"required":["id","llm_job_id","content_type","content_text","created_at","updated_at"],"description":"LLM results store the output content generated by LLM jobs, including summaries, highlights, and analyses. This model links back to the originating LLM job and preserves output details for retrieval and audit purposes.\n\n @namespace Processing\n @author AutoBE - https://github.com/wrtnlabs/autobe"},"IPoliticalNewsCrawlerLlmJobResult.ICreate":{"type":"object","properties":{"llm_job_id":{"type":"string","format":"uuid","description":"Associated LLM job's political_news_crawler_llm_jobs.id.\n\nUUID format required."},"content_type":{"type":"string","description":"Type of generated content, e.g., 'summary', 'highlight', 'analysis'.\n\nMandatory content type string."},"content_text":{"type":"string","description":"Generated content text by the LLM.\n\nFull text content to store."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the content is generated.\n\nISO 8601 date-time string format."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the content was last updated.\n\nISO 8601 date-time string format."}},"required":["llm_job_id","content_type","content_text","created_at","updated_at"],"description":"Create request schema for new LLM job result record."},"IPoliticalNewsCrawlerLlmJobResult.IUpdate":{"type":"object","properties":{"llm_job_id":{"type":"string","format":"uuid","description":"Associated LLM job's political_news_crawler_llm_jobs.id.\n\nUUID format, optional for update."},"content_type":{"type":"string","description":"Type of generated content.\n\nOptional string for update."},"content_text":{"type":"string","description":"Generated content text.\n\nOptional full text for update."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the content is generated.\n\nOptional ISO 8601 string."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the content was last updated.\n\nOptional ISO 8601 string."}},"required":[],"description":"Update request schema for LLM job result record."},"IPoliticalNewsCrawlerProcessingMetadataArray":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerProcessingMetadata"},"description":"Array of processing metadata entries."},"IPoliticalNewsCrawlerProcessingMetadataICreate":{"type":"object","properties":{"llm_job_id":{"type":"string","format":"uuid","description":"Associated LLM job's unique identifier (UUID)."},"metadata_key":{"type":"string","description":"Key name of the metadata attribute."},"metadata_value":{"type":"string","description":"Value associated with the metadata attribute."}},"required":["llm_job_id","metadata_key","metadata_value"],"description":"Creation object for a processing metadata entry representing key-value pairs linked to an LLM job."},"IPoliticalNewsCrawlerProcessingMetadata":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the metadata entry."},"llm_job_id":{"type":"string","format":"uuid","description":"Associated LLM job's unique identifier (UUID)."},"metadata_key":{"type":"string","description":"Key name of the metadata attribute."},"metadata_value":{"type":"string","description":"Value associated with the metadata attribute."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the metadata entry was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the metadata entry was last updated."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Timestamp when the metadata entry was soft deleted or null if active."}},"required":["id","llm_job_id","metadata_key","metadata_value","created_at","updated_at"],"description":"Processing metadata entry contains key-value pairs associated with an LLM job used to provide auxiliary contextual information for processing."},"IPoliticalNewsCrawlerProcessingMetadataIUpdateArray":{"type":"array","items":{"$ref":"#/components/schemas/IPoliticalNewsCrawlerProcessingMetadata"},"description":"Array of processing metadata entries for batch update."},"IPoliticalNewsCrawlerProcessingMetadata.IUpdate":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"The unique identifier of the metadata entry.\n\nThis ID is required to identify the record to update."},"metadata_key":{"type":"string","description":"Key name of the metadata attribute.\n\nDescribes the metadata context related to LLM processing."},"metadata_value":{"type":"string","description":"Value of the metadata attribute.\n\nContains the corresponding value for the key."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp of the metadata entry."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp of the metadata entry."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp if applicable."}},"required":["id","metadata_key","metadata_value","created_at","updated_at"],"description":"Update container for processing metadata of LLM jobs.\n\nThis container represents the update DTO for processing metadata related to LLM jobs."},"IPoliticalNewsCrawlerPopularTopics.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination page number.\n\nOptional. If null, default pagination applies."},"limit":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Pagination limit per page.\n\nOptional. Default value is determined by server."},"search_term":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Search keyword for filtering topics by title.\n\nOptional."},"order_by":{"oneOf":[{"const":"createdAt"},{"const":"popularityScore"},{"type":"null"}],"description":"Sorting criterion for the list.\n\nPossible values: 'createdAt', 'popularityScore'\n\nOptional."},"order_direction":{"oneOf":[{"const":"asc"},{"const":"desc"},{"type":"null"}],"description":"Order direction.\n\nPossible values: 'asc' or 'desc'\n\nOptional."}},"description":"Request payload for searching popular political topics.","required":[]},"IPoliticalNewsCrawlerPopularTopic":{"type":"object","properties":{"id":{"type":"string","description":"Primary Key.\n\nUnique identifier of the popular political topic."},"topic_code":{"type":"string","description":"Unique code identifier for the political topic.\n\nCode uniquely identifying the political topic. Used for topic\ncategorization and mapping."},"title":{"type":"string","description":"Official title or name of the popular topic.\n\nHuman-readable title useful for displaying topic summaries and lists."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional detailed description or context about the popular topic.\n\nExtended description providing additional context or information\nabout the political topic."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp.\n\nISO 8601 date-time string indicating when the record was created."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp.\n\nISO 8601 date-time string indicating when the record was last updated."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft deletion timestamp if applicable, otherwise null.\n\nISO 8601 date-time string marking when the record was deleted\n(soft delete). null means active."}},"required":["id","topic_code","title","created_at","updated_at"],"description":"Represents a popular political topic entity.\n\nThis entity contains unique identifiers, codes, titles, and optional descriptions to\ncategorize and track trending political topics.\n\nIt manages lifecycle metadata including creation, update, and optional soft deletion timestamps.\n"},"IPoliticalNewsCrawlerPopularTopic.ICreate":{"type":"object","properties":{"topic_code":{"type":"string","description":"Unique topic code identifier."},"title":{"type":"string","description":"Title of the topic."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional description providing additional context or details."}},"required":["topic_code","title"],"description":"Request payload for creating a new popular political topic."},"IPoliticalNewsCrawlerPopularTopics":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"A unique identifier for the popular political topic.\n\nThis ID uniquely distinguishes each popular political topic within the system and is used to reliably reference topics in relationships.\n\nFormat: UUID string."},"topic_code":{"type":"string","description":"Unique code for the political topic.\n\nThis code serves as an alternate unique identifier for the topic and must be unique across all topics. Used for indexing and quick lookups."},"title":{"type":"string","description":"The official title of the popular political topic.\n\nTitles should clearly identify the topic content and must be provided."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional description for the popular political topic.\n\nProvides additional context or detailed information about the topic.\n\nNullable to allow brief or minimal entries."},"created_at":{"type":"string","format":"date-time","description":"Timestamp marking when the topic record was created.\n\nRecords audit information about data creation times.\nFormat: ISO 8601 date-time string."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp marking when the topic record was last updated.\n\nTracks changes for concurrency and audit.\nFormat: ISO 8601 date-time string."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp. Null if not deleted.\n\nMarks logical removal status.\nFormat: ISO 8601 date-time string or null."}},"required":["id","topic_code","title","created_at","updated_at"],"description":"Representation of a popular political topic as recorded in the political_news_crawler_popular_topics table.\n\nThis entity includes unique identifiers such as UUID and topic code, descriptive title and optional description, and audit timestamps for creation, update, and optional soft deletion.\n\nPopular topics serve as reference points for aggregating popularity scores and news mentions relevant to South Korean political news.\n\nSoft delete capability allows logical removal while preserving historical correctness."},"IPoliticalNewsCrawlerPopularTopic.IUpdate":{"type":"object","properties":{"topic_code":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Unique code identifier for the political topic.\n\nCode uniquely identifying the political topic. Used for topic\ncategorization and mapping."},"title":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Official title or name of the popular topic.\n\nHuman-readable title useful for displaying topic summaries and lists."},"description":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional detailed description or context about the popular topic.\n\nExtended description providing additional context or information\nabout the political topic."}},"required":[],"description":"Update data for a popular political topic.\n\nAllows modification of code, title, and description fields.\n\nSoft-deletions or audit timestamps are managed separately, not here.\n"},"IPoliticalNewsCrawlerPopularityScore.IRequest":{"type":"object","properties":{"page":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Page number.\n\nOptional pagination parameter indicating the page to retrieve."},"limit":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Limitation of records per a page.\n\nOptional pagination parameter limiting items per page.\nDefault is 100 if omitted."},"scoreMin":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Filter by popularity score greater than or equal.\n\nOptional filter to retrieve popularity scores with scores above this value."},"scoreMax":{"oneOf":[{"type":"number"},{"type":"null"}],"description":"Filter by popularity score less than or equal.\n\nOptional filter to retrieve popularity scores with scores below this value."},"snapshotAtStart":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter by snapshot date - start.\n\nOptional timestamp to filter scores by snapshot start time."},"snapshotAtEnd":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Filter by snapshot date - end.\n\nOptional timestamp to filter scores by snapshot end time."}},"required":[],"description":"Request parameters for popularity scores list filtering, sorting, and pagination."},"IPoliticalNewsCrawlerPopularityScores":{"type":"object","properties":{"id":{"type":"string","description":"Primary Key.\n\nUnique identifier of the popularity score snapshot record."},"political_news_crawler_popular_topic_id":{"type":"string","description":"Referenced popular topic's ID.\n\nForeign key to the associated popular topic."},"score":{"type":"number","description":"Calculated popularity score for the topic at this snapshot."},"decay_factor":{"type":"number","description":"Decay factor applied to the score based on the age of the topic mention."},"snapshot_at":{"type":"string","format":"date-time","description":"Timestamp when this popularity score snapshot was taken."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft deletion timestamp if applicable, otherwise null."}},"required":["id","political_news_crawler_popular_topic_id","score","decay_factor","snapshot_at","created_at","updated_at"],"description":"Snapshot record capturing popularity scores for political topics."},"IPoliticalNewsCrawlerTopicMentions.IRequest":{"type":"object","properties":{"limit":{"type":"integer","format":"int32","description":"Number of items to retrieve in the page."},"offset":{"type":"integer","format":"int32","description":"Pagination offset for items."},"search":{"type":"string","description":"Search string to filter mentions context."},"political_news_crawler_popular_topic_id":{"type":"string","format":"uuid","description":"Foreign key to the popular topic entity."},"political_news_crawler_crawled_news_id":{"type":"string","format":"uuid","description":"Foreign key to the crawled news entity."}},"required":[],"description":"Request parameters for querying topic mentions with filtering, searching, and pagination."},"IPoliticalNewsCrawlerTopicMentions":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"political_news_crawler_popular_topic_id":{"type":"string","format":"uuid","description":"Referenced popular topic's Identifier."},"political_news_crawler_crawled_news_id":{"type":"string","format":"uuid","description":"Referenced crawled news item's Identifier."},"mention_context":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional text snippet or context where the topic is mentioned within the article."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Record last update timestamp."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft deletion timestamp if applicable, otherwise null."}},"required":["id","political_news_crawler_popular_topic_id","political_news_crawler_crawled_news_id","created_at","updated_at"],"description":"Subsidiary table recording mentions of political topics within news articles. Establishes many-to-one relationships with both topics and crawled news records. Supports detailed traceability of topic references and feeds data for popularity calculations. Managed as supporting entity for topic analytics."},"IPoliticalNewsCrawlerTopicMentions.IUpdate":{"type":"object","properties":{"mention_context":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Text snippet of the mention to update."}},"required":[],"description":"Used for updating a topic mention's content snippet. All fields are optional and nullable."},"IPoliticalNewsCrawlerApiAccessLog.IRequest":{"type":"object","properties":{"limit":{"type":"integer","format":"int32","description":"Number of log entries to retrieve per page."},"offset":{"type":"integer","format":"int32","description":"Pagination offset for log entries."},"search":{"type":"string","description":"Search keyword to filter API access logs by path or client info."},"http_method":{"type":"string","description":"Filter logs by HTTP method (e.g., GET, POST)."},"path":{"type":"string","description":"Filter logs by API endpoint path."},"status_code":{"type":"integer","format":"int32","description":"Filter logs by HTTP status code."},"client_ip":{"type":"string","description":"Filter logs by client's IP address."},"user_agent":{"type":"string","description":"Filter logs by client's user agent string."},"date_from":{"type":"string","format":"date-time","description":"Start date/time for logs filtering (ISO 8601)."},"date_to":{"type":"string","format":"date-time","description":"End date/time for logs filtering (ISO 8601)."}},"required":[],"description":"Request parameters for searching and paginating API access logs."},"IPoliticalNewsCrawlerApiAccessLog":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"http_method":{"type":"string","description":"HTTP request method used in the API call."},"path":{"type":"string","description":"API endpoint path being accessed."},"status_code":{"type":"integer","format":"int32","description":"HTTP response status code returned to the client."},"client_ip":{"type":"string","description":"IP address of the client making the API request."},"user_agent":{"type":"string","description":"User agent string of the client or application making the request."},"duration_ms":{"type":"integer","format":"int32","description":"Duration of the API request processing in milliseconds."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the log entry was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the log entry was last updated."}},"required":["id","http_method","path","status_code","client_ip","user_agent","duration_ms","created_at","updated_at"],"description":"Records detailed log entries for every API access to track client requests, including request method, path, response status, client IP address, user agent, request duration in milliseconds, and timestamp. Supports comprehensive API usage analytics and operational monitoring."},"IPoliticalNewsCrawlerApiErrorLog.IRequest":{"type":"object","description":"Request type for filtering and paginating API error logs.","properties":{"path":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter by API path pattern for the error logs."},"error_code":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter errors by error code."},"start_date":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter logs within this start time (inclusive)."},"end_date":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Filter logs within this end time (inclusive)."},"page":{"oneOf":[{"type":"integer","minimum":0},{"type":"null"}],"description":"Pagination page number."},"limit":{"oneOf":[{"type":"integer","minimum":0},{"type":"null"}],"description":"Number of items per page."},"sort_order":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sort order for returned items."}},"required":[]},"IPoliticalNewsCrawlerApiErrorLog":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"path":{"type":"string","description":"API endpoint path where the error occurred."},"error_code":{"type":"string","description":"Error code identifying the type of API error."},"error_message":{"type":"string","description":"Descriptive error message to assist debugging."},"client_ip":{"type":"string","description":"IP address of the client causing the error."},"user_agent":{"type":"string","description":"User agent string of the client application."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the error log was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the error log was last updated."}},"required":["id","path","error_code","error_message","client_ip","user_agent","created_at","updated_at"],"description":"Captures detailed records of API errors, including the API path, error code, error message, client IP, user agent, occurrence timestamp, and update timestamp. Enables error analysis and system troubleshooting for API endpoints."},"IPoliticalNewsCrawlerApiUsageMetricRequest":{"type":"object","description":"Request type for filtering API usage metrics with pagination and sort options.","properties":{"http_method":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"HTTP method to filter usage metrics."},"path":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"API path to filter usage metrics."},"period_start":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Start datetime for the metric aggregation period."},"period_end":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"End datetime for the metric aggregation period."},"page":{"oneOf":[{"type":"integer","minimum":0},{"type":"null"}],"description":"Page number for pagination."},"limit":{"oneOf":[{"type":"integer","minimum":0},{"type":"null"}],"description":"Number of items per page."},"sort_order":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Sort order direction."}},"required":[]},"IPoliticalNewsCrawlerApiUsageMetric":{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier.\n\nThis ID defines each unique metric record in the API usage metrics."},"http_method":{"type":"string","description":"HTTP method for the API call aggregation.\n\nDefines the request method such as GET, POST, etc."},"path":{"type":"string","description":"Path of the API endpoint.\n\nThe endpoint path being aggregated."},"period_start":{"type":"string","format":"date-time","description":"Start time of the aggregation period.\n\nISO 8601 date-time representation marking aggregation window start."},"period_end":{"type":"string","format":"date-time","description":"End time of the aggregation period.\n\nISO 8601 date-time representation marking aggregation window end."},"total_calls":{"type":"integer","description":"Total number of API calls within the aggregation period."},"max_response_ms":{"type":"integer","description":"Maximum response time in milliseconds.\n\nThe longest duration for a single API call."},"avg_response_ms":{"type":"integer","description":"Average response time in milliseconds.\n\nRepresents the mean duration of API calls."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp of this record.\n\nISO 8601 date-time of record insertion."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp of this record.\n\nISO 8601 date-time of last modification."}},"required":["id","http_method","path","period_start","period_end","total_calls","max_response_ms","avg_response_ms","created_at","updated_at"],"description":"Represents aggregated usage metrics of API calls over defined periods.\n\nEach record includes total calls, maximum and average response times,\nassociated HTTP method, path, and timestamps for aggregation."},"IPoliticalNewsCrawlerCrawlAlertRequest":{"type":"object","properties":{"crawl_source_id":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional filter for crawl source IDs."},"alert_type":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional filter for alert types, e.g., 'ban_detected'.\n\nAllows filtering alerts by their event classification."},"severity":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional filter for severity levels such as 'info', 'warning', 'critical'."},"page":{"oneOf":[{"type":"integer","minimum":0},{"type":"null"}],"description":"Pagination - current page number."},"limit":{"oneOf":[{"type":"integer","minimum":1},{"type":"null"}],"description":"Pagination - number of records per page."}},"required":[],"description":"Represents filtering and pagination parameters for retrieving crawl alerts.\n\nIncludes optional fields for crawl source identification, alert type, severity level, and paging controls."},"IPoliticalNewsCrawlerCrawlAlerts":{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier."},"crawl_source_id":{"type":"string","description":"Crawl source identifier that generated this alert."},"alert_type":{"type":"string","description":"Type of alert event.\n\nExamples include 'ban_detected', 'network_error', 'throttle_warning'."},"message":{"type":"string","description":"Human-readable message describing the alert.\n\nProvides operational context and details."},"severity":{"type":"string","description":"Severity level of the alert.\n\nValues include 'info', 'warning', 'critical'."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when alert was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when alert was last updated."}},"required":["id","crawl_source_id","alert_type","message","severity","created_at","updated_at"],"description":"Represents an alert event related to crawling.\n\nThis contains details about the alert type, message, severity level, and\nassociated crawl source provenance."},"IPoliticalNewsCrawlerCrawlAlerts.ICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","description":"Crawl source identifier.\n\nRequired to link alert to source."},"alert_type":{"type":"string","description":"Alert event type.\n\nExamples include 'ban_detected', 'network_error', 'throttle_warning'."},"message":{"type":"string","description":"Alert message.\n\nDetailed description of the alert event."},"severity":{"oneOf":[{"const":"info","description":"Alert severity level.\n\nMust be one of 'info', 'warning', or 'critical'."},{"const":"warning","description":"Alert severity level.\n\nMust be one of 'info', 'warning', or 'critical'."},{"const":"critical","description":"Alert severity level.\n\nMust be one of 'info', 'warning', or 'critical'."}],"description":"Alert severity level.\n\nMust be one of 'info', 'warning', or 'critical'."}},"required":["crawl_source_id","alert_type","message","severity"],"description":"Information needed to create a crawl alert event.\n\nIncludes required identifiers and metadata about the alert type,\nmessage content, and severity level."},"IPoliticalNewsCrawlerCrawlAlerts.IUpdate":{"type":"object","properties":{"alert_type":{"type":"string","description":"Type of alert event indicating the category, e.g., 'ban_detected', 'network_error', 'throttle_warning'."},"message":{"type":"string","description":"Detailed description of the alert event and context for operational understanding."},"severity":{"oneOf":[{"const":"info","description":"Severity level of the alert such as 'info', 'warning', 'critical'."},{"const":"warning","description":"Severity level of the alert such as 'info', 'warning', 'critical'."},{"const":"critical","description":"Severity level of the alert such as 'info', 'warning', 'critical'."}],"description":"Severity level of the alert such as 'info', 'warning', 'critical'."}},"required":["alert_type","message","severity"],"description":"Request schema for updating political news crawler crawl alert. Defines alert type, message, and severity level."},"IPoliticalNewsCrawlerProcessingAlert.IRequest":{"type":"object","properties":{"alert_type":{"oneOf":[{"type":"string","description":"Filter alerts by type (e.g., 'llm_failure') to limit results."},{"type":"null"}],"description":"Filter alerts by type (e.g., 'llm_failure') to limit results."},"severity":{"oneOf":[{"const":"info","description":"Filter alerts by severity level, including 'info', 'warning', and 'critical'."},{"const":"warning","description":"Filter alerts by severity level, including 'info', 'warning', and 'critical'."},{"const":"critical","description":"Filter alerts by severity level, including 'info', 'warning', and 'critical'."},{"type":"null"}],"description":"Filter alerts by severity level, including 'info', 'warning', and 'critical'."},"message":{"oneOf":[{"type":"string","description":"Filter by partial or full alert message text content."},{"type":"null"}],"description":"Filter by partial or full alert message text content."},"page":{"oneOf":[{"type":"integer","minimum":0,"description":"Page number for pagination, defaults to 0 (first page)."},{"type":"null"}],"description":"Page number for pagination, defaults to 0 (first page)."},"limit":{"oneOf":[{"type":"integer","minimum":1,"description":"Number of records per page, defaults to 100."},{"type":"null"}],"description":"Number of records per page, defaults to 100."}},"required":[],"description":"Request schema for listing and filtering political news crawler processing alerts."},"IPoliticalNewsCrawlerProcessingAlert.ICreate":{"type":"object","properties":{"alert_type":{"type":"string","description":"Type of processing alert such as 'llm_failure', 'queue_overflow', 'retry_limit_reached'."},"message":{"type":"string","description":"Detailed description of the processing alert event for operational use."},"severity":{"oneOf":[{"const":"info","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},{"const":"warning","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},{"const":"critical","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."}],"description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."}},"required":["alert_type","message","severity"],"description":"Request schema for creating political news crawler processing alert events."},"IPoliticalNewsCrawlerProcessingAlert":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the processing alert."},"alert_type":{"type":"string","description":"Category of processing alert such as 'llm_failure', 'queue_overflow', 'retry_limit_reached'."},"message":{"type":"string","description":"Detailed description of the processing alert event for operational use."},"severity":{"oneOf":[{"const":"info","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},{"const":"warning","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},{"const":"critical","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."}],"description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the alert was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp for last update of the alert."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, null if not deleted."}},"required":["id","alert_type","message","severity","created_at","updated_at"],"description":"Represents a processing alert event record for the political news crawler backend system."},"IPoliticalNewsCrawlerProcessingAlert.IUpdate":{"type":"object","properties":{"alert_type":{"type":"string","description":"Category of processing alert such as 'llm_failure', 'queue_overflow', 'retry_limit_reached'."},"message":{"type":"string","description":"Detailed description of the processing alert event for operational use."},"severity":{"type":"string","description":"Severity level of the alert (e.g., 'info', 'warning', 'critical')."},"id":{"type":"string","format":"uuid","description":"Primary Key."},"created_at":{"type":"string","format":"date-time","description":"Metadata entry creation timestamp."},"updated_at":{"type":"string","format":"date-time","description":"Metadata entry last update timestamp."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp, null if not deleted."}},"required":[],"description":"Update data schema for editing existing processing alert. All fields are optional and nullable."},"IPoliticalNewsCrawlerApiAlert.IRequest":{"type":"object","properties":{"alert_type":{"type":"string","description":"Type of API alert event such as 'rate_limit_exceeded', 'endpoint_error', or 'error_spike'."},"message":{"type":"string","description":"Detailed message describing the API alert context."},"severity":{"type":"string","description":"Severity level of the alert such as 'info', 'warning', 'critical'."},"created_at_lt":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Query filter for alerts created before specified date."},"created_at_gt":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Query filter for alerts created after specified date."},"updated_at_lt":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Query filter for alerts updated before specified date."},"updated_at_gt":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Query filter for alerts updated after specified date."},"page":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Page number for pagination."},"limit":{"oneOf":[{"type":"integer"},{"type":"null"}],"description":"Limit number of records per page."},"orderBy":{"type":"string","description":"Ordering field and direction, e.g., 'created_at desc'."}},"required":[],"description":"Request schema for paginated and filtered query of API alert records."},"IPoliticalNewsCrawlerApiAlert.ICreate":{"type":"object","properties":{"alert_type":{"type":"string","description":"Type of API alert event such as 'rate_limit_exceeded', 'endpoint_error', or 'error_spike'."},"message":{"type":"string","description":"Detailed message describing the API alert context."},"severity":{"type":"string","description":"Severity level of the alert such as 'info', 'warning', 'critical'."}},"required":["alert_type","message","severity"],"description":"Creation schema for a new API alert entry."},"IPoliticalNewsCrawlerApiAlert":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"alert_type":{"type":"string","description":"Type of API alert event such as 'rate_limit_exceeded', 'endpoint_error', or 'error_spike'."},"message":{"type":"string","description":"Detailed message describing the API alert context."},"severity":{"type":"string","description":"Severity level of the alert such as 'info', 'warning', 'critical'."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the alert was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when the alert was last updated."}},"required":["id","alert_type","message","severity","created_at","updated_at"],"description":"API alert entity storing system error notifications and status messages.\n\nThis entity records detailed information about operational events related to API subsystem errors.\n\nIt captures fields including alert type to identify the nature of the error, a descriptive message for context, severity level to prioritize alerts, and timestamps for creation and last update.\n\nThis structured data facilitates real-time monitoring and analysis of API performance and stability within the politicalNewsCrawler backend system."},"IPoliticalNewsCrawlerApiAlert.IUpdate":{"type":"object","properties":{"alert_type":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Type of API alert event.\n\n@example \"rate_limit_exceeded\""},"id":{"type":"string","format":"uuid","description":"Unique identifier of the API alert to update."},"message":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Detailed message describing the API alert.\n\n@example \"Exceeded rate limit of 1000 per minute\""},"severity":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Severity level such as 'info', 'warning', 'critical'.\n\n@example \"critical\""}},"required":["id"],"description":"Update data for ApiAlert that allows modification of alert_type, message and severity without changing the unique id."},"IPoliticalNewsCrawlerCrawlSources.ISummary":{"type":"object","properties":{"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"id":{"type":"string","format":"uuid","description":"Primary Key."},"is_active":{"type":"boolean","description":"Flag indicating whether the crawl source is active and enabled for crawling."},"source_code":{"type":"string","description":"Unique identifier code for the crawl source."},"source_url":{"type":"string","description":"The base URL of the crawl source website or API."}},"required":["id","source_code","source_url","is_active","created_at"],"description":"Summary view of the politicalNewsCrawler crawl source for listing purposes."},"IPoliticalNewsCrawlerCrawlPolicy.ISummary":{"type":"object","properties":{"ban_detection_enabled":{"type":"boolean","description":"Flag to enable detection and handling of bans during crawling."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"id":{"type":"string","format":"uuid","description":"Primary Key."},"max_crawl_frequency_minutes":{"type":"integer","description":"Maximum allowed crawl frequency in minutes."},"policy_name":{"type":"string","description":"Unique name identifier for the crawl policy."}},"required":["id","policy_name","max_crawl_frequency_minutes","ban_detection_enabled","created_at"],"description":"Summary view of the politicalNewsCrawler crawl policy optimized for list display."},"IPoliticalNewsCrawlerCrawlSchedule.ISummary":{"type":"object","properties":{"crawl_policy_id":{"type":"string","format":"uuid","description":"Reference to Crawl Policy. Reference id."},"crawl_source_id":{"type":"string","format":"uuid","description":"Reference to Crawling Source. Reference id."},"created_at":{"type":"string","format":"date-time","description":"Record creation timestamp."},"id":{"type":"string","format":"uuid","description":"Primary Key."},"is_enabled":{"type":"boolean","description":"Flag indicating if this schedule is enabled."},"schedule_expression":{"type":"string","description":"Cron expression defining the crawl schedule timing."}},"required":["id","crawl_source_id","crawl_policy_id","schedule_expression","is_enabled","created_at"],"description":"Summary view of the politicalNewsCrawler crawl schedule optimized for listing."},"IPoliticalNewsCrawlerGuests.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"ip_address":{"type":"string","description":"IP address of the guest user."},"user_agent":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"User agent string presented by the guest."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the guest record was created."}},"required":["id","ip_address","created_at"],"description":"Summary records of political news crawler guest users with essential identification and creation timestamps."},"IPoliticalNewsCrawlerCrawlAttempt.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"crawl_job_id":{"type":"string","format":"uuid","description":"Associated crawl job identifier."},"started_at":{"type":"string","format":"date-time","description":"Timestamp when this crawl attempt started."},"success":{"type":"boolean","description":"Indicator whether this crawl attempt was successful."}},"required":["id","crawl_job_id","started_at","success"],"description":"Summary records of crawl attempts containing essential identifiers and status."},"IPoliticalNewsCrawlerCrawledNews.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"url":{"type":"string","description":"URL of the crawled news article."},"title":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Title of the news article, if available."},"published_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Publish timestamp of the news article, if known."}},"required":["id","url"],"description":"Summary records of crawled news articles including minimal metadata for display and identification."},"IPoliticalNewsCrawlerRawDataStorage.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"storage_key":{"type":"string","description":"Unique key or path identifying storage location in cloud object storage (e.g., GCP or AWS S3)."},"file_format":{"type":"string","description":"Format of the raw data file such as JSON or XML for processing compatibility."},"file_size_bytes":{"type":"integer","format":"int32","description":"Size of the raw data file in bytes."},"crawl_timestamp":{"type":"string","format":"date-time","description":"Timestamp when the raw data was crawled, used for data freshness and scheduling."}},"required":["id","storage_key","file_format","file_size_bytes","crawl_timestamp"],"description":"Summary records summarizing raw data storage entities with key file attributes for efficient identification and listing."},"IPoliticalNewsCrawlerRawDataStorageLocalCacheFile.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary key identifying the local cache file record."},"raw_data_storage_id":{"type":"string","format":"uuid","description":"Reference ID to the related raw data storage record."},"local_file_path":{"type":"string","description":"Filesystem path or identifier for the local cached file copy."},"file_size_bytes":{"type":"integer","description":"Size of the local cached file in bytes."},"ttl_expiration_at":{"type":"string","format":"date-time","description":"Datetime when the cached file expires and is due for deletion under TTL policy."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft delete timestamp indicating when the cached file was deleted, if applicable."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp record."},"updated_at":{"type":"string","format":"date-time","description":"Last update timestamp record."}},"required":["id","raw_data_storage_id","local_file_path","file_size_bytes","ttl_expiration_at","created_at","updated_at"],"description":"Summary information about a local cache file related to raw crawled data. Provides essential identifiers and metadata for quick access and management."},"IPoliticalNewsCrawlerProcessedContent.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary key identifying the processed content record."},"raw_data_storage_id":{"type":"string","format":"uuid","description":"Foreign key linking to raw data storage."},"llm_job_id":{"oneOf":[{"type":"string","format":"uuid"},{"type":"null"}],"description":"Foreign key to associated LLM job, can be null if not linked."},"content_type":{"type":"string","description":"Type of processed content, e.g., summary, highlight, or analysis."},"generation_timestamp":{"type":"string","format":"date-time","description":"Timestamp when this content was generated."}},"required":["id","raw_data_storage_id","content_type","generation_timestamp"],"description":"Summary of processed content records including basic identification, type and generation time."},"IPoliticalNewsCrawlerLlmJobs.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the LLM job."},"crawl_source_id":{"type":"string","format":"uuid","description":"Source channel ID the raw news came from."},"status":{"type":"string","description":"Status string, e.g., pending, running, completed, failed."},"created_at":{"type":"string","format":"date-time","description":"Timestamp job was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp of last job update."}},"required":["id","crawl_source_id","status","created_at","updated_at"],"description":"Summary information of LLM job including identification and status timestamps."},"IPoliticalNewsCrawlerPopularityScore.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary key identifying the popularity score snapshot."},"political_news_crawler_popular_topic_id":{"type":"string","format":"uuid","description":"Reference to the popular topic ID."},"score":{"type":"number","format":"double","description":"Computed score indicating popularity."},"decay_factor":{"type":"number","format":"double","description":"Decay factor applied to score for time decay."},"snapshot_at":{"type":"string","format":"date-time","description":"Timestamp of the popularity score snapshot."}},"required":["id","political_news_crawler_popular_topic_id","score","decay_factor","snapshot_at"],"description":"Summary data for a popularity score snapshot related to a specific popular topic."},"IPoliticalNewsCrawlerTopicMentions.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"mention_context":{"oneOf":[{"type":"string"},{"type":"null"}],"description":"Optional text snippet or context where the topic is mentioned within the article."}},"required":["id"],"description":"A summary type for topic mentions containing minimal identification and context."},"IPoliticalNewsCrawlerApiAccessLog.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"http_method":{"type":"string","description":"HTTP request method used in the API call."},"path":{"type":"string","description":"API endpoint path being accessed."},"status_code":{"type":"integer","format":"int32","description":"HTTP response status code returned to the client."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the log entry was created."}},"required":["id","http_method","path","status_code","created_at"],"description":"A summary type containing essential API access log information for listing and overview."},"IPoliticalNewsCrawlerApiErrorLog.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"path":{"type":"string","description":"API endpoint path where the error occurred."},"error_code":{"type":"string","description":"Error code identifying the type of API error."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when the error log was created."}},"required":["id","path","error_code","created_at"],"description":"A summary type providing brief error log info for listing and monitoring."},"IPoliticalNewsCrawlerApiUsageMetricSummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key."},"http_method":{"type":"string","description":"HTTP method for which metrics are aggregated."},"path":{"type":"string","description":"API endpoint path for which metrics are aggregated."},"period_start":{"type":"string","format":"date-time","description":"Start timestamp of the aggregation period."},"period_end":{"type":"string","format":"date-time","description":"End timestamp of the aggregation period."},"total_calls":{"type":"integer","format":"int32","description":"Total number of API calls observed in the aggregation period."},"max_response_ms":{"type":"integer","format":"int32","description":"Maximum response time in milliseconds recorded during the period."},"avg_response_ms":{"type":"integer","format":"int32","description":"Average response time in milliseconds over the period."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when this aggregated record was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when this aggregated record was last updated."}},"required":["id","http_method","path","period_start","period_end","total_calls","max_response_ms","avg_response_ms","created_at","updated_at"],"description":"Aggregated API usage metrics capturing total counts of API calls by method and path over specific time periods, including maximum response times and average durations. Supports performance monitoring and traffic analysis for API endpoints."},"IPoliticalNewsCrawlerCrawlAlert":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Primary Key.\n\nUnique identifier of this crawl alert.\n\nFormat: UUID string."},"crawl_source_id":{"type":"string","format":"uuid","description":"Referenced crawl source's ID which triggered the alert.\n\nFormat: UUID string."},"alert_type":{"type":"string","description":"Type of alert event indicating category, e.g., 'ban_detected', 'network_error', 'throttle_warning'."},"message":{"type":"string","description":"Detailed description of the alert event providing operational context."},"severity":{"type":"string","description":"Severity level such as 'info', 'warning', or 'critical' indicating urgency."},"created_at":{"type":"string","format":"date-time","description":"Timestamp when alert record was created."},"updated_at":{"type":"string","format":"date-time","description":"Timestamp when alert record was last updated."},"deleted_at":{"oneOf":[{"type":"string","format":"date-time"},{"type":"null"}],"description":"Soft deletion timestamp if the record is deleted.\n\nNullable."}},"required":["id","crawl_source_id","alert_type","message","severity","created_at","updated_at"],"description":"A crawl alert record represents events such as bans or errors detected during crawling. It captures detailed context to support operational monitoring and troubleshooting.\n\nThis entity is associated with a crawl source and indicates the type and severity of the alert with timestamps.\n\nProperties include declarative alert type, descriptive text message, urgency level, and audit information.\n\nSoft deletion is supported."},"IPoliticalNewsCrawlerCrawlAlert.ICreate":{"type":"object","properties":{"crawl_source_id":{"type":"string","format":"uuid","description":"Referenced crawl source ID which triggered the alert.\nRequired during creation."},"alert_type":{"type":"string","description":"Type of alert event.\nRequired during creation."},"message":{"type":"string","description":"Detailed message describing alert.\nRequired during creation."},"severity":{"type":"string","description":"Severity level (\"info\", \"warning\", \"critical\").\nRequired during creation."}},"required":["crawl_source_id","alert_type","message","severity"],"description":"Properties required to create a new crawl alert. All fields are required without timestamps since they are auto-generated."},"IPoliticalNewsCrawlerCrawlAlert.IUpdate":{"type":"object","properties":{"alert_type":{"type":"string","description":"Type of alert event.\nOptional."},"message":{"type":"string","description":"Detailed message describing alert.\nOptional."},"severity":{"type":"string","description":"Severity level (\"info\", \"warning\", \"critical\").\nOptional."}},"required":[],"description":"Properties used to update an existing crawl alert record. All fields are optional."},"IPoliticalNewsCrawlerCrawlAlert.ISummary":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier of the alert."},"alert_type":{"type":"string","description":"Type of alert event."},"message":{"type":"string","description":"Detailed message content."},"severity":{"type":"string","description":"Severity level."},"created_at":{"type":"string","format":"date-time","description":"Creation timestamp."}},"required":["id","alert_type","message","severity","created_at"],"description":"Summary version of crawl alert record with essential identification and status fields for list views."},"IPoliticalNewsCrawlerApiAlert.ISummary":{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier of the API alert."},"alert_type":{"type":"string","description":"API alert event type such as 'rate_limit_exceeded', 'endpoint_error', 'error_spike'."},"message":{"type":"string","description":"Short message describing the API alert."},"severity":{"type":"string","description":"Severity of the alert ('info', 'warning', 'critical')."},"created_at":{"type":"string","format":"date-time","description":"Timestamp of alert creation."}},"required":["id","alert_type","message","severity","created_at"],"description":"Summary version of API alert record including unique id, event type, message, severity level, and creation time."}}}}
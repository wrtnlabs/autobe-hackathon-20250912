import { tags } from "typia";

/**
 * Stores metadata and references for raw political news data collected from
 * various crawling sources. Ensures durable and consistent storage links to
 * cloud object storage. Tracks source information, crawl job association, and
 * data integrity validations. Includes audit timestamps for traceability.
 *
 * @autobe Generated by AutoBE - https://github.com/wrtnlabs/autobe
 */
export type IPoliticalNewsCrawlerRawDataStorage = {
  /** Primary Key. */
  id: string & tags.Format<"uuid">;

  /** Belonged crawl source's political_news_crawler_crawl_sources.id. */
  crawl_source_id: string & tags.Format<"uuid">;

  /** Optional crawl job reference to political_news_crawler_crawl_jobs.id. */
  crawl_job_id?: (string & tags.Format<"uuid">) | null | undefined;

  /**
   * Unique key or path identifying storage location in cloud object storage
   * (e.g., GCP or AWS S3).
   */
  storage_key: string;

  /**
   * Format of the raw data file such as JSON or XML for processing
   * compatibility.
   */
  file_format: string;

  /** Size of the raw data file in bytes. */
  file_size_bytes: number & tags.Type<"int32">;

  /** Checksum hash to verify file integrity. */
  checksum?: string | null | undefined;

  /**
   * Timestamp when the raw data was crawled, used for data freshness and
   * scheduling.
   */
  crawl_timestamp: string & tags.Format<"date-time">;

  /** Creation timestamp record. */
  created_at: string & tags.Format<"date-time">;

  /** Last update timestamp record. */
  updated_at: string & tags.Format<"date-time">;
};
export namespace IPoliticalNewsCrawlerRawDataStorage {
  /**
   * Retrieve a filtered and paginated list of raw data storage metadata
   * entries for political news crawling. The operation supports filtering by
   * crawl source and crawl job identifiers, file format types, crawl
   * timestamps, and file sizes. Sorting and pagination options enable
   * efficient browsing through large datasets stored in cloud object
   * storage.
   *
   * Security considerations include limited access to authenticated users
   * with appropriate read privileges, as raw data files may contain sensitive
   * or proprietary information.
   *
   * This operation is tightly integrated with the
   * political_news_crawler_raw_data_storage table defined in the Prisma
   * schema, encompassing all relevant fields and relationships. The response
   * returns simplified summary information suited for list displays.
   *
   * There is no request body since this is a PATCH method designed for
   * complex search and filtering inputs.
   */
  export type IRequest = {
    /** Belonged crawl source's political_news_crawler_crawl_sources.id. */
    crawl_source_id?: (string & tags.Format<"uuid">) | null | undefined;

    /** Optional crawl job reference to political_news_crawler_crawl_jobs.id. */
    crawl_job_id?: (string & tags.Format<"uuid">) | null | undefined;

    /**
     * Unique key or path identifying storage location in cloud object
     * storage (e.g., GCP or AWS S3).
     */
    storage_key?: string | null | undefined;

    /**
     * Format of the raw data file such as JSON or XML for processing
     * compatibility.
     */
    file_format?: string | null | undefined;

    /** Size of the raw data file in bytes. */
    file_size_bytes?: number | null | undefined;

    /** Checksum hash to verify file integrity. */
    checksum?: string | null | undefined;

    /**
     * Timestamp when the raw data was crawled, used for data freshness and
     * scheduling.
     */
    crawl_timestamp?: (string & tags.Format<"date-time">) | null | undefined;

    /** Creation timestamp record. */
    created_at?: (string & tags.Format<"date-time">) | null | undefined;

    /** Last update timestamp record. */
    updated_at?: (string & tags.Format<"date-time">) | null | undefined;
  };

  /**
   * Creation info of the crawl source
   *
   * @author AutoBE - https://github.com/wrtnlabs/autobe
   */
  export type ICreate = {
    /**
     * Belonged crawl source's political_news_crawler_crawl_sources.id.
     *
     * Identifier of the crawl source from which this raw data originated.
     */
    crawl_source_id: string;

    /**
     * Optional crawl job reference to political_news_crawler_crawl_jobs.id.
     *
     * Identifier of the crawl job associated with this raw data, if any.
     */
    crawl_job_id?: string | null | undefined;

    /**
     * Unique key or path identifying storage location in cloud object
     * storage (e.g., GCP or AWS S3).
     *
     * Storage key that identifies the raw data file location within cloud
     * storage.
     */
    storage_key: string;

    /**
     * Format of the raw data file such as JSON or XML for processing
     * compatibility.
     *
     * String specifying the file format of the stored raw data.
     */
    file_format: string;

    /**
     * Size of the raw data file in bytes.
     *
     * Integer representing the size of the raw data file in bytes.
     */
    file_size_bytes: number & tags.Type<"int32">;

    /**
     * Checksum hash to verify file integrity.
     *
     * Optional checksum string for data integrity verification.
     */
    checksum?: string | null | undefined;

    /**
     * Timestamp when the raw data was crawled, used for data freshness and
     * scheduling.
     *
     * Date-time when the raw data was obtained.
     */
    crawl_timestamp: string & tags.Format<"date-time">;
  };

  /**
   * Update info of the crawl source
   *
   * @author AutoBE - https://github.com/wrtnlabs/autobe
   */
  export type IUpdate = {
    /**
     * Belonged crawl source's political_news_crawler_crawl_sources.id.
     *
     * Identifier of the crawl source from which this raw data originated.
     */
    crawl_source_id?: string | undefined;

    /**
     * Optional crawl job reference to political_news_crawler_crawl_jobs.id.
     *
     * Identifier of the crawl job associated with this raw data, if any.
     */
    crawl_job_id?: string | null | undefined;

    /**
     * Unique key or path identifying storage location in cloud object
     * storage (e.g., GCP or AWS S3).
     *
     * Storage key that identifies the raw data file location within cloud
     * storage.
     */
    storage_key?: string | undefined;

    /**
     * Format of the raw data file such as JSON or XML for processing
     * compatibility.
     *
     * String specifying the file format of the stored raw data.
     */
    file_format?: string | undefined;

    /**
     * Size of the raw data file in bytes.
     *
     * Integer representing the size of the raw data file in bytes.
     */
    file_size_bytes?: (number & tags.Type<"int32">) | undefined;

    /**
     * Checksum hash to verify file integrity.
     *
     * Optional checksum string for data integrity verification.
     */
    checksum?: string | null | undefined;

    /**
     * Timestamp when the raw data was crawled, used for data freshness and
     * scheduling.
     *
     * Date-time when the raw data was obtained.
     */
    crawl_timestamp?: (string & tags.Format<"date-time">) | undefined;
  };

  /**
   * Summary records summarizing raw data storage entities with key file
   * attributes for efficient identification and listing.
   */
  export type ISummary = {
    /** Primary Key. */
    id: string & tags.Format<"uuid">;

    /**
     * Unique key or path identifying storage location in cloud object
     * storage (e.g., GCP or AWS S3).
     */
    storage_key: string;

    /**
     * Format of the raw data file such as JSON or XML for processing
     * compatibility.
     */
    file_format: string;

    /** Size of the raw data file in bytes. */
    file_size_bytes: number & tags.Type<"int32">;

    /**
     * Timestamp when the raw data was crawled, used for data freshness and
     * scheduling.
     */
    crawl_timestamp: string & tags.Format<"date-time">;
  };
}
